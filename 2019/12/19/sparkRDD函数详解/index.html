<!-- build time:Mon Jun 08 2020 17:24:20 GMT+0800 (GMT+08:00) --><!DOCTYPE html><html class="theme-next gemini use-motion" lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#222"><script src="//cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><link href="//cdn.jsdelivr.net/npm/pace-js@1.0.2/themes/blue/pace-theme-minimal.css" rel="stylesheet"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/blog/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="https://fonts.loli.net/css?family=EB+Garamond:400,400i,700,700i|Noto+Serif+SC:400,500,700&display=swap&subset=chinese-simplified" rel="stylesheet"><link href="//cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"><link href="/blog/css/main.css?v=5.1.4" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/blog/images/bb.ico?v=5.1.4"><link rel="icon" type="image/png" sizes="32x32" href="/blog/images/bb.ico?v=5.1.4"><link rel="icon" type="image/png" sizes="16x16" href="/blog/images/bb.ico?v=5.1.4"><link rel="mask-icon" href="/blog/images/bb.ico?v=5.1.4" color="#222"><meta name="keywords" content="spark,"><meta name="description" content="RDD操作详解12启动spark-shellspark-shell --master spark:&#x2F;&#x2F;hdp-node-01:7077基本转换mapmap是对RDD中的每个元素都执行一个指定的函数来产生一个新的RDD。 任何原RDD中的元素在新RDD中都有且只有一个元素与之对应。举例：1234567scala&gt; val a &#x3D; sc.parallelize(1 to 9, 3)scala&amp;g"><meta property="og:type" content="article"><meta property="og:title" content="sparkRDD函数详解"><meta property="og:url" content="https://gitee.com/lei_binbin/blog/2019/12/19/sparkRDD%E5%87%BD%E6%95%B0%E8%AF%A6%E8%A7%A3/index.html"><meta property="og:site_name" content="bin of books"><meta property="og:description" content="RDD操作详解12启动spark-shellspark-shell --master spark:&#x2F;&#x2F;hdp-node-01:7077基本转换mapmap是对RDD中的每个元素都执行一个指定的函数来产生一个新的RDD。 任何原RDD中的元素在新RDD中都有且只有一个元素与之对应。举例：1234567scala&gt; val a &#x3D; sc.parallelize(1 to 9, 3)scala&amp;g"><meta property="og:locale" content="zh_CN"><meta property="article:published_time" content="2019-12-19T11:20:08.000Z"><meta property="article:modified_time" content="2020-06-08T07:58:38.044Z"><meta property="article:author" content="bin"><meta property="article:tag" content="spark"><meta name="twitter:card" content="summary"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/blog/",scheme:"Gemini",version:"5.1.4",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!0,onmobile:!0},fancybox:!0,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"flipYIn",post_header:"perspectiveRightIn",post_body:"perspectiveLeftIn",coll_header:"perspectiveDownIn",sidebar:"perspectiveUpIn"}},duoshuo:{userId:"0",author:"Author"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="https://gitee.com/lei_binbin/blog/2019/12/19/sparkRDD函数详解/"><script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script><link href="//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css" rel="stylesheet"><script></script><title>sparkRDD函数详解 | bin of books</title><script type="text/javascript" src="/js/src/bubble.js"></script><meta name="generator" content="Hexo 4.2.1"></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/blog/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">bin of books</span> <span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description">记录生活，记录技术，记录点滴！</h1></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/blog/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-archives"><a href="/blog/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li><li class="menu-item menu-item-categories"><a href="/blog/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>分类</a></li><li class="menu-item menu-item-tags"><a href="/blog/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li><li class="menu-item menu-item-about"><a href="/blog/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>搜索</a></li></ul><div class="site-search"><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class="search-icon"><i class="fa fa-search"></i> </span><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span><div class="local-search-input-wrapper"><input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input"></div></div><div id="local-search-result"></div></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="https://gitee.com/lei_binbin/blog/blog/2019/12/19/sparkRDD%E5%87%BD%E6%95%B0%E8%AF%A6%E8%A7%A3/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="bin"><meta itemprop="description" content=""><meta itemprop="image" content="/blog/images/ll.jpg"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="bin of books"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">sparkRDD函数详解</h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><time title="创建时间" itemprop="dateCreated datePublished" datetime="2019-12-19T19:20:08+08:00">2019-12-19 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/blog/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">大数据</span> </a></span></span><span class="post-wordcount"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span title="Words count in article"></span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="RDD操作详解"><a href="#RDD操作详解" class="headerlink" title="RDD操作详解"></a>RDD操作详解</h1><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">启动spark-shell</span><br><span class="line">spark-shell --master spark://hdp-node-01:7077</span><br></pre></td></tr></table></figure></div><h1 id="基本转换"><a href="#基本转换" class="headerlink" title="基本转换"></a>基本转换</h1><h2 id="map"><a href="#map" class="headerlink" title="map"></a>map</h2><p>map是对RDD中的每个元素都执行一个指定的函数来产生一个新的RDD。 任何原RDD中的元素在新RDD中都有且只有一个元素与之对应。<br>举例：</p><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val a = sc.parallelize(1 to 9, 3)</span><br><span class="line">scala&gt; val b = a.map(x =&gt; x*2)</span><br><span class="line">scala&gt; a.collect</span><br><span class="line">res10: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9)</span><br><span class="line">scala&gt; b.collect</span><br><span class="line">res11: Array[Int] = Array(2, 4, 6, 8, 10, 12, 14, 16, 18)</span><br><span class="line">上述例子中把原RDD中每个元素都乘以2来产生一个新的RDD。</span><br></pre></td></tr></table></figure></div><h2 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">filter 是对RDD中的每个元素都执行一个指定的函数来过滤产生一个新的RDD。 任何原RDD中的元素在新RDD中都有且只有一个元素与之对应。</span><br><span class="line">val rdd = sc.parallelize(List(1,2,3,4,5,6))  </span><br><span class="line">val filterRdd = rdd.filter(_ &gt; 5)</span><br><span class="line">filterRdd.collect() //返回所有大于5的数据的一个Array</span><br></pre></td></tr></table></figure></div><h2 id="flatMap"><a href="#flatMap" class="headerlink" title="flatMap"></a>flatMap</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">与map类似，区别是原RDD中的元素经map处理后只能生成一个元素，而原RDD中的元素经flatmap处理后可生成多个元素来构建新RDD。 举例：对原RDD中的每个元素x产生y个元素（从1到y，y为元素x的值）</span><br><span class="line">scala&gt; val a = sc.parallelize(1 to 4, 2)</span><br><span class="line">scala&gt; val b = a.flatMap(x =&gt; 1 to x)</span><br><span class="line">scala&gt; b.collect</span><br><span class="line">res12: Array[Int] = Array(1, 1, 2, 1, 2, 3, 1, 2, 3, 4)</span><br></pre></td></tr></table></figure></div><h2 id="mapPartitions"><a href="#mapPartitions" class="headerlink" title="mapPartitions"></a>mapPartitions</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">mapPartitions是map的一个变种。map的输入函数是应用于RDD中每个元素，而mapPartitions的输入函数是应用于每个分区，也就是把每个分区中的内容作为整体来处理的。 它的函数定义为：</span><br><span class="line">def mapPartitions[U: ClassTag](f: Iterator[T] =&gt; Iterator[U], preservesPartitioning: Boolean = <span class="literal">false</span>): RDD[U]</span><br><span class="line">f即为输入函数，它处理每个分区里面的内容。每个分区中的内容将以Iterator[T]传递给输入函数f，f的输出结果是Iterator[U]。最终的RDD由所有分区经过输入函数处理后的结果合并起来的。</span><br><span class="line">举例：</span><br><span class="line">scala&gt; val a = sc.parallelize(1 to 9, 3)</span><br><span class="line">scala&gt; def myfunc[T](iter: Iterator[T]) : Iterator[(T, T)] = &#123;</span><br><span class="line">  var res = List[(T, T)]() </span><br><span class="line">  var pre = iter.next </span><br><span class="line"><span class="keyword">while</span> (iter.hasNext) &#123;</span><br><span class="line">    val cur = iter.next</span><br><span class="line">    res.::=(pre, cur)</span><br><span class="line">      pre = cur  &#125; </span><br><span class="line">  res.iterator</span><br><span class="line">&#125;</span><br><span class="line">scala&gt; a.mapPartitions(myfunc).collect</span><br><span class="line">res0: Array[(Int, Int)] = Array((2,3), (1,2), (5,6), (4,5), (8,9), (7,8))</span><br><span class="line">上述例子中的函数myfunc是把分区中一个元素和它的下一个元素组成一个Tuple。因为分区中最后一个元素没有下一个元素了，所以(3,4)和(6,7)不在结果中。 mapPartitions还有些变种，比如mapPartitionsWithContext，它能把处理过程中的一些状态信息传递给用户指定的输入函数。还有mapPartitionsWithIndex，它能把分区的index传递给用户指定的输入函数。</span><br></pre></td></tr></table></figure></div><h2 id="mapPartitionsWithIndex"><a href="#mapPartitionsWithIndex" class="headerlink" title="mapPartitionsWithIndex"></a>mapPartitionsWithIndex</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def mapPartitionsWithIndex[U](f: (Int, Iterator[T]) =&gt; Iterator[U], preservesPartitioning: Boolean = <span class="literal">false</span>)(implicit arg0: ClassTag[U]): RDD[U]</span><br><span class="line">函数作用同mapPartitions，不过提供了两个参数，第一个参数为分区的索引。</span><br><span class="line"></span><br><span class="line">var rdd1 = sc.makeRDD(1 to 5,2)</span><br><span class="line">//rdd1有两个分区</span><br><span class="line">var rdd2 = rdd1.mapPartitionsWithIndex&#123;</span><br><span class="line">        (x,iter) =&gt; &#123;</span><br><span class="line">          var result = List[String]()</span><br><span class="line">            var i = 0</span><br><span class="line">            <span class="keyword">while</span>(iter.hasNext)&#123;</span><br><span class="line">              i += iter.next()</span><br><span class="line">            &#125;</span><br><span class="line">            result.::(x + <span class="string">"|"</span> + i).iterator</span><br><span class="line">           </span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">//rdd2将rdd1中每个分区的数字累加，并在每个分区的累加结果前面加了分区索引</span><br><span class="line">scala&gt; rdd2.collect</span><br><span class="line">res13: Array[String] = Array(0|3, 1|12)</span><br></pre></td></tr></table></figure></div><h2 id="coalesce"><a href="#coalesce" class="headerlink" title="coalesce"></a>coalesce</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def coalesce(numPartitions: Int, shuffle: Boolean = <span class="literal">false</span>)(implicit ord: Ordering[T] = null): RDD[T]</span><br><span class="line">该函数用于将RDD进行重分区，使用HashPartitioner。</span><br><span class="line">第一个参数为重分区的数目，第二个为是否进行shuffle，默认为<span class="literal">false</span>;</span><br><span class="line">以下面的例子来看：</span><br><span class="line">scala&gt; var data = sc.parallelize(1 to 12, 3) </span><br><span class="line">scala&gt; data.collect </span><br><span class="line">scala&gt; data.partitions.size </span><br><span class="line">scala&gt; var rdd1 = data.coalesce(1) </span><br><span class="line">scala&gt; rdd1.partitions.size </span><br><span class="line">scala&gt; var rdd1 = data.coalesce(4) </span><br><span class="line">scala&gt; rdd1.partitions.size</span><br><span class="line">res2: Int = 1   //如果重分区的数目大于原来的分区数，那么必须指定shuffle参数为<span class="literal">true</span>，//否则，分区数不便</span><br><span class="line">scala&gt; var rdd1 = data.coalesce(4,<span class="literal">true</span>) </span><br><span class="line">scala&gt; rdd1.partitions.size</span><br><span class="line">res3: Int = 4</span><br></pre></td></tr></table></figure></div><h2 id="repartition"><a href="#repartition" class="headerlink" title="repartition"></a>repartition</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T]</span><br><span class="line">该函数其实就是coalesce函数第二个参数为<span class="literal">true</span>的实现</span><br><span class="line">scala&gt; var data = sc.parallelize(1 to 12, 3) </span><br><span class="line">scala&gt; data.collect </span><br><span class="line">scala&gt; data.partitions.size </span><br><span class="line">scala&gt; var rdd1 = data. repartition(1) </span><br><span class="line">scala&gt; rdd1.partitions.size </span><br><span class="line">scala&gt; var rdd1 = data. repartition(4) </span><br><span class="line">scala&gt; rdd1.partitions.size</span><br><span class="line">res3: Int = 4</span><br></pre></td></tr></table></figure></div><h2 id="randomSplit"><a href="#randomSplit" class="headerlink" title="randomSplit"></a>randomSplit</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">def randomSplit(weights: Array[Double], seed: Long = Utils.random.nextLong): Array[RDD[T]]</span><br><span class="line">该函数根据weights权重，将一个RDD切分成多个RDD。</span><br><span class="line">该权重参数为一个Double数组</span><br><span class="line">第二个参数为random的种子，基本可忽略。</span><br><span class="line">scala&gt; var rdd = sc.makeRDD(1 to 12,12)</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[16] at makeRDD at :21</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.collect</span><br><span class="line">res6: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)  </span><br><span class="line"> </span><br><span class="line">scala&gt; var splitRDD = rdd.randomSplit(Array(0.5, 0.1, 0.2, 0.2))</span><br><span class="line">splitRDD: Array[org.apache.spark.rdd.RDD[Int]] = Array(MapPartitionsRDD[17] at randomSplit at :23, </span><br><span class="line">MapPartitionsRDD[18] at randomSplit at :23, </span><br><span class="line">MapPartitionsRDD[19] at randomSplit at :23, </span><br><span class="line">MapPartitionsRDD[20] at randomSplit at :23)</span><br><span class="line"> </span><br><span class="line">//这里注意：randomSplit的结果是一个RDD数组</span><br><span class="line">scala&gt; splitRDD.size</span><br><span class="line">res8: Int = 4</span><br><span class="line">//由于randomSplit的第一个参数weights中传入的值有4个，因此，就会切分成4个RDD,</span><br><span class="line">//把原来的rdd按照权重0.5, 0.1, 0.2, 0.2，随机划分到这4个RDD中，权重高的RDD，划分到//的几率就大一些。</span><br><span class="line">//注意，权重的总和加起来为1，否则会不正常 </span><br><span class="line">scala&gt; splitRDD(0).collect</span><br><span class="line">res10: Array[Int] = Array(1, 4)</span><br><span class="line"> </span><br><span class="line">scala&gt; splitRDD(1).collect</span><br><span class="line">res11: Array[Int] = Array(3)                                                    </span><br><span class="line"> </span><br><span class="line">scala&gt; splitRDD(2).collect</span><br><span class="line">res12: Array[Int] = Array(5, 9)</span><br><span class="line"> </span><br><span class="line">scala&gt; splitRDD(3).collect</span><br><span class="line">res13: Array[Int] = Array(2, 6, 7, 8, 10)</span><br></pre></td></tr></table></figure></div><h2 id="glom"><a href="#glom" class="headerlink" title="glom"></a>glom</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def glom(): RDD[Array[T]]</span><br><span class="line">该函数是将RDD中每一个分区中类型为T的元素转换成Array[T]，这样每一个分区就只有一个数组元素。</span><br><span class="line"></span><br><span class="line">scala&gt; var rdd = sc.makeRDD(1 to 10,3)</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[38] at makeRDD at :21</span><br><span class="line">scala&gt; rdd.partitions.size</span><br><span class="line">res33: Int = 3  //该RDD有3个分区</span><br><span class="line">scala&gt; rdd.glom().collect</span><br><span class="line">res35: Array[Array[Int]] = Array(Array(1, 2, 3), Array(4, 5, 6), Array(7, 8, 9, 10))</span><br><span class="line">//glom将每个分区中的元素放到一个数组中，这样，结果就变成了3个数组</span><br></pre></td></tr></table></figure></div><h2 id="union并集"><a href="#union并集" class="headerlink" title="union并集"></a>union并集</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val rdd1 = sc.parallelize(List(5, 6, 4, 3))</span><br><span class="line">val rdd2 = sc.parallelize(List(1, 2, 3, 4))</span><br><span class="line">//求并集</span><br><span class="line">val rdd3 = rdd1.union(rdd2)</span><br><span class="line">rdd3.collect</span><br></pre></td></tr></table></figure></div><h2 id="distinct"><a href="#distinct" class="headerlink" title="distinct"></a>distinct</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">去重</span><br><span class="line">val rdd1 = sc.parallelize(List(5, 6, 4, 3))</span><br><span class="line">val rdd2 = sc.parallelize(List(1, 2, 3, 4))</span><br><span class="line">//求并集</span><br><span class="line">val rdd3 = rdd1.union(rdd2)</span><br><span class="line">//去重输出</span><br><span class="line">rdd3.distinct.collect</span><br></pre></td></tr></table></figure></div><h2 id="intersection交集"><a href="#intersection交集" class="headerlink" title="intersection交集"></a>intersection交集</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val rdd1 = sc.parallelize(List(5, 6, 4, 3))</span><br><span class="line">val rdd2 = sc.parallelize(List(1, 2, 3, 4))</span><br><span class="line">//求交集</span><br><span class="line">val rdd4 = rdd1.intersection(rdd2) </span><br><span class="line">rdd4.collect</span><br></pre></td></tr></table></figure></div><h2 id="subtract"><a href="#subtract" class="headerlink" title="subtract"></a>subtract</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def subtract(other: RDD[T]): RDD[T]</span><br><span class="line">def subtract(other: RDD[T], numPartitions: Int): RDD[T]</span><br><span class="line">def subtract(other: RDD[T], partitioner: Partitioner)(implicit ord: Ordering[T] = null): RDD[T]</span><br><span class="line">该函数返回在RDD中出现，并且不在otherRDD中出现的元素，不去重。</span><br><span class="line"></span><br><span class="line">val rdd1 = sc.parallelize(List(5, 6, 6, 4, 3))</span><br><span class="line">    val rdd2 = sc.parallelize(List(1, 2, 3, 4))</span><br><span class="line">    //求差集</span><br><span class="line">    val rdd4 = rdd1.subtract(rdd2)</span><br><span class="line">rdd4.collect</span><br></pre></td></tr></table></figure></div><h2 id="subtractByKey"><a href="#subtractByKey" class="headerlink" title="subtractByKey"></a>subtractByKey</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def subtractByKey[W](other: RDD[(K, W)])(implicit arg0: ClassTag[W]): RDD[(K, V)]</span><br><span class="line">def subtractByKey[W](other: RDD[(K, W)], numPartitions: Int)(implicit arg0: ClassTag[W]): RDD[(K, V)]</span><br><span class="line">def subtractByKey[W](other: RDD[(K, W)], p: Partitioner)(implicit arg0: ClassTag[W]): RDD[(K, V)]</span><br><span class="line"></span><br><span class="line">subtractByKey和基本转换操作中的subtract类似，只不过这里是针对K的，返回在主RDD中出现，并且不在otherRDD中出现的元素。</span><br><span class="line">参数numPartitions用于指定结果的分区数</span><br><span class="line">参数partitioner用于指定分区函数</span><br><span class="line"></span><br><span class="line">var rdd1 = sc.makeRDD(Array((<span class="string">"A"</span>,<span class="string">"1"</span>),(<span class="string">"B"</span>,<span class="string">"2"</span>),(<span class="string">"C"</span>,<span class="string">"3"</span>)),2)</span><br><span class="line">var rdd2 = sc.makeRDD(Array((<span class="string">"A"</span>,<span class="string">"a"</span>),(<span class="string">"C"</span>,<span class="string">"c"</span>),(<span class="string">"D"</span>,<span class="string">"d"</span>)),2) </span><br><span class="line">scala&gt; rdd1.subtractByKey(rdd2).collect</span><br><span class="line">res13: Array[(String, String)] = Array((B,2))</span><br></pre></td></tr></table></figure></div><h2 id="groupbyKey"><a href="#groupbyKey" class="headerlink" title="groupbyKey"></a>groupbyKey</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">val rdd1 = sc.parallelize(List((<span class="string">"tom"</span>, 1), (<span class="string">"jerry"</span>, 3), (<span class="string">"kitty"</span>, 2)))</span><br><span class="line">    val rdd2 = sc.parallelize(List((<span class="string">"jerry"</span>, 2), (<span class="string">"tom"</span>, 1), (<span class="string">"shuke"</span>, 2)))</span><br><span class="line">    //求并集</span><br><span class="line">    val rdd4 = rdd1 union rdd2</span><br><span class="line">    //按key进行分组</span><br><span class="line">    val rdd5 = rdd4.groupByKey</span><br><span class="line">rdd5.collect</span><br></pre></td></tr></table></figure></div><h2 id="reduceByKey"><a href="#reduceByKey" class="headerlink" title="reduceByKey"></a>reduceByKey</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">顾名思义，reduceByKey就是对元素为KV对的RDD中Key相同的元素的Value进行reduce，因此，Key相同的多个元素的值被reduce为一个值，然后与原RDD中的Key组成一个新的KV对。</span><br><span class="line">举例:</span><br><span class="line">val rdd1 = sc.parallelize(List((<span class="string">"tom"</span>, 1), (<span class="string">"jerry"</span>, 3), (<span class="string">"kitty"</span>, 2)))</span><br><span class="line">    val rdd2 = sc.parallelize(List((<span class="string">"jerry"</span>, 2), (<span class="string">"tom"</span>, 1), (<span class="string">"shuke"</span>, 2)))</span><br><span class="line">    //求并集</span><br><span class="line">    val rdd4 = rdd1 union rdd2</span><br><span class="line">    //按key进行分组</span><br><span class="line">    val rdd6 = rdd4.reduceByKey(_ + _)</span><br><span class="line">    rdd6.collect()</span><br></pre></td></tr></table></figure></div><h2 id="sortByKey"><a href="#sortByKey" class="headerlink" title="sortByKey"></a>sortByKey</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">将List((<span class="string">"tom"</span>, 1), (<span class="string">"jerry"</span>, 3), (<span class="string">"kitty"</span>, 2),  (<span class="string">"shuke"</span>, 1))和List((<span class="string">"jerry"</span>, 2), (<span class="string">"tom"</span>, 3), (<span class="string">"shuke"</span>, 2), (<span class="string">"kitty"</span>, 5))做wordcount，并按名称排序</span><br><span class="line">val rdd1 = sc.parallelize(List((<span class="string">"tom"</span>, 1), (<span class="string">"jerry"</span>, 3), (<span class="string">"kitty"</span>, 2), (<span class="string">"shuke"</span>, 1)))</span><br><span class="line">    val rdd2 = sc.parallelize(List((<span class="string">"jerry"</span>, 2), (<span class="string">"tom"</span>, 3), (<span class="string">"shuke"</span>, 2), (<span class="string">"kitty"</span>, 5)))</span><br><span class="line">    val rdd3 = rdd1.union(rdd2)</span><br><span class="line">    //按key进行聚合</span><br><span class="line">    val rdd4 = rdd3.reduceByKey(_ + _)</span><br><span class="line">    //<span class="literal">false</span>降序</span><br><span class="line">    val rdd5 = rdd4.sortByKey(<span class="literal">false</span>)</span><br><span class="line">rdd5.collect</span><br></pre></td></tr></table></figure></div><h2 id="sortBy"><a href="#sortBy" class="headerlink" title="sortBy"></a>sortBy</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">将List((<span class="string">"tom"</span>, 1), (<span class="string">"jerry"</span>, 3), (<span class="string">"kitty"</span>, 2),  (<span class="string">"shuke"</span>, 1))和List((<span class="string">"jerry"</span>, 2), (<span class="string">"tom"</span>, 3), (<span class="string">"shuke"</span>, 2), (<span class="string">"kitty"</span>, 5))做wordcount，并按数值排序</span><br><span class="line">val rdd1 = sc.parallelize(List((<span class="string">"tom"</span>, 1), (<span class="string">"jerry"</span>, 3), (<span class="string">"kitty"</span>, 2), (<span class="string">"shuke"</span>, 1)))</span><br><span class="line">    val rdd2 = sc.parallelize(List((<span class="string">"jerry"</span>, 2), (<span class="string">"tom"</span>, 3), (<span class="string">"shuke"</span>, 2), (<span class="string">"kitty"</span>, 5)))</span><br><span class="line">    val rdd3 = rdd1.union(rdd2)</span><br><span class="line">    //按key进行聚合</span><br><span class="line">    val rdd4 = rdd3.reduceByKey(_ + _)</span><br><span class="line">    //<span class="literal">false</span>降序</span><br><span class="line">    val rdd5 = rdd4.sortBy(_._2, <span class="literal">false</span>)</span><br><span class="line">    rdd5.collect</span><br></pre></td></tr></table></figure></div><h2 id="zip"><a href="#zip" class="headerlink" title="zip"></a>zip</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">def zip[U](other: RDD[U])(implicit arg0: ClassTag[U]): RDD[(T, U)]</span><br><span class="line"></span><br><span class="line">zip函数用于将两个RDD组合成Key/Value形式的RDD,这里默认两个RDD的partition数量以及元素数量都相同，否则会抛出异常。</span><br><span class="line"></span><br><span class="line">scala&gt; var rdd1 = sc.makeRDD(1 to 5,2)</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at makeRDD at :21</span><br><span class="line"> </span><br><span class="line">scala&gt; var rdd2 = sc.makeRDD(Seq(<span class="string">"A"</span>,<span class="string">"B"</span>,<span class="string">"C"</span>,<span class="string">"D"</span>,<span class="string">"E"</span>),2)</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[2] at makeRDD at :21</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.zip(rdd2).collect</span><br><span class="line">res0: Array[(Int, String)] = Array((1,A), (2,B), (3,C), (4,D), (5,E))           </span><br><span class="line"> </span><br><span class="line">scala&gt; rdd2.zip(rdd1).collect</span><br><span class="line">res1: Array[(String, Int)] = Array((A,1), (B,2), (C,3), (D,4), (E,5))</span><br><span class="line"> </span><br><span class="line">scala&gt; var rdd3 = sc.makeRDD(Seq(<span class="string">"A"</span>,<span class="string">"B"</span>,<span class="string">"C"</span>,<span class="string">"D"</span>,<span class="string">"E"</span>),3)</span><br><span class="line">rdd3: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[5] at makeRDD at :21</span><br><span class="line">scala&gt; rdd1.zip(rdd3).collect</span><br><span class="line">java.lang.IllegalArgumentException: Can<span class="string">'t zip RDDs with unequal numbers of partitions</span></span><br><span class="line"><span class="string">//如果两个RDD分区数不同，则抛出异常</span></span><br></pre></td></tr></table></figure></div><h2 id="zipPartitions"><a href="#zipPartitions" class="headerlink" title="zipPartitions"></a>zipPartitions</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line">zipPartitions函数将多个RDD按照partition组合成为新的RDD，该函数需要组合的RDD具有相同的分区数，但对于每个分区内的元素数量没有要求。</span><br><span class="line">该函数有好几种实现，可分为三类：</span><br><span class="line"></span><br><span class="line">参数是一个RDD</span><br><span class="line">def zipPartitions[B, V](rdd2: RDD[B])(f: (Iterator[T], Iterator[B]) =&gt; Iterator[V])(implicit arg0: ClassTag[B], arg1: ClassTag[V]): RDD[V]</span><br><span class="line"></span><br><span class="line">def zipPartitions[B, V](rdd2: RDD[B], preservesPartitioning: Boolean)(f: (Iterator[T], Iterator[B]) =&gt; Iterator[V])(implicit arg0: ClassTag[B], arg1: ClassTag[V]): RDD[V]</span><br><span class="line"></span><br><span class="line">这两个区别就是参数preservesPartitioning，是否保留父RDD的partitioner分区信息</span><br><span class="line"></span><br><span class="line">映射方法f参数为两个RDD的迭代器。</span><br><span class="line"></span><br><span class="line">scala&gt; var rdd1 = sc.makeRDD(1 to 5,2)</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[22] at makeRDD at :21</span><br><span class="line"> </span><br><span class="line">scala&gt; var rdd2 = sc.makeRDD(Seq(<span class="string">"A"</span>,<span class="string">"B"</span>,<span class="string">"C"</span>,<span class="string">"D"</span>,<span class="string">"E"</span>),2)</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[23] at makeRDD at :21</span><br><span class="line"> </span><br><span class="line">//rdd1两个分区中元素分布：</span><br><span class="line">scala&gt; rdd1.mapPartitionsWithIndex&#123;</span><br><span class="line">     |         (x,iter) =&gt; &#123;</span><br><span class="line">     |           var result = List[String]()</span><br><span class="line">     |             <span class="keyword">while</span>(iter.hasNext)&#123;</span><br><span class="line">     |               result ::= (<span class="string">"part_"</span> + x + <span class="string">"|"</span> + iter.next())</span><br><span class="line">     |             &#125;</span><br><span class="line">     |             result.iterator</span><br><span class="line">     |            </span><br><span class="line">     |         &#125;</span><br><span class="line">     |       &#125;.collect</span><br><span class="line">res17: Array[String] = Array(part_0|2, part_0|1, part_1|5, part_1|4, part_1|3)</span><br><span class="line"> </span><br><span class="line">//rdd2两个分区中元素分布</span><br><span class="line">scala&gt; rdd2.mapPartitionsWithIndex&#123;</span><br><span class="line">     |         (x,iter) =&gt; &#123;</span><br><span class="line">     |           var result = List[String]()</span><br><span class="line">     |             <span class="keyword">while</span>(iter.hasNext)&#123;</span><br><span class="line">     |               result ::= (<span class="string">"part_"</span> + x + <span class="string">"|"</span> + iter.next())</span><br><span class="line">     |             &#125;</span><br><span class="line">     |             result.iterator</span><br><span class="line">     |            </span><br><span class="line">     |         &#125;</span><br><span class="line">     |       &#125;.collect</span><br><span class="line">res18: Array[String] = Array(part_0|B, part_0|A, part_1|E, part_1|D, part_1|C)</span><br><span class="line"> </span><br><span class="line">//rdd1和rdd2做zipPartition</span><br><span class="line">scala&gt; rdd1.zipPartitions(rdd2)&#123;</span><br><span class="line">     |       (rdd1Iter,rdd2Iter) =&gt; &#123;</span><br><span class="line">     |         var result = List[String]()</span><br><span class="line">     |         <span class="keyword">while</span>(rdd1Iter.hasNext &amp;&amp; rdd2Iter.hasNext) &#123;</span><br><span class="line">     |           result::=(rdd1Iter.next() + <span class="string">"_"</span> + rdd2Iter.next())</span><br><span class="line">     |         &#125;</span><br><span class="line">     |         result.iterator</span><br><span class="line">     |       &#125;</span><br><span class="line">     |     &#125;.collect</span><br><span class="line">res19: Array[String] = Array(2_B, 1_A, 5_E, 4_D, 3_C)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">参数是两个RDD</span><br><span class="line">def zipPartitions[B, C, V](rdd2: RDD[B], rdd3: RDD[C])(f: (Iterator[T], Iterator[B], Iterator[C]) =&gt; Iterator[V])(implicit arg0: ClassTag[B], arg1: ClassTag[C], arg2: ClassTag[V]): RDD[V]</span><br><span class="line"></span><br><span class="line">def zipPartitions[B, C, V](rdd2: RDD[B], rdd3: RDD[C], preservesPartitioning: Boolean)(f: (Iterator[T], Iterator[B], Iterator[C]) =&gt; Iterator[V])(implicit arg0: ClassTag[B], arg1: ClassTag[C], arg2: ClassTag[V]): RDD[V]</span><br><span class="line"></span><br><span class="line">用法同上面，只不过该函数参数为两个RDD，映射方法f输入参数为两个RDD的迭代器。</span><br><span class="line"></span><br><span class="line">scala&gt; var rdd1 = sc.makeRDD(1 to 5,2)</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[27] at makeRDD at :21</span><br><span class="line"> </span><br><span class="line">scala&gt; var rdd2 = sc.makeRDD(Seq(<span class="string">"A"</span>,<span class="string">"B"</span>,<span class="string">"C"</span>,<span class="string">"D"</span>,<span class="string">"E"</span>),2)</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[28] at makeRDD at :21</span><br><span class="line"> </span><br><span class="line">scala&gt; var rdd3 = sc.makeRDD(Seq(<span class="string">"a"</span>,<span class="string">"b"</span>,<span class="string">"c"</span>,<span class="string">"d"</span>,<span class="string">"e"</span>),2)</span><br><span class="line">rdd3: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[29] at makeRDD at :21</span><br><span class="line"> </span><br><span class="line">//rdd3中个分区元素分布</span><br><span class="line">scala&gt; rdd3.mapPartitionsWithIndex&#123;</span><br><span class="line">     |         (x,iter) =&gt; &#123;</span><br><span class="line">     |           var result = List[String]()</span><br><span class="line">     |             <span class="keyword">while</span>(iter.hasNext)&#123;</span><br><span class="line">     |               result ::= (<span class="string">"part_"</span> + x + <span class="string">"|"</span> + iter.next())</span><br><span class="line">     |             &#125;</span><br><span class="line">     |             result.iterator</span><br><span class="line">     |            </span><br><span class="line">     |         &#125;</span><br><span class="line">     |       &#125;.collect</span><br><span class="line">res21: Array[String] = Array(part_0|b, part_0|a, part_1|e, part_1|d, part_1|c)</span><br><span class="line"> </span><br><span class="line">//三个RDD做zipPartitions</span><br><span class="line">scala&gt; var rdd4 = rdd1.zipPartitions(rdd2,rdd3)&#123;</span><br><span class="line">     |       (rdd1Iter,rdd2Iter,rdd3Iter) =&gt; &#123;</span><br><span class="line">     |         var result = List[String]()</span><br><span class="line">     |         <span class="keyword">while</span>(rdd1Iter.hasNext &amp;&amp; rdd2Iter.hasNext &amp;&amp; rdd3Iter.hasNext) &#123;</span><br><span class="line">     |           result::=(rdd1Iter.next() + <span class="string">"_"</span> + rdd2Iter.next() + <span class="string">"_"</span> + rdd3Iter.next())</span><br><span class="line">     |         &#125;</span><br><span class="line">     |         result.iterator</span><br><span class="line">     |       &#125;</span><br><span class="line">     |     &#125;</span><br><span class="line">rdd4: org.apache.spark.rdd.RDD[String] = ZippedPartitionsRDD3[33] at zipPartitions at :27</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd4.collect</span><br><span class="line">res23: Array[String] = Array(2_B_b, 1_A_a, 5_E_e, 4_D_d, 3_C_c)</span><br><span class="line"> </span><br><span class="line">参数是三个RDD</span><br><span class="line">def zipPartitions[B, C, D, V](rdd2: RDD[B], rdd3: RDD[C], rdd4: RDD[D])(f: (Iterator[T], Iterator[B], Iterator[C], Iterator[D]) =&gt; Iterator[V])(implicit arg0: ClassTag[B], arg1: ClassTag[C], arg2: ClassTag[D], arg3: ClassTag[V]): RDD[V]</span><br><span class="line"></span><br><span class="line">def zipPartitions[B, C, D, V](rdd2: RDD[B], rdd3: RDD[C], rdd4: RDD[D], preservesPartitioning: Boolean)(f: (Iterator[T], Iterator[B], Iterator[C], Iterator[D]) =&gt; Iterator[V])(implicit arg0: ClassTag[B], arg1: ClassTag[C], arg2: ClassTag[D], arg3: ClassTag[V]): RDD[V]</span><br><span class="line"></span><br><span class="line">用法同上面，只不过这里又多了个一个RDD而已。</span><br></pre></td></tr></table></figure></div><h2 id="zipWithIndex"><a href="#zipWithIndex" class="headerlink" title="zipWithIndex"></a>zipWithIndex</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def zipWithIndex(): RDD[(T, Long)]</span><br><span class="line">该函数将RDD中的元素和这个元素在RDD中的ID（索引号）组合成键/值对。</span><br><span class="line">scala&gt; var rdd2 = sc.makeRDD(Seq(<span class="string">"A"</span>,<span class="string">"B"</span>,<span class="string">"R"</span>,<span class="string">"D"</span>,<span class="string">"F"</span>),2)</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[34] at makeRDD at :21</span><br><span class="line">scala&gt; rdd2.zipWithIndex().collect</span><br><span class="line">res27: Array[(String, Long)] = Array((A,0), (B,1), (R,2), (D,3), (F,4))</span><br></pre></td></tr></table></figure></div><h2 id="zipWithUniqueId"><a href="#zipWithUniqueId" class="headerlink" title="zipWithUniqueId"></a>zipWithUniqueId</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">def zipWithUniqueId(): RDD[(T, Long)]</span><br><span class="line">该函数将RDD中元素和一个唯一ID组合成键/值对，该唯一ID生成算法如下：</span><br><span class="line">每个分区中第一个元素的唯一ID值为：该分区索引号，</span><br><span class="line">每个分区中第N个元素的唯一ID值为：(前一个元素的唯一ID值) + (该RDD总的分区数)</span><br><span class="line">看下面的例子：</span><br><span class="line">scala&gt; var rdd1 = sc.makeRDD(Seq(<span class="string">"A"</span>,<span class="string">"B"</span>,<span class="string">"C"</span>,<span class="string">"D"</span>,<span class="string">"E"</span>,<span class="string">"F"</span>),2)</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[44] at makeRDD at :21</span><br><span class="line">//rdd1有两个分区，</span><br><span class="line">scala&gt; rdd1.zipWithUniqueId().collect</span><br><span class="line">res32: Array[(String, Long)] = Array((A,0), (B,2), (C,4), (D,1), (E,3), (F,5))</span><br><span class="line">//总分区数为2</span><br><span class="line">//第一个分区第一个元素ID为0，第二个分区第一个元素ID为1</span><br><span class="line">//第一个分区第二个元素ID为0+2=2，第一个分区第三个元素ID为2+2=4</span><br><span class="line">//第二个分区第二个元素ID为1+2=3，第二个分区第三个元素ID为3+2=5</span><br></pre></td></tr></table></figure></div><h1 id="键值转换"><a href="#键值转换" class="headerlink" title="键值转换"></a>键值转换</h1><h2 id="partitionBy"><a href="#partitionBy" class="headerlink" title="partitionBy"></a>partitionBy</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">def partitionBy(partitioner: Partitioner): RDD[(K, V)]</span><br><span class="line">该函数根据partitioner函数生成新的ShuffleRDD，将原RDD重新分区。</span><br><span class="line">scala&gt; var rdd1 = sc.makeRDD(Array((1,<span class="string">"A"</span>),(2,<span class="string">"B"</span>),(3,<span class="string">"C"</span>),(4,<span class="string">"D"</span>)),2)</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[23] at makeRDD at :21</span><br><span class="line">scala&gt; rdd1.partitions.size</span><br><span class="line">res20: Int = 2</span><br><span class="line"> </span><br><span class="line">//查看rdd1中每个分区的元素</span><br><span class="line">scala&gt; rdd1.mapPartitionsWithIndex&#123;</span><br><span class="line">     |         (partIdx,iter) =&gt; &#123;</span><br><span class="line">     |           var part_map = scala.collection.mutable.Map[String,List[(Int,String)]]()</span><br><span class="line">     |             <span class="keyword">while</span>(iter.hasNext)&#123;</span><br><span class="line">     |               var part_name = <span class="string">"part_"</span> + partIdx;</span><br><span class="line">     |               var elem = iter.next()</span><br><span class="line">     |               <span class="keyword">if</span>(part_map.contains(part_name)) &#123;</span><br><span class="line">     |                 var elems = part_map(part_name)</span><br><span class="line">     |                 elems ::= elem</span><br><span class="line">     |                 part_map(part_name) = elems</span><br><span class="line">     |               &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">     |                 part_map(part_name) = List[(Int,String)]&#123;elem&#125;</span><br><span class="line">     |               &#125;</span><br><span class="line">     |             &#125;</span><br><span class="line">     |             part_map.iterator</span><br><span class="line">     |            </span><br><span class="line">     |         &#125;</span><br><span class="line">     |       &#125;.collect</span><br><span class="line">res22: Array[(String, List[(Int, String)])] = Array((part_0,List((2,B), (1,A))), (part_1,List((4,D), (3,C))))</span><br><span class="line">//(2,B),(1,A)在part_0中，(4,D),(3,C)在part_1中</span><br><span class="line"> </span><br><span class="line">//使用partitionBy重分区</span><br><span class="line">scala&gt; var rdd2 = rdd1.partitionBy(new org.apache.spark.HashPartitioner(2))</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[(Int, String)] = ShuffledRDD[25] at partitionBy at :23</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd2.partitions.size</span><br><span class="line">res23: Int = 2</span><br><span class="line"> </span><br><span class="line">//查看rdd2中每个分区的元素</span><br><span class="line">scala&gt; rdd2.mapPartitionsWithIndex&#123;</span><br><span class="line">     |         (partIdx,iter) =&gt; &#123;</span><br><span class="line">     |           var part_map = scala.collection.mutable.Map[String,List[(Int,String)]]()</span><br><span class="line">     |             <span class="keyword">while</span>(iter.hasNext)&#123;</span><br><span class="line">     |               var part_name = <span class="string">"part_"</span> + partIdx;</span><br><span class="line">     |               var elem = iter.next()</span><br><span class="line">     |               <span class="keyword">if</span>(part_map.contains(part_name)) &#123;</span><br><span class="line">     |                 var elems = part_map(part_name)</span><br><span class="line">     |                 elems ::= elem</span><br><span class="line">     |                 part_map(part_name) = elems</span><br><span class="line">     |               &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">     |                 part_map(part_name) = List[(Int,String)]&#123;elem&#125;</span><br><span class="line">     |               &#125;</span><br><span class="line">     |             &#125;</span><br><span class="line">     |             part_map.iterator</span><br><span class="line">     |         &#125;</span><br><span class="line">     |       &#125;.collect</span><br><span class="line">res24: Array[(String, List[(Int, String)])] = Array((part_0,List((4,D), (2,B))), (part_1,List((3,C), (1,A))))</span><br><span class="line">//(4,D),(2,B)在part_0中，(3,C),(1,A)在part_1中</span><br></pre></td></tr></table></figure></div><h2 id="mapValues"><a href="#mapValues" class="headerlink" title="mapValues"></a>mapValues</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mapValues顾名思义就是输入函数应用于RDD中Kev-Value的Value，原RDD中的Key保持不变，与新的Value一起组成新的RDD中的元素。因此，该函数只适用于元素为KV对的RDD。</span><br><span class="line">举例：</span><br><span class="line">scala&gt; val a = sc.parallelize(List(<span class="string">"dog"</span>, <span class="string">"tiger"</span>, <span class="string">"lion"</span>, <span class="string">"cat"</span>, <span class="string">"panther"</span>, <span class="string">" eagle"</span>), 2)</span><br><span class="line">scala&gt; val b = a.map(x =&gt; (x.length, x))</span><br><span class="line">scala&gt; b.mapValues(<span class="string">"x"</span> + _ + <span class="string">"x"</span>).collect</span><br><span class="line">res5: Array[(Int, String)] = Array((3,xdogx), (5,xtigerx), (4,xlionx),(3,xcatx), (7,xpantherx), (5,xeaglex))</span><br></pre></td></tr></table></figure></div><h2 id="flatMapValues"><a href="#flatMapValues" class="headerlink" title="flatMapValues"></a>flatMapValues</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">flatMapValues类似于mapValues，不同的在于flatMapValues应用于元素为KV对的RDD中Value。每个一元素的Value被输入函数映射为一系列的值，然后这些值再与原RDD中的Key组成一系列新的KV对。</span><br><span class="line">举例</span><br><span class="line">val a = sc.parallelize(List((1, 2), (3, 4), (5, 6)))</span><br><span class="line">    val b = a.flatMapValues(x =&gt; 1.to(x))</span><br><span class="line">    b.collect.foreach(println)</span><br></pre></td></tr></table></figure></div><h2 id="combineByKey"><a href="#combineByKey" class="headerlink" title="combineByKey"></a>combineByKey</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">def combineByKey[C](createCombiner: (V) =&gt; C, mergeValue: (C, V) =&gt; C, mergeCombiners: (C, C) =&gt; C): RDD[(K, C)]</span><br><span class="line">def combineByKey[C](createCombiner: (V) =&gt; C, mergeValue: (C, V) =&gt; C, mergeCombiners: (C, C) =&gt; C, numPartitions: Int): RDD[(K, C)]</span><br><span class="line">def combineByKey[C](createCombiner: (V) =&gt; C, mergeValue: (C, V) =&gt; C, mergeCombiners: (C, C) =&gt; C, partitioner: Partitioner, mapSideCombine: Boolean = <span class="literal">true</span>, serializer: Serializer = null): RDD[(K, C)]</span><br><span class="line">该函数用于将RDD[K,V]转换成RDD[K,C],这里的V类型和C类型可以相同也可以不同。</span><br><span class="line">其中的参数：</span><br><span class="line">createCombiner：组合器函数，用于将V类型转换成C类型，输入参数为RDD[K,V]中的V,输出为C ,分区内相同的key做一次</span><br><span class="line">mergeValue：合并值函数，将一个C类型和一个V类型值合并成一个C类型，输入参数为(C,V)，输出为C，分区内相同的key循环做</span><br><span class="line">mergeCombiners：分区合并组合器函数，用于将两个C类型值合并成一个C类型，输入参数为(C,C)，输出为C，分区之间循环做</span><br><span class="line">numPartitions：结果RDD分区数，默认保持原有的分区数</span><br><span class="line">partitioner：分区函数,默认为HashPartitioner</span><br><span class="line">mapSideCombine：是否需要在Map端进行combine操作，类似于MapReduce中的combine，默认为<span class="literal">true</span></span><br><span class="line"></span><br><span class="line">看下面例子：</span><br><span class="line">scala&gt; var rdd1 = sc.makeRDD(Array((<span class="string">"A"</span>,1),(<span class="string">"A"</span>,2),(<span class="string">"B"</span>,1),(<span class="string">"B"</span>,2),(<span class="string">"C"</span>,1)))</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[64] at makeRDD at :21 </span><br><span class="line">scala&gt; rdd1.combineByKey(</span><br><span class="line">     |       (v : Int) =&gt; v + <span class="string">"_"</span>,   </span><br><span class="line">     |       (c : String, v : Int) =&gt; c + <span class="string">"@"</span> + v,  </span><br><span class="line">     |       (c1 : String, c2 : String) =&gt; c1 + <span class="string">"$"</span> + c2</span><br><span class="line">     |     ).collect</span><br><span class="line">res60: Array[(String, String)] = Array((A,2_<span class="variable">$1_</span>), (B,1_<span class="variable">$2_</span>), (C,1_))</span><br><span class="line"></span><br><span class="line">其中三个映射函数分别为：</span><br><span class="line">createCombiner: (V) =&gt; C</span><br><span class="line">(v : Int) =&gt; v + “_” //在每一个V值后面加上字符_，返回C类型(String)</span><br><span class="line">mergeValue: (C, V) =&gt; C</span><br><span class="line">(c : String, v : Int) =&gt; c + “@” + v //合并C类型和V类型，中间加字符@,返回C(String)</span><br><span class="line">mergeCombiners: (C, C) =&gt; C</span><br><span class="line">(c1 : String, c2 : String) =&gt; c1 + “$” + c2 //合并C类型和C类型，中间加$，返回C(String)</span><br><span class="line">其他参数为默认值。</span><br><span class="line">最终，将RDD[String,Int]转换为RDD[String,String]。</span><br><span class="line"></span><br><span class="line">再看例子：</span><br><span class="line"></span><br><span class="line">rdd1.combineByKey(</span><br><span class="line">      (v : Int) =&gt; List(v),</span><br><span class="line">      (c : List[Int], v : Int) =&gt; v :: c,</span><br><span class="line">      (c1 : List[Int], c2 : List[Int]) =&gt; c1 ::: c2</span><br><span class="line">).collect</span><br><span class="line">res65: Array[(String, List[Int])] = Array((A,List(2, 1)), (B,List(2, 1)), (C,List(1)))</span><br><span class="line">最终将RDD[String,Int]转换为RDD[String,List[Int]]。</span><br></pre></td></tr></table></figure></div><h2 id="foldByKey"><a href="#foldByKey" class="headerlink" title="foldByKey"></a>foldByKey</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">def foldByKey(zeroValue: V)(func: (V, V) =&gt; V): RDD[(K, V)]</span><br><span class="line"></span><br><span class="line">def foldByKey(zeroValue: V, numPartitions: Int)(func: (V, V) =&gt; V): RDD[(K, V)]</span><br><span class="line"></span><br><span class="line">def foldByKey(zeroValue: V, partitioner: Partitioner)(func: (V, V) =&gt; V): RDD[(K, V)]</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">该函数用于RDD[K,V]根据K将V做折叠、合并处理，其中的参数zeroValue表示先根据映射函数将zeroValue应用于V,进行初始化V,再将映射函数应用于初始化后的V.</span><br><span class="line"></span><br><span class="line">例子：</span><br><span class="line"></span><br><span class="line">scala&gt; var rdd1 = sc.makeRDD(Array((<span class="string">"A"</span>,0),(<span class="string">"A"</span>,2),(<span class="string">"B"</span>,1),(<span class="string">"B"</span>,2),(<span class="string">"C"</span>,1)))</span><br><span class="line">scala&gt; rdd1.foldByKey(0)(_+_).collect</span><br><span class="line">res75: Array[(String, Int)] = Array((A,2), (B,3), (C,1)) </span><br><span class="line">//将rdd1中每个key对应的V进行累加，注意zeroValue=0,需要先初始化V,映射函数为+操</span><br><span class="line">//作，比如(<span class="string">"A"</span>,0), (<span class="string">"A"</span>,2)，先将zeroValue应用于每个V,得到：(<span class="string">"A"</span>,0+0), (<span class="string">"A"</span>,2+0)，即：</span><br><span class="line">//(<span class="string">"A"</span>,0), (<span class="string">"A"</span>,2)，再将映射函数应用于初始化后的V，最后得到(A,0+2),即(A,2)</span><br><span class="line"> </span><br><span class="line">再看：</span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.foldByKey(2)(_+_).collect</span><br><span class="line">res76: Array[(String, Int)] = Array((A,6), (B,7), (C,3))</span><br><span class="line">//先将zeroValue=2应用于每个V,得到：(<span class="string">"A"</span>,0+2), (<span class="string">"A"</span>,2+2)，即：(<span class="string">"A"</span>,2), (<span class="string">"A"</span>,4)，再将映射函</span><br><span class="line">//数应用于初始化后的V，最后得到：(A,2+4)，即：(A,6)</span><br><span class="line"> </span><br><span class="line">再看乘法操作：</span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.foldByKey(0)(_*_).collect</span><br><span class="line">res77: Array[(String, Int)] = Array((A,0), (B,0), (C,0))</span><br><span class="line">//先将zeroValue=0应用于每个V,注意，这次映射函数为乘法，得到：(<span class="string">"A"</span>,0*0), (<span class="string">"A"</span>,2*0)，</span><br><span class="line">//即：(<span class="string">"A"</span>,0), (<span class="string">"A"</span>,0)，再将映射函//数应用于初始化后的V，最后得到：(A,0*0)，即：(A,0)</span><br><span class="line">//其他K也一样，最终都得到了V=0</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.foldByKey(1)(_*_).collect</span><br><span class="line">res78: Array[(String, Int)] = Array((A,0), (B,2), (C,1))</span><br><span class="line">//映射函数为乘法时，需要将zeroValue设为1，才能得到我们想要的结果。</span><br><span class="line"> </span><br><span class="line">在使用foldByKey算子时候，要特别注意映射函数及zeroValue的取值。</span><br></pre></td></tr></table></figure></div><h2 id="reduceByKeyLocally"><a href="#reduceByKeyLocally" class="headerlink" title="reduceByKeyLocally"></a>reduceByKeyLocally</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def reduceByKeyLocally(func: (V, V) =&gt; V): Map[K, V]</span><br><span class="line"></span><br><span class="line">该函数将RDD[K,V]中每个K对应的V值根据映射函数来运算，运算结果映射到一个Map[K,V]中，而不是RDD[K,V]。</span><br><span class="line"></span><br><span class="line">scala&gt; var rdd1 = sc.makeRDD(Array((<span class="string">"A"</span>,0),(<span class="string">"A"</span>,2),(<span class="string">"B"</span>,1),(<span class="string">"B"</span>,2),(<span class="string">"C"</span>,1)))</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[91] at makeRDD at :21</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.reduceByKeyLocally((x,y) =&gt; x + y)</span><br><span class="line">res90: scala.collection.Map[String,Int] = Map(B -&gt; 3, A -&gt; 2, C -&gt; 1)</span><br></pre></td></tr></table></figure></div><h2 id="cogroup和groupByKey的区别"><a href="#cogroup和groupByKey的区别" class="headerlink" title="***cogroup和groupByKey的区别"></a>***cogroup和groupByKey的区别</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">val rdd1 = sc.parallelize(List((<span class="string">"tom"</span>, 1), (<span class="string">"tom"</span>, 2), (<span class="string">"jerry"</span>, 3), (<span class="string">"kitty"</span>, 2)))</span><br><span class="line">   val rdd2 = sc.parallelize(List((<span class="string">"jerry"</span>, 2), (<span class="string">"tom"</span>, 1), (<span class="string">"shuke"</span>, 2)))</span><br><span class="line">   //cogroup</span><br><span class="line">   val rdd3 = rdd1.cogroup(rdd2)</span><br><span class="line">   //groupbykey</span><br><span class="line">   val rdd4 = rdd1.union(rdd2).groupByKey</span><br><span class="line">   //注意cogroup与groupByKey的区别</span><br><span class="line">   rdd3.foreach(println)</span><br><span class="line">   rdd4.foreach(println)</span><br></pre></td></tr></table></figure></div><h2 id="join"><a href="#join" class="headerlink" title="join"></a>join</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val rdd1 = sc.parallelize(List((<span class="string">"tom"</span>, 1), (<span class="string">"jerry"</span>, 3), (<span class="string">"kitty"</span>, 2)))</span><br><span class="line">val rdd2 = sc.parallelize(List((<span class="string">"jerry"</span>, 2), (<span class="string">"tom"</span>, 1), (<span class="string">"shuke"</span>, 2)))</span><br><span class="line">//求jion</span><br><span class="line">val rdd3 = rdd1.join(rdd2)</span><br><span class="line">rdd3.collect</span><br></pre></td></tr></table></figure></div><h2 id="leftOuterJoin"><a href="#leftOuterJoin" class="headerlink" title="leftOuterJoin"></a>leftOuterJoin</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def leftOuterJoin[W](other: RDD[(K, W)]): RDD[(K, (V, Option[W]))]</span><br><span class="line">def leftOuterJoin[W](other: RDD[(K, W)], numPartitions: Int): RDD[(K, (V, Option[W]))]</span><br><span class="line">def leftOuterJoin[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (V, Option[W]))]</span><br><span class="line"></span><br><span class="line">leftOuterJoin类似于SQL中的左外关联left outer join，返回结果以前面的RDD为主，关联不上的记录为空。只能用于两个RDD之间的关联，如果要多个RDD关联，多关联几次即可。</span><br><span class="line">参数numPartitions用于指定结果的分区数</span><br><span class="line">参数partitioner用于指定分区函数</span><br><span class="line"></span><br><span class="line">var rdd1 = sc.makeRDD(Array((<span class="string">"A"</span>,<span class="string">"1"</span>),(<span class="string">"B"</span>,<span class="string">"2"</span>),(<span class="string">"C"</span>,<span class="string">"3"</span>)),2)</span><br><span class="line">var rdd2 = sc.makeRDD(Array((<span class="string">"A"</span>,<span class="string">"a"</span>),(<span class="string">"C"</span>,<span class="string">"c"</span>),(<span class="string">"D"</span>,<span class="string">"d"</span>)),2)</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.leftOuterJoin(rdd2).collect</span><br><span class="line">res11: Array[(String, (String, Option[String]))] = Array((B,(2,None)), (A,(1,Some(a))), (C,(3,Some(c))))</span><br></pre></td></tr></table></figure></div><h2 id="rightOuterJoin"><a href="#rightOuterJoin" class="headerlink" title="rightOuterJoin"></a>rightOuterJoin</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def rightOuterJoin[W](other: RDD[(K, W)]): RDD[(K, (Option[V], W))]</span><br><span class="line">def rightOuterJoin[W](other: RDD[(K, W)], numPartitions: Int): RDD[(K, (Option[V], W))]</span><br><span class="line">def rightOuterJoin[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (Option[V], W))] </span><br><span class="line"></span><br><span class="line">rightOuterJoin类似于SQL中的有外关联right outer join，返回结果以参数中的RDD为主，关联不上的记录为空。只能用于两个RDD之间的关联，如果要多个RDD关联，多关联几次即可。</span><br><span class="line">参数numPartitions用于指定结果的分区数</span><br><span class="line">参数partitioner用于指定分区函数</span><br><span class="line"></span><br><span class="line">var rdd1 = sc.makeRDD(Array((<span class="string">"A"</span>,<span class="string">"1"</span>),(<span class="string">"B"</span>,<span class="string">"2"</span>),(<span class="string">"C"</span>,<span class="string">"3"</span>)),2)</span><br><span class="line">var rdd2 = sc.makeRDD(Array((<span class="string">"A"</span>,<span class="string">"a"</span>),(<span class="string">"C"</span>,<span class="string">"c"</span>),(<span class="string">"D"</span>,<span class="string">"d"</span>)),2)</span><br><span class="line">scala&gt; rdd1.rightOuterJoin(rdd2).collect</span><br><span class="line">res12: Array[(String, (Option[String], String))] = Array((D,(None,d)), (A,(Some(1),a)), (C,(Some(3),c)))</span><br></pre></td></tr></table></figure></div><h1 id="Action操作"><a href="#Action操作" class="headerlink" title="Action操作"></a>Action操作</h1><h2 id="first"><a href="#first" class="headerlink" title="first"></a>first</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def first(): T</span><br><span class="line"></span><br><span class="line">first返回RDD中的第一个元素，不排序。</span><br><span class="line"></span><br><span class="line">scala&gt; var rdd1 = sc.makeRDD(Array((<span class="string">"A"</span>,<span class="string">"1"</span>),(<span class="string">"B"</span>,<span class="string">"2"</span>),(<span class="string">"C"</span>,<span class="string">"3"</span>)),2)</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[33] at makeRDD at :21</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.first</span><br><span class="line">res14: (String, String) = (A,1)</span><br><span class="line"> </span><br><span class="line">scala&gt; var rdd1 = sc.makeRDD(Seq(10, 4, 2, 12, 3))</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at makeRDD at :21</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.first</span><br><span class="line">res8: Int = 10</span><br></pre></td></tr></table></figure></div><h2 id="count"><a href="#count" class="headerlink" title="count"></a>count</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def count(): Long</span><br><span class="line"></span><br><span class="line">count返回RDD中的元素数量。</span><br><span class="line"></span><br><span class="line">scala&gt; var rdd1 = sc.makeRDD(Array((<span class="string">"A"</span>,<span class="string">"1"</span>),(<span class="string">"B"</span>,<span class="string">"2"</span>),(<span class="string">"C"</span>,<span class="string">"3"</span>)),2)</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[34] at makeRDD at :21</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.count</span><br><span class="line">res15: Long = 3</span><br></pre></td></tr></table></figure></div><h2 id="reduce"><a href="#reduce" class="headerlink" title="reduce"></a>reduce</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">def reduce(f: (T, T) ⇒ T): T</span><br><span class="line"></span><br><span class="line">根据映射函数f，对RDD中的元素进行二元计算，返回计算结果。</span><br><span class="line"></span><br><span class="line">scala&gt; var rdd1 = sc.makeRDD(1 to 10,2)</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[36] at makeRDD at :21</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.reduce(_ + _)</span><br><span class="line">res18: Int = 55</span><br><span class="line"> </span><br><span class="line">scala&gt; var rdd2 = sc.makeRDD(Array((<span class="string">"A"</span>,0),(<span class="string">"A"</span>,2),(<span class="string">"B"</span>,1),(<span class="string">"B"</span>,2),(<span class="string">"C"</span>,1)))</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[38] at makeRDD at :21</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd2.reduce((x,y) =&gt; &#123;</span><br><span class="line">     |       (x._1 + y._1,x._2 + y._2)</span><br><span class="line">     |     &#125;)</span><br><span class="line">res21: (String, Int) = (CBBAA,6)</span><br><span class="line"> </span><br><span class="line">collect</span><br><span class="line"></span><br><span class="line">def collect(): Array[T]</span><br><span class="line"></span><br><span class="line">collect用于将一个RDD转换成数组。</span><br><span class="line"></span><br><span class="line">scala&gt; var rdd1 = sc.makeRDD(1 to 10,2)</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[36] at makeRDD at :21</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.collect</span><br><span class="line">res23: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)</span><br></pre></td></tr></table></figure></div><h2 id="take"><a href="#take" class="headerlink" title="take"></a>take</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def take(num: Int): Array[T]</span><br><span class="line"></span><br><span class="line">take用于获取RDD中从0到num-1下标的元素，不排序。</span><br><span class="line"></span><br><span class="line">scala&gt; var rdd1 = sc.makeRDD(Seq(10, 4, 2, 12, 3))</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[40] at makeRDD at :21</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.take(1)</span><br><span class="line">res0: Array[Int] = Array(10)                                                    </span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.take(2)</span><br><span class="line">res1: Array[Int] = Array(10, 4)</span><br></pre></td></tr></table></figure></div><h2 id="top"><a href="#top" class="headerlink" title="top"></a>top</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">def top(num: Int)(implicit ord: Ordering[T]): Array[T]</span><br><span class="line"></span><br><span class="line">top函数用于从RDD中，按照默认（降序）或者指定的排序规则，返回前num个元素。</span><br><span class="line"></span><br><span class="line">scala&gt; var rdd1 = sc.makeRDD(Seq(10, 4, 2, 12, 3))</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[40] at makeRDD at :21</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.top(1)</span><br><span class="line">res2: Array[Int] = Array(12)</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.top(2)</span><br><span class="line">res3: Array[Int] = Array(12, 10)</span><br><span class="line"> </span><br><span class="line">//指定排序规则</span><br><span class="line">scala&gt; implicit val myOrd = implicitly[Ordering[Int]].reverse</span><br><span class="line">myOrd: scala.math.Ordering[Int] = scala.math.Ordering$<span class="variable">$anon</span><span class="variable">$4</span>@767499ef</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.top(1)</span><br><span class="line">res4: Array[Int] = Array(2)</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.top(2)</span><br><span class="line">res5: Array[Int] = Array(2, 3)</span><br></pre></td></tr></table></figure></div><h2 id="takeOrdered"><a href="#takeOrdered" class="headerlink" title="takeOrdered"></a>takeOrdered</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">def takeOrdered(num: Int)(implicit ord: Ordering[T]): Array[T]</span><br><span class="line"></span><br><span class="line">takeOrdered和top类似，只不过以和top相反的顺序返回元素。</span><br><span class="line"></span><br><span class="line">scala&gt; var rdd1 = sc.makeRDD(Seq(10, 4, 2, 12, 3))</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[40] at makeRDD at :21</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.top(1)</span><br><span class="line">res4: Array[Int] = Array(12)</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.top(2)</span><br><span class="line">res5: Array[Int] = Array(12, 10)</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.takeOrdered(1)</span><br><span class="line">res6: Array[Int] = Array(2)</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.takeOrdered(2)</span><br><span class="line">res7: Array[Int] = Array(2, 3)</span><br></pre></td></tr></table></figure></div><h2 id="aggregate"><a href="#aggregate" class="headerlink" title="aggregate"></a>aggregate</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">def aggregate[U](zeroValue: U)(seqOp: (U, T) ⇒ U, combOp: (U, U) ⇒ U)(implicit arg0: ClassTag[U]): U</span><br><span class="line"></span><br><span class="line">aggregate用户聚合RDD中的元素，先使用seqOp将RDD中每个分区中的T类型元素聚合成U类型，再使用combOp将之前每个分区聚合后的U类型聚合成U类型，特别注意seqOp和combOp都会使用zeroValue的值，zeroValue的类型为U。</span><br><span class="line"></span><br><span class="line">var rdd1 = sc.makeRDD(1 to 10,2)</span><br><span class="line">rdd1.mapPartitionsWithIndex&#123;</span><br><span class="line">        (partIdx,iter) =&gt; &#123;</span><br><span class="line">          var part_map = scala.collection.mutable.Map[String,List[Int]]()</span><br><span class="line">            <span class="keyword">while</span>(iter.hasNext)&#123;</span><br><span class="line">              var part_name = <span class="string">"part_"</span> + partIdx;</span><br><span class="line">              var elem = iter.next()</span><br><span class="line">              <span class="keyword">if</span>(part_map.contains(part_name)) &#123;</span><br><span class="line">                var elems = part_map(part_name)</span><br><span class="line">                elems ::= elem</span><br><span class="line">                part_map(part_name) = elems</span><br><span class="line">              &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                part_map(part_name) = List[Int]&#123;elem&#125;</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            part_map.iterator</span><br><span class="line">           </span><br><span class="line">        &#125;</span><br><span class="line">      &#125;.collect</span><br><span class="line">res16: Array[(String, List[Int])] = Array((part_0,List(5, 4, 3, 2, 1)), (part_1,List(10, 9, 8, 7, 6)))</span><br><span class="line"> </span><br><span class="line"><span class="comment">##第一个分区中包含5,4,3,2,1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##第二个分区中包含10,9,8,7,6</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.aggregate(1)(</span><br><span class="line">     |           &#123;(x : Int,y : Int) =&gt; x + y&#125;, </span><br><span class="line">     |           &#123;(a : Int,b : Int) =&gt; a + b&#125;</span><br><span class="line">     |     )</span><br><span class="line">res17: Int = 58</span><br><span class="line"> </span><br><span class="line">结果为什么是58，看下面的计算过程：</span><br><span class="line"></span><br><span class="line"><span class="comment">##先在每个分区中迭代执行 (x : Int,y : Int) =&gt; x + y 并且使用zeroValue的值1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##即：part_0中 zeroValue+5+4+3+2+1 = 1+5+4+3+2+1 = 16</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## part_1中 zeroValue+10+9+8+7+6 = 1+10+9+8+7+6 = 41</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##再将两个分区的结果合并(a : Int,b : Int) =&gt; a + b ，并且使用zeroValue的值1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##即：zeroValue+part_0+part_1 = 1 + 16 + 41 = 58</span></span><br><span class="line"></span><br><span class="line">再比如：</span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.aggregate(2)(</span><br><span class="line">     |           &#123;(x : Int,y : Int) =&gt; x + y&#125;, </span><br><span class="line">     |           &#123;(a : Int,b : Int) =&gt; a * b&#125;</span><br><span class="line">     |     )</span><br><span class="line">res18: Int = 1428</span><br><span class="line"> </span><br><span class="line"><span class="comment">##这次zeroValue=2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##part_0中 zeroValue+5+4+3+2+1 = 2+5+4+3+2+1 = 17</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##part_1中 zeroValue+10+9+8+7+6 = 2+10+9+8+7+6 = 42</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##最后：zeroValue*part_0*part_1 = 2 * 17 * 42 = 1428</span></span><br><span class="line"></span><br><span class="line">因此，zeroValue即确定了U的类型，也会对结果产生至关重要的影响，使用时候要特别注意。</span><br></pre></td></tr></table></figure></div><h2 id="fold"><a href="#fold" class="headerlink" title="fold"></a>fold</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def fold(zeroValue: T)(op: (T, T) ⇒ T): T</span><br><span class="line"></span><br><span class="line">fold是aggregate的简化，将aggregate中的seqOp和combOp使用同一个函数op。</span><br><span class="line">var rdd1 = sc.makeRDD(1 to 10, 2)</span><br><span class="line">scala&gt; rdd1.fold(1)(</span><br><span class="line">     |       (x,y) =&gt; x + y    </span><br><span class="line">     |     )</span><br><span class="line">res19: Int = 58</span><br><span class="line"> </span><br><span class="line"><span class="comment">##结果同上面使用aggregate的第一个例子一样，即：</span></span><br><span class="line">scala&gt; rdd1.aggregate(1)(</span><br><span class="line">     |           &#123;(x,y) =&gt; x + y&#125;, </span><br><span class="line">     |           &#123;(a,b) =&gt; a + b&#125;</span><br><span class="line">     |     )</span><br><span class="line">res20: Int = 58</span><br></pre></td></tr></table></figure></div><h2 id="lookup"><a href="#lookup" class="headerlink" title="lookup"></a>lookup</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def lookup(key: K): Seq[V]</span><br><span class="line"></span><br><span class="line">lookup用于(K,V)类型的RDD,指定K值，返回RDD中该K对应的所有V值。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; var rdd1 = sc.makeRDD(Array((<span class="string">"A"</span>,0),(<span class="string">"A"</span>,2),(<span class="string">"B"</span>,1),(<span class="string">"B"</span>,2),(<span class="string">"C"</span>,1)))</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[0] at makeRDD at :21</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.lookup(<span class="string">"A"</span>)</span><br><span class="line">res0: Seq[Int] = WrappedArray(0, 2)</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.lookup(<span class="string">"B"</span>)</span><br><span class="line">res1: Seq[Int] = WrappedArray(1, 2)</span><br></pre></td></tr></table></figure></div><h2 id="countByKey"><a href="#countByKey" class="headerlink" title="countByKey"></a>countByKey</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def countByKey(): Map[K, Long]</span><br><span class="line"></span><br><span class="line">countByKey用于统计RDD[K,V]中每个K的数量。</span><br><span class="line"></span><br><span class="line">scala&gt; var rdd1 = sc.makeRDD(Array((<span class="string">"A"</span>,0),(<span class="string">"A"</span>,2),(<span class="string">"B"</span>,1),(<span class="string">"B"</span>,2),(<span class="string">"B"</span>,3)))</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[7] at makeRDD at :21</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.countByKey</span><br><span class="line">res5: scala.collection.Map[String,Long] = Map(A -&gt; 2, B -&gt; 3)</span><br></pre></td></tr></table></figure></div><h2 id="foreach"><a href="#foreach" class="headerlink" title="foreach"></a>foreach</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">def foreach(f: (T) ⇒ Unit): Unit</span><br><span class="line"></span><br><span class="line">foreach用于遍历RDD,将函数f应用于每一个元素。</span><br><span class="line">但要注意，如果对RDD执行foreach，只会在Executor端有效，而并不是Driver端。</span><br><span class="line">比如：rdd.foreach(println)，只会在Executor的stdout中打印出来，Driver端是看不到的。</span><br><span class="line">我在Spark1.4中是这样，不知道是否真如此。</span><br><span class="line">这时候，使用accumulator共享变量与foreach结合，倒是个不错的选择。</span><br><span class="line"></span><br><span class="line">scala&gt; var cnt = sc.accumulator(0)</span><br><span class="line">cnt: org.apache.spark.Accumulator[Int] = 0</span><br><span class="line"> </span><br><span class="line">scala&gt; var rdd1 = sc.makeRDD(1 to 10,2)</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[5] at makeRDD at :21</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.foreach(x =&gt; cnt += x)</span><br><span class="line"> </span><br><span class="line">scala&gt; cnt.value</span><br><span class="line">res51: Int = 55</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.collect.foreach(println)</span><br></pre></td></tr></table></figure></div><h2 id="foreachPartition"><a href="#foreachPartition" class="headerlink" title="foreachPartition"></a>foreachPartition</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">def foreachPartition(f: (Iterator[T]) ⇒ Unit): Unit</span><br><span class="line"></span><br><span class="line">foreachPartition和foreach类似，只不过是对每一个分区使用f。</span><br><span class="line"></span><br><span class="line">scala&gt; var rdd1 = sc.makeRDD(1 to 10,2)</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[5] at makeRDD at :21</span><br><span class="line"> </span><br><span class="line">scala&gt; var allsize = sc.accumulator(0)</span><br><span class="line">size: org.apache.spark.Accumulator[Int] = 0</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">scala&gt;     rdd1.foreachPartition &#123; x =&gt; &#123;</span><br><span class="line">     |       allsize += x.size</span><br><span class="line">     |     &#125;&#125;</span><br><span class="line"> </span><br><span class="line">scala&gt; println(allsize.value)</span><br><span class="line">10</span><br><span class="line"></span><br><span class="line"><span class="comment">## sortBy</span></span><br><span class="line">``` bash</span><br><span class="line">def sortBy[K](f: (T) ⇒ K, ascending: Boolean = <span class="literal">true</span>, numPartitions: Int = this.partitions.length)(implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T]</span><br><span class="line"></span><br><span class="line">sortBy根据给定的排序k函数将RDD中的元素进行排序。</span><br><span class="line"></span><br><span class="line">scala&gt; var rdd1 = sc.makeRDD(Seq(3,6,7,1,2,0),2)</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.sortBy(x =&gt; x).collect</span><br><span class="line">res1: Array[Int] = Array(0, 1, 2, 3, 6, 7) //默认升序</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.sortBy(x =&gt; x,<span class="literal">false</span>).collect</span><br><span class="line">res2: Array[Int] = Array(7, 6, 3, 2, 1, 0)  //降序</span><br><span class="line"> </span><br><span class="line">//RDD[K,V]类型</span><br><span class="line">scala&gt;var rdd1 = sc.makeRDD(Array((<span class="string">"A"</span>,2),(<span class="string">"A"</span>,1),(<span class="string">"B"</span>,6),(<span class="string">"B"</span>,3),(<span class="string">"B"</span>,7)))</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.sortBy(x =&gt; x).collect</span><br><span class="line">res3: Array[(String, Int)] = Array((A,1), (A,2), (B,3), (B,6), (B,7))</span><br><span class="line"> </span><br><span class="line">//按照V进行降序排序</span><br><span class="line">scala&gt; rdd1.sortBy(x =&gt; x._2,<span class="literal">false</span>).collect</span><br><span class="line">res4: Array[(String, Int)] = Array((B,7), (B,6), (B,3), (A,2), (A,1))</span><br></pre></td></tr></table></figure></div><h2 id="saveAsTextFile"><a href="#saveAsTextFile" class="headerlink" title="saveAsTextFile"></a>saveAsTextFile</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">def saveAsTextFile(path: String): Unit</span><br><span class="line"></span><br><span class="line">def saveAsTextFile(path: String, codec: Class[_ &lt;: CompressionCodec]): Unit</span><br><span class="line"></span><br><span class="line">saveAsTextFile用于将RDD以文本文件的格式存储到文件系统中。</span><br><span class="line"></span><br><span class="line">codec参数可以指定压缩的类名。</span><br><span class="line"></span><br><span class="line">var rdd1 = sc.makeRDD(1 to 10,2)</span><br><span class="line">scala&gt; rdd1.saveAsTextFile(<span class="string">"hdfs://cdh5/tmp/lxw1234.com/"</span>) //保存到HDFS</span><br><span class="line">hadoop fs -ls /tmp/lxw1234.com</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r--   2 lxw1234 supergroup        0 2015-07-10 09:15 /tmp/lxw1234.com/_SUCCESS</span><br><span class="line">-rw-r--r--   2 lxw1234 supergroup        21 2015-07-10 09:15 /tmp/lxw1234.com/part-00000</span><br><span class="line"> </span><br><span class="line">hadoop fs -cat /tmp/lxw1234.com/part-00000</span><br><span class="line"></span><br><span class="line">注意：如果使用rdd1.saveAsTextFile(“file:///tmp/lxw1234.com”)将文件保存到本地文件系统，那么只会保存在Executor所在机器的本地目录。</span><br><span class="line"></span><br><span class="line">//指定压缩格式保存</span><br><span class="line"></span><br><span class="line">rdd1.saveAsTextFile(<span class="string">"hdfs://cdh5/tmp/lxw1234.com/"</span>,classOf[com.hadoop.compression.lzo.LzopCodec])</span><br><span class="line"> </span><br><span class="line">hadoop fs -ls /tmp/lxw1234.com</span><br><span class="line">-rw-r--r--   2 lxw1234 supergroup    0 2015-07-10 09:20 /tmp/lxw1234.com/_SUCCESS</span><br><span class="line">-rw-r--r--   2 lxw1234 supergroup    71 2015-07-10 09:20 /tmp/lxw1234.com/part-00000.lzo</span><br><span class="line"> </span><br><span class="line">hadoop fs -text /tmp/lxw1234.com/part-00000.lzo</span><br></pre></td></tr></table></figure></div><h2 id="saveAsSequenceFile"><a href="#saveAsSequenceFile" class="headerlink" title="saveAsSequenceFile"></a>saveAsSequenceFile</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">saveAsSequenceFile用于将RDD以SequenceFile的文件格式保存到HDFS上。</span><br><span class="line">用法同saveAsTextFile。</span><br></pre></td></tr></table></figure></div><h2 id="saveAsObjectFile"><a href="#saveAsObjectFile" class="headerlink" title="saveAsObjectFile"></a>saveAsObjectFile</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def saveAsObjectFile(path: String): Unit</span><br><span class="line"></span><br><span class="line">saveAsObjectFile用于将RDD中的元素序列化成对象，存储到文件中。</span><br><span class="line"></span><br><span class="line">对于HDFS，默认采用SequenceFile保存。</span><br><span class="line"></span><br><span class="line">var rdd1 = sc.makeRDD(1 to 10,2)</span><br><span class="line">rdd1.saveAsObjectFile(<span class="string">"hdfs://cdh5/tmp/lxw1234.com/"</span>)</span><br><span class="line"> </span><br><span class="line">hadoop fs -cat /tmp/lxw1234.com/part-00000</span><br><span class="line">SEQ !org.apache.hadoop.io.NullWritable<span class="string">"org.apache.hadoop.io.BytesWritableT</span></span><br></pre></td></tr></table></figure></div><h2 id="saveAsHadoopFile"><a href="#saveAsHadoopFile" class="headerlink" title="saveAsHadoopFile"></a>saveAsHadoopFile</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">def saveAsHadoopFile(path: String, keyClass: Class[_], valueClass: Class[_], outputFormatClass: Class[_ &lt;: OutputFormat[_, _]], codec: Class[_ &lt;: CompressionCodec]): Unit</span><br><span class="line">def saveAsHadoopFile(path: String, keyClass: Class[_], valueClass: Class[_], outputFormatClass: Class[_ &lt;: OutputFormat[_, _]], conf: JobConf = …, codec: Option[Class[_ &lt;: CompressionCodec]] = None): Unit</span><br><span class="line"></span><br><span class="line">saveAsHadoopFile是将RDD存储在HDFS上的文件中，支持老版本Hadoop API。</span><br><span class="line"></span><br><span class="line">可以指定outputKeyClass、outputValueClass以及压缩格式。</span><br><span class="line">每个分区输出一个文件。</span><br><span class="line">var rdd1 = sc.makeRDD(Array((<span class="string">"A"</span>,2),(<span class="string">"A"</span>,1),(<span class="string">"B"</span>,6),(<span class="string">"B"</span>,3),(<span class="string">"B"</span>,7)))</span><br><span class="line">import org.apache.hadoop.mapred.TextOutputFormat</span><br><span class="line">import org.apache.hadoop.io.Text</span><br><span class="line">import org.apache.hadoop.io.IntWritable</span><br><span class="line"> </span><br><span class="line">rdd1.saveAsHadoopFile(<span class="string">"/tmp/lxw1234.com/"</span>,classOf[Text],classOf[IntWritable],classOf[TextOutputFormat[Text,IntWritable]])</span><br><span class="line"> </span><br><span class="line">rdd1.saveAsHadoopFile(<span class="string">"/tmp/lxw1234.com/"</span>,classOf[Text],classOf[IntWritable],classOf[TextOutputFormat[Text,IntWritable]],</span><br><span class="line">                      classOf[com.hadoop.compression.lzo.LzopCodec])</span><br></pre></td></tr></table></figure></div><h2 id="saveAsHadoopDataset"><a href="#saveAsHadoopDataset" class="headerlink" title="saveAsHadoopDataset"></a>saveAsHadoopDataset</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line">def saveAsHadoopDataset(conf: JobConf): Unit</span><br><span class="line">saveAsHadoopDataset用于将RDD保存到除了HDFS的其他存储中，比如HBase。</span><br><span class="line">在JobConf中，通常需要关注或者设置五个参数：</span><br><span class="line">文件的保存路径、key值的class类型、value值的class类型、RDD的输出格式(OutputFormat)、以及压缩相关的参数。</span><br><span class="line"> </span><br><span class="line"><span class="comment">##使用saveAsHadoopDataset将RDD保存到HDFS中</span></span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line">import SparkContext._</span><br><span class="line">import org.apache.hadoop.mapred.TextOutputFormat</span><br><span class="line">import org.apache.hadoop.io.Text</span><br><span class="line">import org.apache.hadoop.io.IntWritable</span><br><span class="line">import org.apache.hadoop.mapred.JobConf</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">var rdd1 = sc.makeRDD(Array((<span class="string">"A"</span>,2),(<span class="string">"A"</span>,1),(<span class="string">"B"</span>,6),(<span class="string">"B"</span>,3),(<span class="string">"B"</span>,7)))</span><br><span class="line">var jobConf = new JobConf()</span><br><span class="line">jobConf.setOutputFormat(classOf[TextOutputFormat[Text,IntWritable]])</span><br><span class="line">jobConf.setOutputKeyClass(classOf[Text])</span><br><span class="line">jobConf.setOutputValueClass(classOf[IntWritable])</span><br><span class="line">jobConf.set(<span class="string">"mapred.output.dir"</span>,<span class="string">"/tmp/lxw1234/"</span>)</span><br><span class="line">rdd1.saveAsHadoopDataset(jobConf)</span><br><span class="line"> </span><br><span class="line">结果：</span><br><span class="line">hadoop fs -cat /tmp/lxw1234/part-00000</span><br><span class="line">A       2</span><br><span class="line">A       1</span><br><span class="line">hadoop fs -cat /tmp/lxw1234/part-00001</span><br><span class="line">B       6</span><br><span class="line">B       3</span><br><span class="line">B       7</span><br><span class="line"> </span><br><span class="line"><span class="comment">##保存数据到HBASE</span></span><br><span class="line"></span><br><span class="line">HBase建表：</span><br><span class="line"></span><br><span class="line">create ‘lxw1234′,&#123;NAME =&gt; ‘f1′,VERSIONS =&gt; 1&#125;,&#123;NAME =&gt; ‘f2′,VERSIONS =&gt; 1&#125;,&#123;NAME =&gt; ‘f3′,VERSIONS =&gt; 1&#125;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line">import SparkContext._</span><br><span class="line">import org.apache.hadoop.mapred.TextOutputFormat</span><br><span class="line">import org.apache.hadoop.io.Text</span><br><span class="line">import org.apache.hadoop.io.IntWritable</span><br><span class="line">import org.apache.hadoop.mapred.JobConf</span><br><span class="line">import org.apache.hadoop.hbase.HBaseConfiguration</span><br><span class="line">import org.apache.hadoop.hbase.mapred.TableOutputFormat</span><br><span class="line">import org.apache.hadoop.hbase.client.Put</span><br><span class="line">import org.apache.hadoop.hbase.util.Bytes</span><br><span class="line">import org.apache.hadoop.hbase.io.ImmutableBytesWritable</span><br><span class="line"> </span><br><span class="line">var conf = HBaseConfiguration.create()</span><br><span class="line">    var jobConf = new JobConf(conf)</span><br><span class="line">    jobConf.set(<span class="string">"hbase.zookeeper.quorum"</span>,<span class="string">"zkNode1,zkNode2,zkNode3"</span>)</span><br><span class="line">    jobConf.set(<span class="string">"zookeeper.znode.parent"</span>,<span class="string">"/hbase"</span>)</span><br><span class="line">    jobConf.set(TableOutputFormat.OUTPUT_TABLE,<span class="string">"lxw1234"</span>)</span><br><span class="line">    jobConf.setOutputFormat(classOf[TableOutputFormat])</span><br><span class="line">    </span><br><span class="line">    var rdd1 = sc.makeRDD(Array((<span class="string">"A"</span>,2),(<span class="string">"B"</span>,6),(<span class="string">"C"</span>,7)))</span><br><span class="line">    rdd1.map(x =&gt; </span><br><span class="line">      &#123;</span><br><span class="line">        var put = new Put(Bytes.toBytes(x._1))</span><br><span class="line">        put.add(Bytes.toBytes(<span class="string">"f1"</span>), Bytes.toBytes(<span class="string">"c1"</span>), Bytes.toBytes(x._2))</span><br><span class="line">        (new ImmutableBytesWritable,put)</span><br><span class="line">      &#125;</span><br><span class="line">    ).saveAsHadoopDataset(jobConf)</span><br><span class="line"> </span><br><span class="line"><span class="comment">##结果：</span></span><br><span class="line">hbase(main):005:0&gt; scan <span class="string">'lxw1234'</span></span><br><span class="line">ROW     COLUMN+CELL                                                                                                </span><br><span class="line"> A       column=f1:c1, timestamp=1436504941187, value=\x00\x00\x00\x02                                              </span><br><span class="line"> B       column=f1:c1, timestamp=1436504941187, value=\x00\x00\x00\x06                                              </span><br><span class="line"> C       column=f1:c1, timestamp=1436504941187, value=\x00\x00\x00\x07                                              </span><br><span class="line">3 row(s) <span class="keyword">in</span> 0.0550 seconds</span><br><span class="line"> </span><br><span class="line">注意：保存到HBase，运行时候需要在SPARK_CLASSPATH中加入HBase相关的jar包。</span><br></pre></td></tr></table></figure></div><h2 id="saveAsNewAPIHadoopFile"><a href="#saveAsNewAPIHadoopFile" class="headerlink" title="saveAsNewAPIHadoopFile"></a>saveAsNewAPIHadoopFile</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def saveAsNewAPIHadoopFile[F &lt;: OutputFormat[K, V]](path: String)(implicit fm: ClassTag[F]): Unit</span><br><span class="line"></span><br><span class="line">def saveAsNewAPIHadoopFile(path: String, keyClass: Class[_], valueClass: Class[_], outputFormatClass: Class[_ &lt;: OutputFormat[_, _]], conf: Configuration = self.context.hadoopConfiguration): Unit</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">saveAsNewAPIHadoopFile用于将RDD数据保存到HDFS上，使用新版本Hadoop API。</span><br><span class="line"></span><br><span class="line">用法基本同saveAsHadoopFile。</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line">import SparkContext._</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat</span><br><span class="line">import org.apache.hadoop.io.Text</span><br><span class="line">import org.apache.hadoop.io.IntWritable</span><br><span class="line"> </span><br><span class="line">var rdd1 = sc.makeRDD(Array((<span class="string">"A"</span>,2),(<span class="string">"A"</span>,1),(<span class="string">"B"</span>,6),(<span class="string">"B"</span>,3),(<span class="string">"B"</span>,7)))</span><br><span class="line">rdd1.saveAsNewAPIHadoopFile(<span class="string">"/tmp/lxw1234/"</span>,classOf[Text],classOf[IntWritable],classOf[TextOutputFormat[Text,IntWritable]])</span><br></pre></td></tr></table></figure></div><h2 id="saveAsNewAPIHadoopDataset"><a href="#saveAsNewAPIHadoopDataset" class="headerlink" title="saveAsNewAPIHadoopDataset"></a>saveAsNewAPIHadoopDataset</h2><div class="highlight-wrap" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true" data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">def saveAsNewAPIHadoopDataset(conf: Configuration): Unit</span><br><span class="line"></span><br><span class="line">作用同saveAsHadoopDataset,只不过采用新版本Hadoop API。</span><br><span class="line"></span><br><span class="line">以写入HBase为例：</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">HBase建表：</span><br><span class="line"></span><br><span class="line">create ‘lxw1234′,&#123;NAME =&gt; ‘f1′,VERSIONS =&gt; 1&#125;,&#123;NAME =&gt; ‘f2′,VERSIONS =&gt; 1&#125;,&#123;NAME =&gt; ‘f3′,VERSIONS =&gt; 1&#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">完整的Spark应用程序：</span><br><span class="line"></span><br><span class="line">package com.lxw1234.test</span><br><span class="line"> </span><br><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line">import SparkContext._</span><br><span class="line">import org.apache.hadoop.hbase.HBaseConfiguration</span><br><span class="line">import org.apache.hadoop.mapreduce.Job</span><br><span class="line">import org.apache.hadoop.hbase.mapreduce.TableOutputFormat</span><br><span class="line">import org.apache.hadoop.hbase.io.ImmutableBytesWritable</span><br><span class="line">import org.apache.hadoop.hbase.client.Result</span><br><span class="line">import org.apache.hadoop.hbase.util.Bytes</span><br><span class="line">import org.apache.hadoop.hbase.client.Put</span><br><span class="line"> </span><br><span class="line">object Test &#123;</span><br><span class="line">  def main(args : Array[String]) &#123;</span><br><span class="line">   val sparkConf = new SparkConf().setMaster(<span class="string">"spark://lxw1234.com:7077"</span>).setAppName(<span class="string">"lxw1234.com"</span>)</span><br><span class="line">   val sc = new SparkContext(sparkConf);</span><br><span class="line">   var rdd1 = sc.makeRDD(Array((<span class="string">"A"</span>,2),(<span class="string">"B"</span>,6),(<span class="string">"C"</span>,7)))</span><br><span class="line">   </span><br><span class="line">    sc.hadoopConfiguration.set(<span class="string">"hbase.zookeeper.quorum "</span>,<span class="string">"zkNode1,zkNode2,zkNode3"</span>)</span><br><span class="line">    sc.hadoopConfiguration.set(<span class="string">"zookeeper.znode.parent"</span>,<span class="string">"/hbase"</span>)</span><br><span class="line">    sc.hadoopConfiguration.set(TableOutputFormat.OUTPUT_TABLE,<span class="string">"lxw1234"</span>)</span><br><span class="line">    var job = new Job(sc.hadoopConfiguration)</span><br><span class="line">    job.setOutputKeyClass(classOf[ImmutableBytesWritable])</span><br><span class="line">    job.setOutputValueClass(classOf[Result])</span><br><span class="line">    job.setOutputFormatClass(classOf[TableOutputFormat[ImmutableBytesWritable]])</span><br><span class="line">    </span><br><span class="line">    rdd1.map(</span><br><span class="line">      x =&gt; &#123;</span><br><span class="line">        var put = new Put(Bytes.toBytes(x._1))</span><br><span class="line">        put.add(Bytes.toBytes(<span class="string">"f1"</span>), Bytes.toBytes(<span class="string">"c1"</span>), Bytes.toBytes(x._2))</span><br><span class="line">        (new ImmutableBytesWritable,put)</span><br><span class="line">      &#125;    </span><br><span class="line">    ).saveAsNewAPIHadoopDataset(job.getConfiguration)</span><br><span class="line">    </span><br><span class="line">    sc.stop()   </span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">注意：保存到HBase，运行时候需要在SPARK_CLASSPATH中加入HBase相关的jar包。</span><br></pre></td></tr></table></figure></div></div><div></div><div><div><div class="read-over">-------------------本文结束 <i class="fa fa-paw"></i> 感谢您的阅读-------------------</div></div></div><footer class="post-footer"><div class="post-tags"><a href="/blog/tags/spark/" rel="tag"><i class="fa fa-tag"></i> spark</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/blog/2019/11/19/sqoop_canshu/" rel="next" title="sqoop一些参数"><i class="fa fa-chevron-left"></i> sqoop一些参数</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/blog/2020/05/05/datax_jiaoben/" rel="prev" title="datax个人书写脚本">datax个人书写脚本 <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article><div class="post-spread"><div data-weibo-title="分享到微博" data-qq-title="分享到QQ" data-douban-title="分享到豆瓣" class="social-share" class="share-component" data-disabled="qzone,google+,linkedin" data-description="Share.js - 一键分享到微博，QQ空间，腾讯微博，人人，豆瓣...">分享到：</div></div></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div id="sidebar-dimmer"></div><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><a href="/"><img class="site-author-image" itemprop="image" src="/blog/images/ll.jpg" alt="bin"></a><p class="site-author-name" itemprop="name">bin</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/blog/archives/%7C%7Carchive"><span class="site-state-item-count">4</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/blog/categories/index.html"><span class="site-state-item-count">1</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/blog/tags/index.html"><span class="site-state-item-count">3</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="links-of-author motion-element"><span class="links-of-author-item"><a rel="external nofollow" href="mailto:bin10754@163.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i>E-Mail</a> </span><span class="links-of-author-item"><a rel="external nofollow" href="https://github.com/a1075427136/a1075427136.github.io" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a rel="external nofollow" href="http://wpa.qq.com/msgrd?V=1&Uin=295188316&Site=1075427136" target="_blank" title="QQ"><i class="fa fa-fw fa-globe"></i>QQ</a></span></div><div class="links-of-blogroll motion-element links-of-blogroll-block"><div class="links-of-blogroll-title"><i class="fa fa-history fa-" aria-hidden="true"></i> 近期文章</div><ul class="links-of-blogroll-list"><li class="my-links-of-blogroll-li"><a href="/blog/2020/05/05/datax_jiaoben/" title="datax个人书写脚本" target="_blank">datax个人书写脚本</a></li><li class="my-links-of-blogroll-li"><a href="/blog/2019/12/19/sparkRDD%E5%87%BD%E6%95%B0%E8%AF%A6%E8%A7%A3/" title="sparkRDD函数详解" target="_blank">sparkRDD函数详解</a></li><li class="my-links-of-blogroll-li"><a href="/blog/2019/11/19/sqoop_canshu/" title="sqoop一些参数" target="_blank">sqoop一些参数</a></li><li class="my-links-of-blogroll-li"><a href="/blog/2019/11/19/hive_hanshu/" title="hive常用函数" target="_blank">hive常用函数</a></li></ul></div><div><canvas id="canvas" style="width:60%"></canvas></div><script>!function(){function t(t){var r=[];a.fillStyle="#005eac";var h=new Date,u=70,s=30,v=h.getHours(),g=Math.floor(v/10),m=v%10;r.push({num:g}),r.push({num:m}),r.push({num:10});var c=h.getMinutes(),g=Math.floor(c/10),m=c%10;r.push({num:g}),r.push({num:m}),r.push({num:10});var M=h.getSeconds(),g=Math.floor(M/10),m=M%10;r.push({num:g}),r.push({num:m});for(var p=0;p<r.length;p++)r[p].offsetX=u,u=f(u,s,r[p].num,t),p<r.length-1&&10!=r[p].num&&10!=r[p+1].num&&(u+=l);if(0==i.length)i=r;else for(var C=0;C<i.length;C++)i[C].num!=r[C].num&&(n(r[C]),i[C].num=r[C].num);return e(t),o(),h}function n(t){for(var n=t.num,e=m[n],o=0;o<e.length;o++)for(var f=0;f<e[o].length;f++)if(1==e[o][f]){var a={offsetX:t.offsetX+u+2*u*f,offsetY:30+u+2*u*o,color:g[Math.floor(Math.random()*g.length)],g:1.5+Math.random(),vx:4*Math.pow(-1,Math.ceil(10*Math.random()))+Math.random(),vy:-5};v.push(a)}}function e(t){for(var n=0;n<v.length;n++)t.beginPath(),t.fillStyle=v[n].color,t.arc(v[n].offsetX,v[n].offsetY,u,0,2*Math.PI),t.fill()}function o(){for(var t=0,n=0;n<v.length;n++){var e=v[n];e.offsetX+=e.vx,e.offsetY+=e.vy,e.vy+=e.g,e.offsetY>h-u&&(e.offsetY=h-u,e.vy=-e.vy*s),e.offsetX>u&&e.offsetX<r-u&&(v[t]=v[n],t++)}for(;t<v.length;t++)v.pop()}function f(t,n,e,o){for(var f=m[e],a=0;a<f.length;a++)for(var r=0;r<f[a].length;r++)1==f[a][r]&&(o.beginPath(),o.arc(t+u+2*u*r,n+u+2*u*a,u,0,2*Math.PI),o.fill());return o.beginPath(),t+=f[0].length*u*2}var a,r=820,h=250,u=7,l=10,s=.65,v=[];const g=["#33B5E5","#0099CC","#AA66CC","#9933CC","#99CC00","#669900","#FFBB33","#FF8800","#FF4444","#CC0000"];var i=[],m=[[[0,0,1,1,1,0,0],[0,1,1,0,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,0,1,1,0],[0,0,1,1,1,0,0]],[[0,0,0,1,1,0,0],[0,1,1,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[1,1,1,1,1,1,1]],[[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,0,1,1,0,0,0],[0,1,1,0,0,0,0],[1,1,0,0,0,0,0],[1,1,0,0,0,1,1],[1,1,1,1,1,1,1]],[[1,1,1,1,1,1,1],[0,0,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,0,1,1,1,0,0],[0,0,0,0,1,1,0],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[0,0,0,0,1,1,0],[0,0,0,1,1,1,0],[0,0,1,1,1,1,0],[0,1,1,0,1,1,0],[1,1,0,0,1,1,0],[1,1,1,1,1,1,1],[0,0,0,0,1,1,0],[0,0,0,0,1,1,0],[0,0,0,0,1,1,0],[0,0,0,1,1,1,1]],[[1,1,1,1,1,1,1],[1,1,0,0,0,0,0],[1,1,0,0,0,0,0],[1,1,1,1,1,1,0],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[0,0,0,0,1,1,0],[0,0,1,1,0,0,0],[0,1,1,0,0,0,0],[1,1,0,0,0,0,0],[1,1,0,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[1,1,1,1,1,1,1],[1,1,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,1,1,0,0,0],[0,0,1,1,0,0,0],[0,0,1,1,0,0,0],[0,0,1,1,0,0,0]],[[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,1,1,0,0,0,0]],[[0,0,0,0],[0,0,0,0],[0,1,1,0],[0,1,1,0],[0,0,0,0],[0,0,0,0],[0,1,1,0],[0,1,1,0],[0,0,0,0],[0,0,0,0]]],c=document.getElementById("canvas");c.width=r,c.height=h,a=c.getContext("2d");new Date;setInterval(function(){a.clearRect(0,0,a.canvas.width,a.canvas.height),t(a)},50)}()</script><div id="days"></div><script language="javascript">function show_date_time(){window.setTimeout("show_date_time()",1e3),BirthDay=new Date("08/07/2019 20:00:00"),today=new Date,timeold=today.getTime()-BirthDay.getTime(),sectimeold=timeold/1e3,secondsold=Math.floor(sectimeold),msPerDay=864e5,e_daysold=timeold/msPerDay,daysold=Math.floor(e_daysold),e_hrsold=24*(e_daysold-daysold),hrsold=setzero(Math.floor(e_hrsold)),e_minsold=60*(e_hrsold-hrsold),minsold=setzero(Math.floor(60*(e_hrsold-hrsold))),seconds=setzero(Math.floor(60*(e_minsold-minsold))),document.getElementById("days").innerHTML="已运行"+daysold+"天"+hrsold+"时"+minsold+"分"+seconds+"秒"}function setzero(e){return e<10&&(e="0"+e),e}show_date_time()</script></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#RDD操作详解"><span class="nav-number">1.</span> <span class="nav-text">RDD操作详解</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#基本转换"><span class="nav-number">2.</span> <span class="nav-text">基本转换</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#map"><span class="nav-number">2.1.</span> <span class="nav-text">map</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#filter"><span class="nav-number">2.2.</span> <span class="nav-text">filter</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#flatMap"><span class="nav-number">2.3.</span> <span class="nav-text">flatMap</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mapPartitions"><span class="nav-number">2.4.</span> <span class="nav-text">mapPartitions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mapPartitionsWithIndex"><span class="nav-number">2.5.</span> <span class="nav-text">mapPartitionsWithIndex</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#coalesce"><span class="nav-number">2.6.</span> <span class="nav-text">coalesce</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#repartition"><span class="nav-number">2.7.</span> <span class="nav-text">repartition</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#randomSplit"><span class="nav-number">2.8.</span> <span class="nav-text">randomSplit</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#glom"><span class="nav-number">2.9.</span> <span class="nav-text">glom</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#union并集"><span class="nav-number">2.10.</span> <span class="nav-text">union并集</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#distinct"><span class="nav-number">2.11.</span> <span class="nav-text">distinct</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#intersection交集"><span class="nav-number">2.12.</span> <span class="nav-text">intersection交集</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#subtract"><span class="nav-number">2.13.</span> <span class="nav-text">subtract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#subtractByKey"><span class="nav-number">2.14.</span> <span class="nav-text">subtractByKey</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#groupbyKey"><span class="nav-number">2.15.</span> <span class="nav-text">groupbyKey</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#reduceByKey"><span class="nav-number">2.16.</span> <span class="nav-text">reduceByKey</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#sortByKey"><span class="nav-number">2.17.</span> <span class="nav-text">sortByKey</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#sortBy"><span class="nav-number">2.18.</span> <span class="nav-text">sortBy</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#zip"><span class="nav-number">2.19.</span> <span class="nav-text">zip</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#zipPartitions"><span class="nav-number">2.20.</span> <span class="nav-text">zipPartitions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#zipWithIndex"><span class="nav-number">2.21.</span> <span class="nav-text">zipWithIndex</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#zipWithUniqueId"><span class="nav-number">2.22.</span> <span class="nav-text">zipWithUniqueId</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#键值转换"><span class="nav-number">3.</span> <span class="nav-text">键值转换</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#partitionBy"><span class="nav-number">3.1.</span> <span class="nav-text">partitionBy</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mapValues"><span class="nav-number">3.2.</span> <span class="nav-text">mapValues</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#flatMapValues"><span class="nav-number">3.3.</span> <span class="nav-text">flatMapValues</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#combineByKey"><span class="nav-number">3.4.</span> <span class="nav-text">combineByKey</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#foldByKey"><span class="nav-number">3.5.</span> <span class="nav-text">foldByKey</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#reduceByKeyLocally"><span class="nav-number">3.6.</span> <span class="nav-text">reduceByKeyLocally</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cogroup和groupByKey的区别"><span class="nav-number">3.7.</span> <span class="nav-text">***cogroup和groupByKey的区别</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#join"><span class="nav-number">3.8.</span> <span class="nav-text">join</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#leftOuterJoin"><span class="nav-number">3.9.</span> <span class="nav-text">leftOuterJoin</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#rightOuterJoin"><span class="nav-number">3.10.</span> <span class="nav-text">rightOuterJoin</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Action操作"><span class="nav-number">4.</span> <span class="nav-text">Action操作</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#first"><span class="nav-number">4.1.</span> <span class="nav-text">first</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#count"><span class="nav-number">4.2.</span> <span class="nav-text">count</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#reduce"><span class="nav-number">4.3.</span> <span class="nav-text">reduce</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#take"><span class="nav-number">4.4.</span> <span class="nav-text">take</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#top"><span class="nav-number">4.5.</span> <span class="nav-text">top</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#takeOrdered"><span class="nav-number">4.6.</span> <span class="nav-text">takeOrdered</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#aggregate"><span class="nav-number">4.7.</span> <span class="nav-text">aggregate</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#fold"><span class="nav-number">4.8.</span> <span class="nav-text">fold</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lookup"><span class="nav-number">4.9.</span> <span class="nav-text">lookup</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#countByKey"><span class="nav-number">4.10.</span> <span class="nav-text">countByKey</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#foreach"><span class="nav-number">4.11.</span> <span class="nav-text">foreach</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#foreachPartition"><span class="nav-number">4.12.</span> <span class="nav-text">foreachPartition</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#saveAsTextFile"><span class="nav-number">4.13.</span> <span class="nav-text">saveAsTextFile</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#saveAsSequenceFile"><span class="nav-number">4.14.</span> <span class="nav-text">saveAsSequenceFile</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#saveAsObjectFile"><span class="nav-number">4.15.</span> <span class="nav-text">saveAsObjectFile</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#saveAsHadoopFile"><span class="nav-number">4.16.</span> <span class="nav-text">saveAsHadoopFile</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#saveAsHadoopDataset"><span class="nav-number">4.17.</span> <span class="nav-text">saveAsHadoopDataset</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#saveAsNewAPIHadoopFile"><span class="nav-number">4.18.</span> <span class="nav-text">saveAsNewAPIHadoopFile</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#saveAsNewAPIHadoopDataset"><span class="nav-number">4.19.</span> <span class="nav-text">saveAsNewAPIHadoopDataset</span></a></li></ol></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div class="copyright">&copy; 2019 &mdash; <span itemprop="copyrightYear">2020</span> <span class="with-love"><i class="fa fa-heartbeat"></i> </span><span class="author" itemprop="copyrightHolder">bin</span></div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv"><i class="fa fa-user"></i>访问人数 <span class="busuanzi-value" id="busuanzi_value_site_uv"></span> 人次 </span><span class="site-pv"><i class="fa fa-eye"></i>总访问量 <span class="busuanzi-value" id="busuanzi_value_site_pv"></span> 次</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span id="scrollpercent"><span>0</span>%</span></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="//cdn.jsdelivr.net/npm/jquery@2.1.3/dist/jquery.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/fastclick/1.0.6/fastclick.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/jquery.lazyload/1.9.3/jquery.lazyload.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/velocity-animate@1.2.1/velocity.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/npm/velocity-animate@1.2.1/velocity.ui.min.js"></script><script type="text/javascript" src="/blog/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/blog/lib/canvas-nest/canvas-nest.min.js"></script><script type="text/javascript" src="/blog/js/src/utils.js?v=5.1.4"></script><script type="text/javascript" src="/blog/js/src/motion.js?v=5.1.4"></script><script type="text/javascript" src="/blog/js/src/affix.js?v=5.1.4"></script><script type="text/javascript" src="/blog/js/src/schemes/pisces.js?v=5.1.4"></script><script type="text/javascript" src="/blog/js/src/scrollspy.js?v=5.1.4"></script><script type="text/javascript" src="/blog/js/src/post-details.js?v=5.1.4"></script><script type="text/javascript" src="/blog/js/src/bootstrap.js?v=5.1.4"></script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="search-popup-overlay local-search-pop-overlay"></div>').css("overflow","hidden"),$(".search-popup-overlay").click(onPopupClose),$(".popup").toggle();var t=$("#local-search-input");t.attr("autocapitalize","none"),t.attr("autocorrect","off"),t.focus()}var isfetched=!1,isXml=!0,search_path="search.xml";0===search_path.length?search_path="search.xml":/json$/i.test(search_path)&&(isXml=!1);var path="/blog/"+search_path,onPopupClose=function(t){$(".popup").hide(),$("#local-search-input").val(""),$(".search-result-list").remove(),$("#no-result").remove(),$(".local-search-pop-overlay").remove(),$("body").css("overflow","")},searchFunc=function(t,e,o){"use strict";$("body").append('<div class="search-popup-overlay local-search-pop-overlay"><div id="search-loading-icon"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div>').css("overflow","hidden"),$("#search-loading-icon").css("margin","20% auto 0 auto").css("text-align","center"),$.ajax({url:t,dataType:isXml?"xml":"json",async:!0,success:function(t){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var n=isXml?$("entry",t).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get():t,r=document.getElementById(e),s=document.getElementById(o),a=function(){var t=r.value.trim().toLowerCase(),e=t.split(/[\s\-]+/);e.length>1&&e.push(t);var o=[];if(t.length>0&&n.forEach(function(n){function r(e,o,n,r){for(var s=r[r.length-1],a=s.position,i=s.word,l=[],h=0;a+i.length<=n&&0!=r.length;){i===t&&h++,l.push({position:a,length:i.length});var p=a+i.length;for(r.pop();0!=r.length&&(s=r[r.length-1],a=s.position,i=s.word,p>a);)r.pop()}return c+=h,{hits:l,start:o,end:n,searchTextCount:h}}function s(t,e){var o="",n=e.start;return e.hits.forEach(function(e){o+=t.substring(n,e.position);var r=e.position+e.length;o+='<b class="search-keyword">'+t.substring(e.position,r)+"</b>",n=r}),o+=t.substring(n,e.end)}var a=!1,i=0,c=0,l=n.title.trim(),h=l.toLowerCase(),p=n.content.trim().replace(/<[^>]+>/g,""),u=p.toLowerCase(),f=decodeURIComponent(n.url),d=[],g=[];if(""!=l&&(e.forEach(function(t){function e(t,e,o){var n=t.length;if(0===n)return[];var r=0,s=[],a=[];for(o||(e=e.toLowerCase(),t=t.toLowerCase());(s=e.indexOf(t,r))>-1;)a.push({position:s,word:t}),r=s+n;return a}d=d.concat(e(t,h,!1)),g=g.concat(e(t,u,!1))}),(d.length>0||g.length>0)&&(a=!0,i=d.length+g.length)),a){[d,g].forEach(function(t){t.sort(function(t,e){return e.position!==t.position?e.position-t.position:t.word.length-e.word.length})});var v=[];0!=d.length&&v.push(r(l,0,l.length,d));for(var $=[];0!=g.length;){var C=g[g.length-1],m=C.position,x=C.word,w=m-20,y=m+80;w<0&&(w=0),y<m+x.length&&(y=m+x.length),y>p.length&&(y=p.length),$.push(r(p,w,y,g))}$.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hits.length!==e.hits.length?e.hits.length-t.hits.length:t.start-e.start});var T=parseInt("1");T>=0&&($=$.slice(0,T));var b="";b+=0!=v.length?"<li><a href='"+f+"' class='search-result-title'>"+s(l,v[0])+"</a>":"<li><a href='"+f+"' class='search-result-title'>"+l+"</a>",$.forEach(function(t){b+="<a href='"+f+'\'><p class="search-result">'+s(p,t)+"...</p></a>"}),b+="</li>",o.push({item:b,searchTextCount:c,hitCount:i,id:o.length})}}),1===e.length&&""===e[0])s.innerHTML='<div id="no-result"><i class="fa fa-search fa-5x" /></div>';else if(0===o.length)s.innerHTML='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>';else{o.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hitCount!==e.hitCount?e.hitCount-t.hitCount:e.id-t.id});var a='<ul class="search-result-list">';o.forEach(function(t){a+=t.item}),a+="</ul>",s.innerHTML=a}};r.addEventListener("input",a),$(".local-search-pop-overlay").remove(),$("body").css("overflow",""),proceedsearch()}})};$(".popup-trigger").click(function(t){t.stopPropagation(),isfetched===!1?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(onPopupClose),$(".popup").click(function(t){t.stopPropagation()}),$(document).on("keyup",function(t){var e=27===t.which&&$(".search-popup").is(":visible");e&&onPopupClose()})</script><script>!function(){var t=document.createElement("script"),s=window.location.protocol.split(":")[0];"https"===s?t.src="https://zz.bdstatic.com/linksubmit/push.js":t.src="http://push.zhanzhang.baidu.com/push.js";var e=document.getElementsByTagName("script")[0];e.parentNode.insertBefore(t,e)}()</script><script src="/js/cursor/love.min.js"></script><script src="/js/src/activate-power-mode.min.js"></script><script>POWERMODE.colorful=!0,POWERMODE.shake=!1,document.body.addEventListener("input",POWERMODE)</script><link rel="stylesheet" href="/dist/APlayer.min.css"><div id="aplayer"></div><script type="text/javascript" src="/dist/APlayer.min.js"></script><script type="text/javascript" src="/dist/music.js"></script><script type="text/javascript" src="/js/src/clipboard.min.js"></script><script type="text/javascript" src="/js/src/clipboard-use.js"></script><link rel="stylesheet" href="/sharejs/css/share.min.css"><script src="/sharejs/js/social-share.min.js"></script><script type="text/javascript" src="/js/src/linkcard.js"></script></body></html><!-- rebuild by neat -->
<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>datax个人书写脚本</title>
      <link href="/bin1002/2020/05/05/datax_jiaoben/"/>
      <url>/bin1002/2020/05/05/datax_jiaoben/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon Jun 08 2020 17:37:04 GMT+0800 (GMT+08:00) --><h1 id="mysql到mysql"><a href="#mysql到mysql" class="headerlink" title="mysql到mysql"></a>mysql到mysql</h1><h2 id="win版"><a href="#win版" class="headerlink" title="win版"></a>win版</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">CHCP 65001</span><br><span class="line"></span><br><span class="line">:: mysql 到 mysql</span><br><span class="line"></span><br><span class="line">:::源库数据库信息（读取）</span><br><span class="line">::源库数据库类型（Mysql、Oracle、SqlServer）</span><br><span class="line"><span class="built_in">set</span> source_type=mysqlreader</span><br><span class="line">::源库连接串</span><br><span class="line"><span class="built_in">set</span> source_ip=jdbc:mysql://ip:3306/<span class="built_in">test</span>?useUnicode=<span class="literal">true</span>&amp;characterEncoding=UTF-8&amp;useSSL=<span class="literal">false</span></span><br><span class="line">::源库用户名 </span><br><span class="line"><span class="built_in">set</span> source_name=admin</span><br><span class="line">::源库密码</span><br><span class="line"><span class="built_in">set</span> source_password=123456</span><br><span class="line">::源库的查询sql</span><br><span class="line"><span class="built_in">set</span> source_sql=select * from demo</span><br><span class="line"></span><br><span class="line">::字段要统一  一一对应</span><br><span class="line"></span><br><span class="line">:::目标库数据库信息（写入）</span><br><span class="line">::目标库数据库类型 参数查百度</span><br><span class="line"><span class="built_in">set</span> target_type=mysqlwriter</span><br><span class="line">::目标库连接串</span><br><span class="line"><span class="built_in">set</span> target_ip=jdbc:mysql://ip:3306/<span class="built_in">test</span>?useUnicode=<span class="literal">true</span>&amp;characterEncoding=UTF-8&amp;useSSL=<span class="literal">false</span></span><br><span class="line">::目标库用户名</span><br><span class="line"><span class="built_in">set</span> target_name=admin</span><br><span class="line">::目标库密码</span><br><span class="line"><span class="built_in">set</span> target_password=123456</span><br><span class="line">::目标库落地表</span><br><span class="line"><span class="built_in">set</span> target_table=demo</span><br><span class="line"></span><br><span class="line">::落地目标模式（replace、update、insert）</span><br><span class="line"><span class="built_in">set</span> writeMode=insert</span><br><span class="line"></span><br><span class="line">::编辑回写json文件</span><br><span class="line"><span class="built_in">set</span> json=&#123; <span class="string">"job"</span>: &#123; <span class="string">"content"</span>: [ &#123; <span class="string">"reader"</span>: &#123; <span class="string">"name"</span>: <span class="string">"%source_type%"</span>, <span class="string">"parameter"</span>: &#123; <span class="string">"column"</span>: [ <span class="string">"*"</span>  ], <span class="string">"connection"</span>: [ &#123; <span class="string">"jdbcUrl"</span>: [<span class="string">"%source_ip%"</span>], <span class="string">"querySql"</span>: [<span class="string">"%source_sql%"</span>] &#125; ], <span class="string">"password"</span>: <span class="string">"%source_password%"</span>, <span class="string">"username"</span>: <span class="string">"%source_name%"</span> &#125; &#125;,  <span class="string">"writer"</span>: &#123; <span class="string">"name"</span>: <span class="string">"%target_type%"</span>, <span class="string">"parameter"</span>: &#123; <span class="string">"column"</span>: [ <span class="string">"*"</span>  ],<span class="string">"writeMode"</span>:<span class="string">"%writeMode%"</span>, <span class="string">"connection"</span>: [ &#123; <span class="string">"jdbcUrl"</span>: <span class="string">"%target_ip%"</span>, <span class="string">"table"</span>: [<span class="string">"%target_table%"</span>] &#125; ], <span class="string">"password"</span>: <span class="string">"%target_password%"</span>, <span class="string">"username"</span>: <span class="string">"%target_name%"</span>  &#125; &#125; &#125; ], <span class="string">"setting"</span>: &#123; <span class="string">"speed"</span>: &#123; <span class="string">"channel"</span>: 5,<span class="string">"record"</span>:-1,<span class="string">"byte"</span>:-1,<span class="string">"batchSize"</span>:2048 &#125; &#125; &#125; &#125;</span><br><span class="line">::输出json文件到指定目录</span><br><span class="line"><span class="built_in">echo</span> %json% &gt; E:\datax\job\%target_table%.json</span><br><span class="line"></span><br><span class="line">::执行datax 参数为配置好的json（目录自己改）</span><br><span class="line">python E:\datax\bin\datax.py --jvm=<span class="string">"-Xms1G -Xmx1G"</span> E:\datax\job\%target_table%.json</span><br><span class="line"><span class="built_in">echo</span> 完毕</span><br><span class="line">pause 任意键退出</span><br></pre></td></tr></table></figure></div><h2 id="linux版-此版本为双方库表结构相同"><a href="#linux版-此版本为双方库表结构相同" class="headerlink" title="linux版 (此版本为双方库表结构相同)"></a>linux版 (此版本为双方库表结构相同)</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="comment">#源库(oracle) 到 目标库mysql</span></span><br><span class="line"><span class="comment">#指定表模式</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#开始时间</span></span><br><span class="line">start_time=$(date +%s)</span><br><span class="line"></span><br><span class="line"><span class="comment">#源库数据库信息（读取）</span></span><br><span class="line">source_type=oraclereader</span><br><span class="line"><span class="comment">#源库连接串</span></span><br><span class="line">source_ip=jdbc:oracle:thin:@ip:1521:orcl</span><br><span class="line"><span class="comment">#源库用户名</span></span><br><span class="line">source_name=admin</span><br><span class="line"><span class="comment">#源库密码</span></span><br><span class="line">source_password=123456</span><br><span class="line"><span class="comment">#目标库数据库信息（写入）</span></span><br><span class="line"><span class="comment">#目标库数据库类型 参数查百度</span></span><br><span class="line">target_type=mysqlwriter</span><br><span class="line"></span><br><span class="line"><span class="comment">#目标库用户名</span></span><br><span class="line">target_name=admin</span><br><span class="line"><span class="comment">#目标库密码</span></span><br><span class="line">target_password=123456</span><br><span class="line"><span class="comment">#落地目标模式</span></span><br><span class="line">writeMode=insert</span><br><span class="line"><span class="comment">#同步表名称 所有表名 空格隔开</span></span><br><span class="line">tables=(demo1 demo2 demo3)</span><br><span class="line"><span class="comment">#循环导入数据</span></span><br><span class="line"><span class="keyword">for</span> table <span class="keyword">in</span> <span class="variable">$&#123;tables[*]&#125;</span></span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">\cp /opt/datax/job/demo.json -rf /opt/datax/job/json_tmp/<span class="variable">$table</span>.json</span><br><span class="line">sed -i <span class="string">"s/%source_type%/<span class="variable">$source_type</span>/g"</span> /opt/datax/job/json_tmp/<span class="variable">$table</span>.json</span><br><span class="line">sed -i <span class="string">"s/%source_ip%/<span class="variable">$source_ip</span>/g"</span> /opt/datax/job/json_tmp/<span class="variable">$table</span>.json</span><br><span class="line">sed -i <span class="string">"s/%source_name%/<span class="variable">$source_name</span>/g"</span> /opt/datax/job/json_tmp/<span class="variable">$table</span>.json</span><br><span class="line">sed -i <span class="string">"s/%source_password%/<span class="variable">$source_password</span>/g"</span> /opt/datax/job/json_tmp/<span class="variable">$table</span>.json</span><br><span class="line">sed -i <span class="string">"s/%target_type%/<span class="variable">$target_type</span>/g"</span> /opt/datax/job/json_tmp/<span class="variable">$table</span>.json</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#目标库连接串</span></span><br><span class="line">sed -i <span class="string">"s/\%target_ip\%/jdbc:mysql:\/\/目标库ip:3306\/数据库名\?useUnicode=true\&amp;characterEncoding=utf8/g"</span> /opt/datax/job/json_tmp/<span class="variable">$table</span>.json</span><br><span class="line"></span><br><span class="line">sed -i <span class="string">"s/%target_name%/<span class="variable">$target_name</span>/g"</span> /opt/datax/job/json_tmp/<span class="variable">$table</span>.json</span><br><span class="line">sed -i <span class="string">"s/%target_password%/<span class="variable">$target_password</span>/g"</span> /opt/datax/job/json_tmp/<span class="variable">$table</span>.json</span><br><span class="line">sed -i <span class="string">"s/%writeMode%/<span class="variable">$writeMode</span>/g"</span> /opt/datax/job/json_tmp/<span class="variable">$table</span>.json</span><br><span class="line">sed -i <span class="string">"s/%table%/<span class="variable">$table</span>/g"</span> /opt/datax/job/json_tmp/<span class="variable">$table</span>.json</span><br><span class="line"><span class="comment">#执行datax</span></span><br><span class="line">python /opt/datax/bin/datax.py --jvm=<span class="string">"-Xms1G -Xmx1G"</span> /opt/datax/job/json_tmp/<span class="variable">$table</span>.json</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"完成<span class="variable">$table</span>同步"</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"===================================="</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"===================================="</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"===================================="</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"===================================="</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"===================================="</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"===================================="</span></span><br><span class="line"><span class="comment">#清理json文件</span></span><br><span class="line">rm -rf /opt/datax/job/json_tmp/<span class="variable">$table</span>.json</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"><span class="comment">#结束时间</span></span><br><span class="line">end_time=$(date +%s)</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"<span class="variable">$(date +%Y%m%d_%H%M%S)</span> 完成数据同步<span class="variable">$&#123;tables[*]&#125;</span>"</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"用时<span class="variable">$((($end_time-$start_time)</span>/60))分钟<span class="variable">$((($end_time-$start_time)</span>%60))秒"</span> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">该模板使用到的json：</span><br><span class="line">&#123; <span class="string">"job"</span>: &#123; <span class="string">"content"</span>: [ &#123; <span class="string">"reader"</span>: &#123; <span class="string">"name"</span>: <span class="string">"%source_type%"</span>, <span class="string">"parameter"</span>: &#123; <span class="string">"column"</span>: [<span class="string">"*"</span>], <span class="string">"connection"</span>: [ &#123; <span class="string">"jdbcUrl"</span>: [<span class="string">"%source_ip%"</span>], <span class="string">"table"</span>: [<span class="string">"%table%"</span>] &#125; ], <span class="string">"password"</span>: <span class="string">"%source_password%"</span>, <span class="string">"username"</span>: <span class="string">"%source_name%"</span>, &#125; &#125;, <span class="string">"writer"</span>: &#123; <span class="string">"name"</span>: <span class="string">"%target_type%"</span>, <span class="string">"parameter"</span>: &#123; <span class="string">"column"</span>: [<span class="string">"*"</span>], <span class="string">"connection"</span>: [ &#123; <span class="string">"jdbcUrl"</span>: <span class="string">"%target_ip%"</span>, <span class="string">"table"</span>: [<span class="string">"%table%"</span>] &#125; ], <span class="string">"password"</span>: <span class="string">"%target_password%"</span>, <span class="string">"username"</span>: <span class="string">"%target_name%"</span>, &#125; &#125; &#125; ], <span class="string">"setting"</span>: &#123; <span class="string">"speed"</span>: &#123; <span class="string">"channel"</span>: 5 &#125; &#125; &#125; &#125;</span><br></pre></td></tr></table></figure></div><h3 id="切换不同类型的数据库只需更改-数据库type参数。-具体支持某些数据库自行百度"><a href="#切换不同类型的数据库只需更改-数据库type参数。-具体支持某些数据库自行百度" class="headerlink" title="切换不同类型的数据库只需更改 数据库type参数。 具体支持某些数据库自行百度"></a>切换不同类型的数据库只需更改 数据库type参数。 具体支持某些数据库自行百度</h3><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ETL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>sparkRDD函数详解</title>
      <link href="/bin1002/2019/12/19/sparkRDD%E5%87%BD%E6%95%B0%E8%AF%A6%E8%A7%A3/"/>
      <url>/bin1002/2019/12/19/sparkRDD%E5%87%BD%E6%95%B0%E8%AF%A6%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon Jun 08 2020 17:37:04 GMT+0800 (GMT+08:00) --><h1 id="RDD操作详解"><a href="#RDD操作详解" class="headerlink" title="RDD操作详解"></a>RDD操作详解</h1><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">启动spark-shell</span><br><span class="line">spark-shell --master spark://hdp-node-01:7077</span><br></pre></td></tr></table></figure></div><h1 id="基本转换"><a href="#基本转换" class="headerlink" title="基本转换"></a>基本转换</h1><h2 id="map"><a href="#map" class="headerlink" title="map"></a>map</h2><p>map是对RDD中的每个元素都执行一个指定的函数来产生一个新的RDD。 任何原RDD中的元素在新RDD中都有且只有一个元素与之对应。<br>举例：</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val a = sc.parallelize(1 to 9, 3)</span><br><span class="line">scala&gt; val b = a.map(x =&gt; x*2)</span><br><span class="line">scala&gt; a.collect</span><br><span class="line">res10: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9)</span><br><span class="line">scala&gt; b.collect</span><br><span class="line">res11: Array[Int] = Array(2, 4, 6, 8, 10, 12, 14, 16, 18)</span><br><span class="line">上述例子中把原RDD中每个元素都乘以2来产生一个新的RDD。</span><br></pre></td></tr></table></figure></div><h2 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">filter 是对RDD中的每个元素都执行一个指定的函数来过滤产生一个新的RDD。 任何原RDD中的元素在新RDD中都有且只有一个元素与之对应。</span><br><span class="line">val rdd = sc.parallelize(List(1,2,3,4,5,6))  </span><br><span class="line">val filterRdd = rdd.filter(_ &gt; 5)</span><br><span class="line">filterRdd.collect() //返回所有大于5的数据的一个Array</span><br></pre></td></tr></table></figure></div><h2 id="flatMap"><a href="#flatMap" class="headerlink" title="flatMap"></a>flatMap</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">与map类似，区别是原RDD中的元素经map处理后只能生成一个元素，而原RDD中的元素经flatmap处理后可生成多个元素来构建新RDD。 举例：对原RDD中的每个元素x产生y个元素（从1到y，y为元素x的值）</span><br><span class="line">scala&gt; val a = sc.parallelize(1 to 4, 2)</span><br><span class="line">scala&gt; val b = a.flatMap(x =&gt; 1 to x)</span><br><span class="line">scala&gt; b.collect</span><br><span class="line">res12: Array[Int] = Array(1, 1, 2, 1, 2, 3, 1, 2, 3, 4)</span><br></pre></td></tr></table></figure></div><h2 id="mapPartitions"><a href="#mapPartitions" class="headerlink" title="mapPartitions"></a>mapPartitions</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">mapPartitions是map的一个变种。map的输入函数是应用于RDD中每个元素，而mapPartitions的输入函数是应用于每个分区，也就是把每个分区中的内容作为整体来处理的。 它的函数定义为：</span><br><span class="line">def mapPartitions[U: ClassTag](f: Iterator[T] =&gt; Iterator[U], preservesPartitioning: Boolean = <span class="literal">false</span>): RDD[U]</span><br><span class="line">f即为输入函数，它处理每个分区里面的内容。每个分区中的内容将以Iterator[T]传递给输入函数f，f的输出结果是Iterator[U]。最终的RDD由所有分区经过输入函数处理后的结果合并起来的。</span><br><span class="line">举例：</span><br><span class="line">scala&gt; val a = sc.parallelize(1 to 9, 3)</span><br><span class="line">scala&gt; def myfunc[T](iter: Iterator[T]) : Iterator[(T, T)] = &#123;</span><br><span class="line">  var res = List[(T, T)]() </span><br><span class="line">  var pre = iter.next </span><br><span class="line"><span class="keyword">while</span> (iter.hasNext) &#123;</span><br><span class="line">    val cur = iter.next</span><br><span class="line">    res.::=(pre, cur)</span><br><span class="line">      pre = cur  &#125; </span><br><span class="line">  res.iterator</span><br><span class="line">&#125;</span><br><span class="line">scala&gt; a.mapPartitions(myfunc).collect</span><br><span class="line">res0: Array[(Int, Int)] = Array((2,3), (1,2), (5,6), (4,5), (8,9), (7,8))</span><br><span class="line">上述例子中的函数myfunc是把分区中一个元素和它的下一个元素组成一个Tuple。因为分区中最后一个元素没有下一个元素了，所以(3,4)和(6,7)不在结果中。 mapPartitions还有些变种，比如mapPartitionsWithContext，它能把处理过程中的一些状态信息传递给用户指定的输入函数。还有mapPartitionsWithIndex，它能把分区的index传递给用户指定的输入函数。</span><br></pre></td></tr></table></figure></div><h2 id="mapPartitionsWithIndex"><a href="#mapPartitionsWithIndex" class="headerlink" title="mapPartitionsWithIndex"></a>mapPartitionsWithIndex</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def mapPartitionsWithIndex[U](f: (Int, Iterator[T]) =&gt; Iterator[U], preservesPartitioning: Boolean = <span class="literal">false</span>)(implicit arg0: ClassTag[U]): RDD[U]</span><br><span class="line">函数作用同mapPartitions，不过提供了两个参数，第一个参数为分区的索引。</span><br><span class="line"></span><br><span class="line">var rdd1 = sc.makeRDD(1 to 5,2)</span><br><span class="line">//rdd1有两个分区</span><br><span class="line">var rdd2 = rdd1.mapPartitionsWithIndex&#123;</span><br><span class="line">        (x,iter) =&gt; &#123;</span><br><span class="line">          var result = List[String]()</span><br><span class="line">            var i = 0</span><br><span class="line">            <span class="keyword">while</span>(iter.hasNext)&#123;</span><br><span class="line">              i += iter.next()</span><br><span class="line">            &#125;</span><br><span class="line">            result.::(x + <span class="string">"|"</span> + i).iterator</span><br><span class="line">           </span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">//rdd2将rdd1中每个分区的数字累加，并在每个分区的累加结果前面加了分区索引</span><br><span class="line">scala&gt; rdd2.collect</span><br><span class="line">res13: Array[String] = Array(0|3, 1|12)</span><br></pre></td></tr></table></figure></div><h2 id="coalesce"><a href="#coalesce" class="headerlink" title="coalesce"></a>coalesce</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def coalesce(numPartitions: Int, shuffle: Boolean = <span class="literal">false</span>)(implicit ord: Ordering[T] = null): RDD[T]</span><br><span class="line">该函数用于将RDD进行重分区，使用HashPartitioner。</span><br><span class="line">第一个参数为重分区的数目，第二个为是否进行shuffle，默认为<span class="literal">false</span>;</span><br><span class="line">以下面的例子来看：</span><br><span class="line">scala&gt; var data = sc.parallelize(1 to 12, 3) </span><br><span class="line">scala&gt; data.collect </span><br><span class="line">scala&gt; data.partitions.size </span><br><span class="line">scala&gt; var rdd1 = data.coalesce(1) </span><br><span class="line">scala&gt; rdd1.partitions.size </span><br><span class="line">scala&gt; var rdd1 = data.coalesce(4) </span><br><span class="line">scala&gt; rdd1.partitions.size</span><br><span class="line">res2: Int = 1   //如果重分区的数目大于原来的分区数，那么必须指定shuffle参数为<span class="literal">true</span>，//否则，分区数不便</span><br><span class="line">scala&gt; var rdd1 = data.coalesce(4,<span class="literal">true</span>) </span><br><span class="line">scala&gt; rdd1.partitions.size</span><br><span class="line">res3: Int = 4</span><br></pre></td></tr></table></figure></div><h2 id="repartition"><a href="#repartition" class="headerlink" title="repartition"></a>repartition</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T]</span><br><span class="line">该函数其实就是coalesce函数第二个参数为<span class="literal">true</span>的实现</span><br><span class="line">scala&gt; var data = sc.parallelize(1 to 12, 3) </span><br><span class="line">scala&gt; data.collect </span><br><span class="line">scala&gt; data.partitions.size </span><br><span class="line">scala&gt; var rdd1 = data. repartition(1) </span><br><span class="line">scala&gt; rdd1.partitions.size </span><br><span class="line">scala&gt; var rdd1 = data. repartition(4) </span><br><span class="line">scala&gt; rdd1.partitions.size</span><br><span class="line">res3: Int = 4</span><br></pre></td></tr></table></figure></div><h2 id="randomSplit"><a href="#randomSplit" class="headerlink" title="randomSplit"></a>randomSplit</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">def randomSplit(weights: Array[Double], seed: Long = Utils.random.nextLong): Array[RDD[T]]</span><br><span class="line">该函数根据weights权重，将一个RDD切分成多个RDD。</span><br><span class="line">该权重参数为一个Double数组</span><br><span class="line">第二个参数为random的种子，基本可忽略。</span><br><span class="line">scala&gt; var rdd = sc.makeRDD(1 to 12,12)</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[16] at makeRDD at :21</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.collect</span><br><span class="line">res6: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)  </span><br><span class="line"> </span><br><span class="line">scala&gt; var splitRDD = rdd.randomSplit(Array(0.5, 0.1, 0.2, 0.2))</span><br><span class="line">splitRDD: Array[org.apache.spark.rdd.RDD[Int]] = Array(MapPartitionsRDD[17] at randomSplit at :23, </span><br><span class="line">MapPartitionsRDD[18] at randomSplit at :23, </span><br><span class="line">MapPartitionsRDD[19] at randomSplit at :23, </span><br><span class="line">MapPartitionsRDD[20] at randomSplit at :23)</span><br><span class="line"> </span><br><span class="line">//这里注意：randomSplit的结果是一个RDD数组</span><br><span class="line">scala&gt; splitRDD.size</span><br><span class="line">res8: Int = 4</span><br><span class="line">//由于randomSplit的第一个参数weights中传入的值有4个，因此，就会切分成4个RDD,</span><br><span class="line">//把原来的rdd按照权重0.5, 0.1, 0.2, 0.2，随机划分到这4个RDD中，权重高的RDD，划分到//的几率就大一些。</span><br><span class="line">//注意，权重的总和加起来为1，否则会不正常 </span><br><span class="line">scala&gt; splitRDD(0).collect</span><br><span class="line">res10: Array[Int] = Array(1, 4)</span><br><span class="line"> </span><br><span class="line">scala&gt; splitRDD(1).collect</span><br><span class="line">res11: Array[Int] = Array(3)                                                    </span><br><span class="line"> </span><br><span class="line">scala&gt; splitRDD(2).collect</span><br><span class="line">res12: Array[Int] = Array(5, 9)</span><br><span class="line"> </span><br><span class="line">scala&gt; splitRDD(3).collect</span><br><span class="line">res13: Array[Int] = Array(2, 6, 7, 8, 10)</span><br></pre></td></tr></table></figure></div><h2 id="glom"><a href="#glom" class="headerlink" title="glom"></a>glom</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def glom(): RDD[Array[T]]</span><br><span class="line">该函数是将RDD中每一个分区中类型为T的元素转换成Array[T]，这样每一个分区就只有一个数组元素。</span><br><span class="line"></span><br><span class="line">scala&gt; var rdd = sc.makeRDD(1 to 10,3)</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[38] at makeRDD at :21</span><br><span class="line">scala&gt; rdd.partitions.size</span><br><span class="line">res33: Int = 3  //该RDD有3个分区</span><br><span class="line">scala&gt; rdd.glom().collect</span><br><span class="line">res35: Array[Array[Int]] = Array(Array(1, 2, 3), Array(4, 5, 6), Array(7, 8, 9, 10))</span><br><span class="line">//glom将每个分区中的元素放到一个数组中，这样，结果就变成了3个数组</span><br></pre></td></tr></table></figure></div><h2 id="union并集"><a href="#union并集" class="headerlink" title="union并集"></a>union并集</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val rdd1 = sc.parallelize(List(5, 6, 4, 3))</span><br><span class="line">val rdd2 = sc.parallelize(List(1, 2, 3, 4))</span><br><span class="line">//求并集</span><br><span class="line">val rdd3 = rdd1.union(rdd2)</span><br><span class="line">rdd3.collect</span><br></pre></td></tr></table></figure></div><h2 id="distinct"><a href="#distinct" class="headerlink" title="distinct"></a>distinct</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">去重</span><br><span class="line">val rdd1 = sc.parallelize(List(5, 6, 4, 3))</span><br><span class="line">val rdd2 = sc.parallelize(List(1, 2, 3, 4))</span><br><span class="line">//求并集</span><br><span class="line">val rdd3 = rdd1.union(rdd2)</span><br><span class="line">//去重输出</span><br><span class="line">rdd3.distinct.collect</span><br></pre></td></tr></table></figure></div><h2 id="intersection交集"><a href="#intersection交集" class="headerlink" title="intersection交集"></a>intersection交集</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val rdd1 = sc.parallelize(List(5, 6, 4, 3))</span><br><span class="line">val rdd2 = sc.parallelize(List(1, 2, 3, 4))</span><br><span class="line">//求交集</span><br><span class="line">val rdd4 = rdd1.intersection(rdd2) </span><br><span class="line">rdd4.collect</span><br></pre></td></tr></table></figure></div><h2 id="subtract"><a href="#subtract" class="headerlink" title="subtract"></a>subtract</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def subtract(other: RDD[T]): RDD[T]</span><br><span class="line">def subtract(other: RDD[T], numPartitions: Int): RDD[T]</span><br><span class="line">def subtract(other: RDD[T], partitioner: Partitioner)(implicit ord: Ordering[T] = null): RDD[T]</span><br><span class="line">该函数返回在RDD中出现，并且不在otherRDD中出现的元素，不去重。</span><br><span class="line"></span><br><span class="line">val rdd1 = sc.parallelize(List(5, 6, 6, 4, 3))</span><br><span class="line">    val rdd2 = sc.parallelize(List(1, 2, 3, 4))</span><br><span class="line">    //求差集</span><br><span class="line">    val rdd4 = rdd1.subtract(rdd2)</span><br><span class="line">rdd4.collect</span><br></pre></td></tr></table></figure></div><h2 id="subtractByKey"><a href="#subtractByKey" class="headerlink" title="subtractByKey"></a>subtractByKey</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def subtractByKey[W](other: RDD[(K, W)])(implicit arg0: ClassTag[W]): RDD[(K, V)]</span><br><span class="line">def subtractByKey[W](other: RDD[(K, W)], numPartitions: Int)(implicit arg0: ClassTag[W]): RDD[(K, V)]</span><br><span class="line">def subtractByKey[W](other: RDD[(K, W)], p: Partitioner)(implicit arg0: ClassTag[W]): RDD[(K, V)]</span><br><span class="line"></span><br><span class="line">subtractByKey和基本转换操作中的subtract类似，只不过这里是针对K的，返回在主RDD中出现，并且不在otherRDD中出现的元素。</span><br><span class="line">参数numPartitions用于指定结果的分区数</span><br><span class="line">参数partitioner用于指定分区函数</span><br><span class="line"></span><br><span class="line">var rdd1 = sc.makeRDD(Array((<span class="string">"A"</span>,<span class="string">"1"</span>),(<span class="string">"B"</span>,<span class="string">"2"</span>),(<span class="string">"C"</span>,<span class="string">"3"</span>)),2)</span><br><span class="line">var rdd2 = sc.makeRDD(Array((<span class="string">"A"</span>,<span class="string">"a"</span>),(<span class="string">"C"</span>,<span class="string">"c"</span>),(<span class="string">"D"</span>,<span class="string">"d"</span>)),2) </span><br><span class="line">scala&gt; rdd1.subtractByKey(rdd2).collect</span><br><span class="line">res13: Array[(String, String)] = Array((B,2))</span><br></pre></td></tr></table></figure></div><h2 id="groupbyKey"><a href="#groupbyKey" class="headerlink" title="groupbyKey"></a>groupbyKey</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">val rdd1 = sc.parallelize(List((<span class="string">"tom"</span>, 1), (<span class="string">"jerry"</span>, 3), (<span class="string">"kitty"</span>, 2)))</span><br><span class="line">    val rdd2 = sc.parallelize(List((<span class="string">"jerry"</span>, 2), (<span class="string">"tom"</span>, 1), (<span class="string">"shuke"</span>, 2)))</span><br><span class="line">    //求并集</span><br><span class="line">    val rdd4 = rdd1 union rdd2</span><br><span class="line">    //按key进行分组</span><br><span class="line">    val rdd5 = rdd4.groupByKey</span><br><span class="line">rdd5.collect</span><br></pre></td></tr></table></figure></div><h2 id="reduceByKey"><a href="#reduceByKey" class="headerlink" title="reduceByKey"></a>reduceByKey</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">顾名思义，reduceByKey就是对元素为KV对的RDD中Key相同的元素的Value进行reduce，因此，Key相同的多个元素的值被reduce为一个值，然后与原RDD中的Key组成一个新的KV对。</span><br><span class="line">举例:</span><br><span class="line">val rdd1 = sc.parallelize(List((<span class="string">"tom"</span>, 1), (<span class="string">"jerry"</span>, 3), (<span class="string">"kitty"</span>, 2)))</span><br><span class="line">    val rdd2 = sc.parallelize(List((<span class="string">"jerry"</span>, 2), (<span class="string">"tom"</span>, 1), (<span class="string">"shuke"</span>, 2)))</span><br><span class="line">    //求并集</span><br><span class="line">    val rdd4 = rdd1 union rdd2</span><br><span class="line">    //按key进行分组</span><br><span class="line">    val rdd6 = rdd4.reduceByKey(_ + _)</span><br><span class="line">    rdd6.collect()</span><br></pre></td></tr></table></figure></div><h2 id="sortByKey"><a href="#sortByKey" class="headerlink" title="sortByKey"></a>sortByKey</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">将List((<span class="string">"tom"</span>, 1), (<span class="string">"jerry"</span>, 3), (<span class="string">"kitty"</span>, 2),  (<span class="string">"shuke"</span>, 1))和List((<span class="string">"jerry"</span>, 2), (<span class="string">"tom"</span>, 3), (<span class="string">"shuke"</span>, 2), (<span class="string">"kitty"</span>, 5))做wordcount，并按名称排序</span><br><span class="line">val rdd1 = sc.parallelize(List((<span class="string">"tom"</span>, 1), (<span class="string">"jerry"</span>, 3), (<span class="string">"kitty"</span>, 2), (<span class="string">"shuke"</span>, 1)))</span><br><span class="line">    val rdd2 = sc.parallelize(List((<span class="string">"jerry"</span>, 2), (<span class="string">"tom"</span>, 3), (<span class="string">"shuke"</span>, 2), (<span class="string">"kitty"</span>, 5)))</span><br><span class="line">    val rdd3 = rdd1.union(rdd2)</span><br><span class="line">    //按key进行聚合</span><br><span class="line">    val rdd4 = rdd3.reduceByKey(_ + _)</span><br><span class="line">    //<span class="literal">false</span>降序</span><br><span class="line">    val rdd5 = rdd4.sortByKey(<span class="literal">false</span>)</span><br><span class="line">rdd5.collect</span><br></pre></td></tr></table></figure></div><h2 id="sortBy"><a href="#sortBy" class="headerlink" title="sortBy"></a>sortBy</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">将List((<span class="string">"tom"</span>, 1), (<span class="string">"jerry"</span>, 3), (<span class="string">"kitty"</span>, 2),  (<span class="string">"shuke"</span>, 1))和List((<span class="string">"jerry"</span>, 2), (<span class="string">"tom"</span>, 3), (<span class="string">"shuke"</span>, 2), (<span class="string">"kitty"</span>, 5))做wordcount，并按数值排序</span><br><span class="line">val rdd1 = sc.parallelize(List((<span class="string">"tom"</span>, 1), (<span class="string">"jerry"</span>, 3), (<span class="string">"kitty"</span>, 2), (<span class="string">"shuke"</span>, 1)))</span><br><span class="line">    val rdd2 = sc.parallelize(List((<span class="string">"jerry"</span>, 2), (<span class="string">"tom"</span>, 3), (<span class="string">"shuke"</span>, 2), (<span class="string">"kitty"</span>, 5)))</span><br><span class="line">    val rdd3 = rdd1.union(rdd2)</span><br><span class="line">    //按key进行聚合</span><br><span class="line">    val rdd4 = rdd3.reduceByKey(_ + _)</span><br><span class="line">    //<span class="literal">false</span>降序</span><br><span class="line">    val rdd5 = rdd4.sortBy(_._2, <span class="literal">false</span>)</span><br><span class="line">    rdd5.collect</span><br></pre></td></tr></table></figure></div><h2 id="zip"><a href="#zip" class="headerlink" title="zip"></a>zip</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">def zip[U](other: RDD[U])(implicit arg0: ClassTag[U]): RDD[(T, U)]</span><br><span class="line"></span><br><span class="line">zip函数用于将两个RDD组合成Key/Value形式的RDD,这里默认两个RDD的partition数量以及元素数量都相同，否则会抛出异常。</span><br><span class="line"></span><br><span class="line">scala&gt; var rdd1 = sc.makeRDD(1 to 5,2)</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at makeRDD at :21</span><br><span class="line"> </span><br><span class="line">scala&gt; var rdd2 = sc.makeRDD(Seq(<span class="string">"A"</span>,<span class="string">"B"</span>,<span class="string">"C"</span>,<span class="string">"D"</span>,<span class="string">"E"</span>),2)</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[2] at makeRDD at :21</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.zip(rdd2).collect</span><br><span class="line">res0: Array[(Int, String)] = Array((1,A), (2,B), (3,C), (4,D), (5,E))           </span><br><span class="line"> </span><br><span class="line">scala&gt; rdd2.zip(rdd1).collect</span><br><span class="line">res1: Array[(String, Int)] = Array((A,1), (B,2), (C,3), (D,4), (E,5))</span><br><span class="line"> </span><br><span class="line">scala&gt; var rdd3 = sc.makeRDD(Seq(<span class="string">"A"</span>,<span class="string">"B"</span>,<span class="string">"C"</span>,<span class="string">"D"</span>,<span class="string">"E"</span>),3)</span><br><span class="line">rdd3: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[5] at makeRDD at :21</span><br><span class="line">scala&gt; rdd1.zip(rdd3).collect</span><br><span class="line">java.lang.IllegalArgumentException: Can<span class="string">'t zip RDDs with unequal numbers of partitions</span></span><br><span class="line"><span class="string">//如果两个RDD分区数不同，则抛出异常</span></span><br></pre></td></tr></table></figure></div><h2 id="zipPartitions"><a href="#zipPartitions" class="headerlink" title="zipPartitions"></a>zipPartitions</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line">zipPartitions函数将多个RDD按照partition组合成为新的RDD，该函数需要组合的RDD具有相同的分区数，但对于每个分区内的元素数量没有要求。</span><br><span class="line">该函数有好几种实现，可分为三类：</span><br><span class="line"></span><br><span class="line">参数是一个RDD</span><br><span class="line">def zipPartitions[B, V](rdd2: RDD[B])(f: (Iterator[T], Iterator[B]) =&gt; Iterator[V])(implicit arg0: ClassTag[B], arg1: ClassTag[V]): RDD[V]</span><br><span class="line"></span><br><span class="line">def zipPartitions[B, V](rdd2: RDD[B], preservesPartitioning: Boolean)(f: (Iterator[T], Iterator[B]) =&gt; Iterator[V])(implicit arg0: ClassTag[B], arg1: ClassTag[V]): RDD[V]</span><br><span class="line"></span><br><span class="line">这两个区别就是参数preservesPartitioning，是否保留父RDD的partitioner分区信息</span><br><span class="line"></span><br><span class="line">映射方法f参数为两个RDD的迭代器。</span><br><span class="line"></span><br><span class="line">scala&gt; var rdd1 = sc.makeRDD(1 to 5,2)</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[22] at makeRDD at :21</span><br><span class="line"> </span><br><span class="line">scala&gt; var rdd2 = sc.makeRDD(Seq(<span class="string">"A"</span>,<span class="string">"B"</span>,<span class="string">"C"</span>,<span class="string">"D"</span>,<span class="string">"E"</span>),2)</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[23] at makeRDD at :21</span><br><span class="line"> </span><br><span class="line">//rdd1两个分区中元素分布：</span><br><span class="line">scala&gt; rdd1.mapPartitionsWithIndex&#123;</span><br><span class="line">     |         (x,iter) =&gt; &#123;</span><br><span class="line">     |           var result = List[String]()</span><br><span class="line">     |             <span class="keyword">while</span>(iter.hasNext)&#123;</span><br><span class="line">     |               result ::= (<span class="string">"part_"</span> + x + <span class="string">"|"</span> + iter.next())</span><br><span class="line">     |             &#125;</span><br><span class="line">     |             result.iterator</span><br><span class="line">     |            </span><br><span class="line">     |         &#125;</span><br><span class="line">     |       &#125;.collect</span><br><span class="line">res17: Array[String] = Array(part_0|2, part_0|1, part_1|5, part_1|4, part_1|3)</span><br><span class="line"> </span><br><span class="line">//rdd2两个分区中元素分布</span><br><span class="line">scala&gt; rdd2.mapPartitionsWithIndex&#123;</span><br><span class="line">     |         (x,iter) =&gt; &#123;</span><br><span class="line">     |           var result = List[String]()</span><br><span class="line">     |             <span class="keyword">while</span>(iter.hasNext)&#123;</span><br><span class="line">     |               result ::= (<span class="string">"part_"</span> + x + <span class="string">"|"</span> + iter.next())</span><br><span class="line">     |             &#125;</span><br><span class="line">     |             result.iterator</span><br><span class="line">     |            </span><br><span class="line">     |         &#125;</span><br><span class="line">     |       &#125;.collect</span><br><span class="line">res18: Array[String] = Array(part_0|B, part_0|A, part_1|E, part_1|D, part_1|C)</span><br><span class="line"> </span><br><span class="line">//rdd1和rdd2做zipPartition</span><br><span class="line">scala&gt; rdd1.zipPartitions(rdd2)&#123;</span><br><span class="line">     |       (rdd1Iter,rdd2Iter) =&gt; &#123;</span><br><span class="line">     |         var result = List[String]()</span><br><span class="line">     |         <span class="keyword">while</span>(rdd1Iter.hasNext &amp;&amp; rdd2Iter.hasNext) &#123;</span><br><span class="line">     |           result::=(rdd1Iter.next() + <span class="string">"_"</span> + rdd2Iter.next())</span><br><span class="line">     |         &#125;</span><br><span class="line">     |         result.iterator</span><br><span class="line">     |       &#125;</span><br><span class="line">     |     &#125;.collect</span><br><span class="line">res19: Array[String] = Array(2_B, 1_A, 5_E, 4_D, 3_C)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">参数是两个RDD</span><br><span class="line">def zipPartitions[B, C, V](rdd2: RDD[B], rdd3: RDD[C])(f: (Iterator[T], Iterator[B], Iterator[C]) =&gt; Iterator[V])(implicit arg0: ClassTag[B], arg1: ClassTag[C], arg2: ClassTag[V]): RDD[V]</span><br><span class="line"></span><br><span class="line">def zipPartitions[B, C, V](rdd2: RDD[B], rdd3: RDD[C], preservesPartitioning: Boolean)(f: (Iterator[T], Iterator[B], Iterator[C]) =&gt; Iterator[V])(implicit arg0: ClassTag[B], arg1: ClassTag[C], arg2: ClassTag[V]): RDD[V]</span><br><span class="line"></span><br><span class="line">用法同上面，只不过该函数参数为两个RDD，映射方法f输入参数为两个RDD的迭代器。</span><br><span class="line"></span><br><span class="line">scala&gt; var rdd1 = sc.makeRDD(1 to 5,2)</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[27] at makeRDD at :21</span><br><span class="line"> </span><br><span class="line">scala&gt; var rdd2 = sc.makeRDD(Seq(<span class="string">"A"</span>,<span class="string">"B"</span>,<span class="string">"C"</span>,<span class="string">"D"</span>,<span class="string">"E"</span>),2)</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[28] at makeRDD at :21</span><br><span class="line"> </span><br><span class="line">scala&gt; var rdd3 = sc.makeRDD(Seq(<span class="string">"a"</span>,<span class="string">"b"</span>,<span class="string">"c"</span>,<span class="string">"d"</span>,<span class="string">"e"</span>),2)</span><br><span class="line">rdd3: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[29] at makeRDD at :21</span><br><span class="line"> </span><br><span class="line">//rdd3中个分区元素分布</span><br><span class="line">scala&gt; rdd3.mapPartitionsWithIndex&#123;</span><br><span class="line">     |         (x,iter) =&gt; &#123;</span><br><span class="line">     |           var result = List[String]()</span><br><span class="line">     |             <span class="keyword">while</span>(iter.hasNext)&#123;</span><br><span class="line">     |               result ::= (<span class="string">"part_"</span> + x + <span class="string">"|"</span> + iter.next())</span><br><span class="line">     |             &#125;</span><br><span class="line">     |             result.iterator</span><br><span class="line">     |            </span><br><span class="line">     |         &#125;</span><br><span class="line">     |       &#125;.collect</span><br><span class="line">res21: Array[String] = Array(part_0|b, part_0|a, part_1|e, part_1|d, part_1|c)</span><br><span class="line"> </span><br><span class="line">//三个RDD做zipPartitions</span><br><span class="line">scala&gt; var rdd4 = rdd1.zipPartitions(rdd2,rdd3)&#123;</span><br><span class="line">     |       (rdd1Iter,rdd2Iter,rdd3Iter) =&gt; &#123;</span><br><span class="line">     |         var result = List[String]()</span><br><span class="line">     |         <span class="keyword">while</span>(rdd1Iter.hasNext &amp;&amp; rdd2Iter.hasNext &amp;&amp; rdd3Iter.hasNext) &#123;</span><br><span class="line">     |           result::=(rdd1Iter.next() + <span class="string">"_"</span> + rdd2Iter.next() + <span class="string">"_"</span> + rdd3Iter.next())</span><br><span class="line">     |         &#125;</span><br><span class="line">     |         result.iterator</span><br><span class="line">     |       &#125;</span><br><span class="line">     |     &#125;</span><br><span class="line">rdd4: org.apache.spark.rdd.RDD[String] = ZippedPartitionsRDD3[33] at zipPartitions at :27</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd4.collect</span><br><span class="line">res23: Array[String] = Array(2_B_b, 1_A_a, 5_E_e, 4_D_d, 3_C_c)</span><br><span class="line"> </span><br><span class="line">参数是三个RDD</span><br><span class="line">def zipPartitions[B, C, D, V](rdd2: RDD[B], rdd3: RDD[C], rdd4: RDD[D])(f: (Iterator[T], Iterator[B], Iterator[C], Iterator[D]) =&gt; Iterator[V])(implicit arg0: ClassTag[B], arg1: ClassTag[C], arg2: ClassTag[D], arg3: ClassTag[V]): RDD[V]</span><br><span class="line"></span><br><span class="line">def zipPartitions[B, C, D, V](rdd2: RDD[B], rdd3: RDD[C], rdd4: RDD[D], preservesPartitioning: Boolean)(f: (Iterator[T], Iterator[B], Iterator[C], Iterator[D]) =&gt; Iterator[V])(implicit arg0: ClassTag[B], arg1: ClassTag[C], arg2: ClassTag[D], arg3: ClassTag[V]): RDD[V]</span><br><span class="line"></span><br><span class="line">用法同上面，只不过这里又多了个一个RDD而已。</span><br></pre></td></tr></table></figure></div><h2 id="zipWithIndex"><a href="#zipWithIndex" class="headerlink" title="zipWithIndex"></a>zipWithIndex</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def zipWithIndex(): RDD[(T, Long)]</span><br><span class="line">该函数将RDD中的元素和这个元素在RDD中的ID（索引号）组合成键/值对。</span><br><span class="line">scala&gt; var rdd2 = sc.makeRDD(Seq(<span class="string">"A"</span>,<span class="string">"B"</span>,<span class="string">"R"</span>,<span class="string">"D"</span>,<span class="string">"F"</span>),2)</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[34] at makeRDD at :21</span><br><span class="line">scala&gt; rdd2.zipWithIndex().collect</span><br><span class="line">res27: Array[(String, Long)] = Array((A,0), (B,1), (R,2), (D,3), (F,4))</span><br></pre></td></tr></table></figure></div><h2 id="zipWithUniqueId"><a href="#zipWithUniqueId" class="headerlink" title="zipWithUniqueId"></a>zipWithUniqueId</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">def zipWithUniqueId(): RDD[(T, Long)]</span><br><span class="line">该函数将RDD中元素和一个唯一ID组合成键/值对，该唯一ID生成算法如下：</span><br><span class="line">每个分区中第一个元素的唯一ID值为：该分区索引号，</span><br><span class="line">每个分区中第N个元素的唯一ID值为：(前一个元素的唯一ID值) + (该RDD总的分区数)</span><br><span class="line">看下面的例子：</span><br><span class="line">scala&gt; var rdd1 = sc.makeRDD(Seq(<span class="string">"A"</span>,<span class="string">"B"</span>,<span class="string">"C"</span>,<span class="string">"D"</span>,<span class="string">"E"</span>,<span class="string">"F"</span>),2)</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[44] at makeRDD at :21</span><br><span class="line">//rdd1有两个分区，</span><br><span class="line">scala&gt; rdd1.zipWithUniqueId().collect</span><br><span class="line">res32: Array[(String, Long)] = Array((A,0), (B,2), (C,4), (D,1), (E,3), (F,5))</span><br><span class="line">//总分区数为2</span><br><span class="line">//第一个分区第一个元素ID为0，第二个分区第一个元素ID为1</span><br><span class="line">//第一个分区第二个元素ID为0+2=2，第一个分区第三个元素ID为2+2=4</span><br><span class="line">//第二个分区第二个元素ID为1+2=3，第二个分区第三个元素ID为3+2=5</span><br></pre></td></tr></table></figure></div><h1 id="键值转换"><a href="#键值转换" class="headerlink" title="键值转换"></a>键值转换</h1><h2 id="partitionBy"><a href="#partitionBy" class="headerlink" title="partitionBy"></a>partitionBy</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">def partitionBy(partitioner: Partitioner): RDD[(K, V)]</span><br><span class="line">该函数根据partitioner函数生成新的ShuffleRDD，将原RDD重新分区。</span><br><span class="line">scala&gt; var rdd1 = sc.makeRDD(Array((1,<span class="string">"A"</span>),(2,<span class="string">"B"</span>),(3,<span class="string">"C"</span>),(4,<span class="string">"D"</span>)),2)</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[23] at makeRDD at :21</span><br><span class="line">scala&gt; rdd1.partitions.size</span><br><span class="line">res20: Int = 2</span><br><span class="line"> </span><br><span class="line">//查看rdd1中每个分区的元素</span><br><span class="line">scala&gt; rdd1.mapPartitionsWithIndex&#123;</span><br><span class="line">     |         (partIdx,iter) =&gt; &#123;</span><br><span class="line">     |           var part_map = scala.collection.mutable.Map[String,List[(Int,String)]]()</span><br><span class="line">     |             <span class="keyword">while</span>(iter.hasNext)&#123;</span><br><span class="line">     |               var part_name = <span class="string">"part_"</span> + partIdx;</span><br><span class="line">     |               var elem = iter.next()</span><br><span class="line">     |               <span class="keyword">if</span>(part_map.contains(part_name)) &#123;</span><br><span class="line">     |                 var elems = part_map(part_name)</span><br><span class="line">     |                 elems ::= elem</span><br><span class="line">     |                 part_map(part_name) = elems</span><br><span class="line">     |               &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">     |                 part_map(part_name) = List[(Int,String)]&#123;elem&#125;</span><br><span class="line">     |               &#125;</span><br><span class="line">     |             &#125;</span><br><span class="line">     |             part_map.iterator</span><br><span class="line">     |            </span><br><span class="line">     |         &#125;</span><br><span class="line">     |       &#125;.collect</span><br><span class="line">res22: Array[(String, List[(Int, String)])] = Array((part_0,List((2,B), (1,A))), (part_1,List((4,D), (3,C))))</span><br><span class="line">//(2,B),(1,A)在part_0中，(4,D),(3,C)在part_1中</span><br><span class="line"> </span><br><span class="line">//使用partitionBy重分区</span><br><span class="line">scala&gt; var rdd2 = rdd1.partitionBy(new org.apache.spark.HashPartitioner(2))</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[(Int, String)] = ShuffledRDD[25] at partitionBy at :23</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd2.partitions.size</span><br><span class="line">res23: Int = 2</span><br><span class="line"> </span><br><span class="line">//查看rdd2中每个分区的元素</span><br><span class="line">scala&gt; rdd2.mapPartitionsWithIndex&#123;</span><br><span class="line">     |         (partIdx,iter) =&gt; &#123;</span><br><span class="line">     |           var part_map = scala.collection.mutable.Map[String,List[(Int,String)]]()</span><br><span class="line">     |             <span class="keyword">while</span>(iter.hasNext)&#123;</span><br><span class="line">     |               var part_name = <span class="string">"part_"</span> + partIdx;</span><br><span class="line">     |               var elem = iter.next()</span><br><span class="line">     |               <span class="keyword">if</span>(part_map.contains(part_name)) &#123;</span><br><span class="line">     |                 var elems = part_map(part_name)</span><br><span class="line">     |                 elems ::= elem</span><br><span class="line">     |                 part_map(part_name) = elems</span><br><span class="line">     |               &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">     |                 part_map(part_name) = List[(Int,String)]&#123;elem&#125;</span><br><span class="line">     |               &#125;</span><br><span class="line">     |             &#125;</span><br><span class="line">     |             part_map.iterator</span><br><span class="line">     |         &#125;</span><br><span class="line">     |       &#125;.collect</span><br><span class="line">res24: Array[(String, List[(Int, String)])] = Array((part_0,List((4,D), (2,B))), (part_1,List((3,C), (1,A))))</span><br><span class="line">//(4,D),(2,B)在part_0中，(3,C),(1,A)在part_1中</span><br></pre></td></tr></table></figure></div><h2 id="mapValues"><a href="#mapValues" class="headerlink" title="mapValues"></a>mapValues</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mapValues顾名思义就是输入函数应用于RDD中Kev-Value的Value，原RDD中的Key保持不变，与新的Value一起组成新的RDD中的元素。因此，该函数只适用于元素为KV对的RDD。</span><br><span class="line">举例：</span><br><span class="line">scala&gt; val a = sc.parallelize(List(<span class="string">"dog"</span>, <span class="string">"tiger"</span>, <span class="string">"lion"</span>, <span class="string">"cat"</span>, <span class="string">"panther"</span>, <span class="string">" eagle"</span>), 2)</span><br><span class="line">scala&gt; val b = a.map(x =&gt; (x.length, x))</span><br><span class="line">scala&gt; b.mapValues(<span class="string">"x"</span> + _ + <span class="string">"x"</span>).collect</span><br><span class="line">res5: Array[(Int, String)] = Array((3,xdogx), (5,xtigerx), (4,xlionx),(3,xcatx), (7,xpantherx), (5,xeaglex))</span><br></pre></td></tr></table></figure></div><h2 id="flatMapValues"><a href="#flatMapValues" class="headerlink" title="flatMapValues"></a>flatMapValues</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">flatMapValues类似于mapValues，不同的在于flatMapValues应用于元素为KV对的RDD中Value。每个一元素的Value被输入函数映射为一系列的值，然后这些值再与原RDD中的Key组成一系列新的KV对。</span><br><span class="line">举例</span><br><span class="line">val a = sc.parallelize(List((1, 2), (3, 4), (5, 6)))</span><br><span class="line">    val b = a.flatMapValues(x =&gt; 1.to(x))</span><br><span class="line">    b.collect.foreach(println)</span><br></pre></td></tr></table></figure></div><h2 id="combineByKey"><a href="#combineByKey" class="headerlink" title="combineByKey"></a>combineByKey</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">def combineByKey[C](createCombiner: (V) =&gt; C, mergeValue: (C, V) =&gt; C, mergeCombiners: (C, C) =&gt; C): RDD[(K, C)]</span><br><span class="line">def combineByKey[C](createCombiner: (V) =&gt; C, mergeValue: (C, V) =&gt; C, mergeCombiners: (C, C) =&gt; C, numPartitions: Int): RDD[(K, C)]</span><br><span class="line">def combineByKey[C](createCombiner: (V) =&gt; C, mergeValue: (C, V) =&gt; C, mergeCombiners: (C, C) =&gt; C, partitioner: Partitioner, mapSideCombine: Boolean = <span class="literal">true</span>, serializer: Serializer = null): RDD[(K, C)]</span><br><span class="line">该函数用于将RDD[K,V]转换成RDD[K,C],这里的V类型和C类型可以相同也可以不同。</span><br><span class="line">其中的参数：</span><br><span class="line">createCombiner：组合器函数，用于将V类型转换成C类型，输入参数为RDD[K,V]中的V,输出为C ,分区内相同的key做一次</span><br><span class="line">mergeValue：合并值函数，将一个C类型和一个V类型值合并成一个C类型，输入参数为(C,V)，输出为C，分区内相同的key循环做</span><br><span class="line">mergeCombiners：分区合并组合器函数，用于将两个C类型值合并成一个C类型，输入参数为(C,C)，输出为C，分区之间循环做</span><br><span class="line">numPartitions：结果RDD分区数，默认保持原有的分区数</span><br><span class="line">partitioner：分区函数,默认为HashPartitioner</span><br><span class="line">mapSideCombine：是否需要在Map端进行combine操作，类似于MapReduce中的combine，默认为<span class="literal">true</span></span><br><span class="line"></span><br><span class="line">看下面例子：</span><br><span class="line">scala&gt; var rdd1 = sc.makeRDD(Array((<span class="string">"A"</span>,1),(<span class="string">"A"</span>,2),(<span class="string">"B"</span>,1),(<span class="string">"B"</span>,2),(<span class="string">"C"</span>,1)))</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[64] at makeRDD at :21 </span><br><span class="line">scala&gt; rdd1.combineByKey(</span><br><span class="line">     |       (v : Int) =&gt; v + <span class="string">"_"</span>,   </span><br><span class="line">     |       (c : String, v : Int) =&gt; c + <span class="string">"@"</span> + v,  </span><br><span class="line">     |       (c1 : String, c2 : String) =&gt; c1 + <span class="string">"$"</span> + c2</span><br><span class="line">     |     ).collect</span><br><span class="line">res60: Array[(String, String)] = Array((A,2_<span class="variable">$1_</span>), (B,1_<span class="variable">$2_</span>), (C,1_))</span><br><span class="line"></span><br><span class="line">其中三个映射函数分别为：</span><br><span class="line">createCombiner: (V) =&gt; C</span><br><span class="line">(v : Int) =&gt; v + “_” //在每一个V值后面加上字符_，返回C类型(String)</span><br><span class="line">mergeValue: (C, V) =&gt; C</span><br><span class="line">(c : String, v : Int) =&gt; c + “@” + v //合并C类型和V类型，中间加字符@,返回C(String)</span><br><span class="line">mergeCombiners: (C, C) =&gt; C</span><br><span class="line">(c1 : String, c2 : String) =&gt; c1 + “$” + c2 //合并C类型和C类型，中间加$，返回C(String)</span><br><span class="line">其他参数为默认值。</span><br><span class="line">最终，将RDD[String,Int]转换为RDD[String,String]。</span><br><span class="line"></span><br><span class="line">再看例子：</span><br><span class="line"></span><br><span class="line">rdd1.combineByKey(</span><br><span class="line">      (v : Int) =&gt; List(v),</span><br><span class="line">      (c : List[Int], v : Int) =&gt; v :: c,</span><br><span class="line">      (c1 : List[Int], c2 : List[Int]) =&gt; c1 ::: c2</span><br><span class="line">).collect</span><br><span class="line">res65: Array[(String, List[Int])] = Array((A,List(2, 1)), (B,List(2, 1)), (C,List(1)))</span><br><span class="line">最终将RDD[String,Int]转换为RDD[String,List[Int]]。</span><br></pre></td></tr></table></figure></div><h2 id="foldByKey"><a href="#foldByKey" class="headerlink" title="foldByKey"></a>foldByKey</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">def foldByKey(zeroValue: V)(func: (V, V) =&gt; V): RDD[(K, V)]</span><br><span class="line"></span><br><span class="line">def foldByKey(zeroValue: V, numPartitions: Int)(func: (V, V) =&gt; V): RDD[(K, V)]</span><br><span class="line"></span><br><span class="line">def foldByKey(zeroValue: V, partitioner: Partitioner)(func: (V, V) =&gt; V): RDD[(K, V)]</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">该函数用于RDD[K,V]根据K将V做折叠、合并处理，其中的参数zeroValue表示先根据映射函数将zeroValue应用于V,进行初始化V,再将映射函数应用于初始化后的V.</span><br><span class="line"></span><br><span class="line">例子：</span><br><span class="line"></span><br><span class="line">scala&gt; var rdd1 = sc.makeRDD(Array((<span class="string">"A"</span>,0),(<span class="string">"A"</span>,2),(<span class="string">"B"</span>,1),(<span class="string">"B"</span>,2),(<span class="string">"C"</span>,1)))</span><br><span class="line">scala&gt; rdd1.foldByKey(0)(_+_).collect</span><br><span class="line">res75: Array[(String, Int)] = Array((A,2), (B,3), (C,1)) </span><br><span class="line">//将rdd1中每个key对应的V进行累加，注意zeroValue=0,需要先初始化V,映射函数为+操</span><br><span class="line">//作，比如(<span class="string">"A"</span>,0), (<span class="string">"A"</span>,2)，先将zeroValue应用于每个V,得到：(<span class="string">"A"</span>,0+0), (<span class="string">"A"</span>,2+0)，即：</span><br><span class="line">//(<span class="string">"A"</span>,0), (<span class="string">"A"</span>,2)，再将映射函数应用于初始化后的V，最后得到(A,0+2),即(A,2)</span><br><span class="line"> </span><br><span class="line">再看：</span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.foldByKey(2)(_+_).collect</span><br><span class="line">res76: Array[(String, Int)] = Array((A,6), (B,7), (C,3))</span><br><span class="line">//先将zeroValue=2应用于每个V,得到：(<span class="string">"A"</span>,0+2), (<span class="string">"A"</span>,2+2)，即：(<span class="string">"A"</span>,2), (<span class="string">"A"</span>,4)，再将映射函</span><br><span class="line">//数应用于初始化后的V，最后得到：(A,2+4)，即：(A,6)</span><br><span class="line"> </span><br><span class="line">再看乘法操作：</span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.foldByKey(0)(_*_).collect</span><br><span class="line">res77: Array[(String, Int)] = Array((A,0), (B,0), (C,0))</span><br><span class="line">//先将zeroValue=0应用于每个V,注意，这次映射函数为乘法，得到：(<span class="string">"A"</span>,0*0), (<span class="string">"A"</span>,2*0)，</span><br><span class="line">//即：(<span class="string">"A"</span>,0), (<span class="string">"A"</span>,0)，再将映射函//数应用于初始化后的V，最后得到：(A,0*0)，即：(A,0)</span><br><span class="line">//其他K也一样，最终都得到了V=0</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.foldByKey(1)(_*_).collect</span><br><span class="line">res78: Array[(String, Int)] = Array((A,0), (B,2), (C,1))</span><br><span class="line">//映射函数为乘法时，需要将zeroValue设为1，才能得到我们想要的结果。</span><br><span class="line"> </span><br><span class="line">在使用foldByKey算子时候，要特别注意映射函数及zeroValue的取值。</span><br></pre></td></tr></table></figure></div><h2 id="reduceByKeyLocally"><a href="#reduceByKeyLocally" class="headerlink" title="reduceByKeyLocally"></a>reduceByKeyLocally</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def reduceByKeyLocally(func: (V, V) =&gt; V): Map[K, V]</span><br><span class="line"></span><br><span class="line">该函数将RDD[K,V]中每个K对应的V值根据映射函数来运算，运算结果映射到一个Map[K,V]中，而不是RDD[K,V]。</span><br><span class="line"></span><br><span class="line">scala&gt; var rdd1 = sc.makeRDD(Array((<span class="string">"A"</span>,0),(<span class="string">"A"</span>,2),(<span class="string">"B"</span>,1),(<span class="string">"B"</span>,2),(<span class="string">"C"</span>,1)))</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[91] at makeRDD at :21</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.reduceByKeyLocally((x,y) =&gt; x + y)</span><br><span class="line">res90: scala.collection.Map[String,Int] = Map(B -&gt; 3, A -&gt; 2, C -&gt; 1)</span><br></pre></td></tr></table></figure></div><h2 id="cogroup和groupByKey的区别"><a href="#cogroup和groupByKey的区别" class="headerlink" title="***cogroup和groupByKey的区别"></a>***cogroup和groupByKey的区别</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">val rdd1 = sc.parallelize(List((<span class="string">"tom"</span>, 1), (<span class="string">"tom"</span>, 2), (<span class="string">"jerry"</span>, 3), (<span class="string">"kitty"</span>, 2)))</span><br><span class="line">   val rdd2 = sc.parallelize(List((<span class="string">"jerry"</span>, 2), (<span class="string">"tom"</span>, 1), (<span class="string">"shuke"</span>, 2)))</span><br><span class="line">   //cogroup</span><br><span class="line">   val rdd3 = rdd1.cogroup(rdd2)</span><br><span class="line">   //groupbykey</span><br><span class="line">   val rdd4 = rdd1.union(rdd2).groupByKey</span><br><span class="line">   //注意cogroup与groupByKey的区别</span><br><span class="line">   rdd3.foreach(println)</span><br><span class="line">   rdd4.foreach(println)</span><br></pre></td></tr></table></figure></div><h2 id="join"><a href="#join" class="headerlink" title="join"></a>join</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val rdd1 = sc.parallelize(List((<span class="string">"tom"</span>, 1), (<span class="string">"jerry"</span>, 3), (<span class="string">"kitty"</span>, 2)))</span><br><span class="line">val rdd2 = sc.parallelize(List((<span class="string">"jerry"</span>, 2), (<span class="string">"tom"</span>, 1), (<span class="string">"shuke"</span>, 2)))</span><br><span class="line">//求jion</span><br><span class="line">val rdd3 = rdd1.join(rdd2)</span><br><span class="line">rdd3.collect</span><br></pre></td></tr></table></figure></div><h2 id="leftOuterJoin"><a href="#leftOuterJoin" class="headerlink" title="leftOuterJoin"></a>leftOuterJoin</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def leftOuterJoin[W](other: RDD[(K, W)]): RDD[(K, (V, Option[W]))]</span><br><span class="line">def leftOuterJoin[W](other: RDD[(K, W)], numPartitions: Int): RDD[(K, (V, Option[W]))]</span><br><span class="line">def leftOuterJoin[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (V, Option[W]))]</span><br><span class="line"></span><br><span class="line">leftOuterJoin类似于SQL中的左外关联left outer join，返回结果以前面的RDD为主，关联不上的记录为空。只能用于两个RDD之间的关联，如果要多个RDD关联，多关联几次即可。</span><br><span class="line">参数numPartitions用于指定结果的分区数</span><br><span class="line">参数partitioner用于指定分区函数</span><br><span class="line"></span><br><span class="line">var rdd1 = sc.makeRDD(Array((<span class="string">"A"</span>,<span class="string">"1"</span>),(<span class="string">"B"</span>,<span class="string">"2"</span>),(<span class="string">"C"</span>,<span class="string">"3"</span>)),2)</span><br><span class="line">var rdd2 = sc.makeRDD(Array((<span class="string">"A"</span>,<span class="string">"a"</span>),(<span class="string">"C"</span>,<span class="string">"c"</span>),(<span class="string">"D"</span>,<span class="string">"d"</span>)),2)</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.leftOuterJoin(rdd2).collect</span><br><span class="line">res11: Array[(String, (String, Option[String]))] = Array((B,(2,None)), (A,(1,Some(a))), (C,(3,Some(c))))</span><br></pre></td></tr></table></figure></div><h2 id="rightOuterJoin"><a href="#rightOuterJoin" class="headerlink" title="rightOuterJoin"></a>rightOuterJoin</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def rightOuterJoin[W](other: RDD[(K, W)]): RDD[(K, (Option[V], W))]</span><br><span class="line">def rightOuterJoin[W](other: RDD[(K, W)], numPartitions: Int): RDD[(K, (Option[V], W))]</span><br><span class="line">def rightOuterJoin[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (Option[V], W))] </span><br><span class="line"></span><br><span class="line">rightOuterJoin类似于SQL中的有外关联right outer join，返回结果以参数中的RDD为主，关联不上的记录为空。只能用于两个RDD之间的关联，如果要多个RDD关联，多关联几次即可。</span><br><span class="line">参数numPartitions用于指定结果的分区数</span><br><span class="line">参数partitioner用于指定分区函数</span><br><span class="line"></span><br><span class="line">var rdd1 = sc.makeRDD(Array((<span class="string">"A"</span>,<span class="string">"1"</span>),(<span class="string">"B"</span>,<span class="string">"2"</span>),(<span class="string">"C"</span>,<span class="string">"3"</span>)),2)</span><br><span class="line">var rdd2 = sc.makeRDD(Array((<span class="string">"A"</span>,<span class="string">"a"</span>),(<span class="string">"C"</span>,<span class="string">"c"</span>),(<span class="string">"D"</span>,<span class="string">"d"</span>)),2)</span><br><span class="line">scala&gt; rdd1.rightOuterJoin(rdd2).collect</span><br><span class="line">res12: Array[(String, (Option[String], String))] = Array((D,(None,d)), (A,(Some(1),a)), (C,(Some(3),c)))</span><br></pre></td></tr></table></figure></div><h1 id="Action操作"><a href="#Action操作" class="headerlink" title="Action操作"></a>Action操作</h1><h2 id="first"><a href="#first" class="headerlink" title="first"></a>first</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def first(): T</span><br><span class="line"></span><br><span class="line">first返回RDD中的第一个元素，不排序。</span><br><span class="line"></span><br><span class="line">scala&gt; var rdd1 = sc.makeRDD(Array((<span class="string">"A"</span>,<span class="string">"1"</span>),(<span class="string">"B"</span>,<span class="string">"2"</span>),(<span class="string">"C"</span>,<span class="string">"3"</span>)),2)</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[33] at makeRDD at :21</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.first</span><br><span class="line">res14: (String, String) = (A,1)</span><br><span class="line"> </span><br><span class="line">scala&gt; var rdd1 = sc.makeRDD(Seq(10, 4, 2, 12, 3))</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at makeRDD at :21</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.first</span><br><span class="line">res8: Int = 10</span><br></pre></td></tr></table></figure></div><h2 id="count"><a href="#count" class="headerlink" title="count"></a>count</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def count(): Long</span><br><span class="line"></span><br><span class="line">count返回RDD中的元素数量。</span><br><span class="line"></span><br><span class="line">scala&gt; var rdd1 = sc.makeRDD(Array((<span class="string">"A"</span>,<span class="string">"1"</span>),(<span class="string">"B"</span>,<span class="string">"2"</span>),(<span class="string">"C"</span>,<span class="string">"3"</span>)),2)</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[34] at makeRDD at :21</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.count</span><br><span class="line">res15: Long = 3</span><br></pre></td></tr></table></figure></div><h2 id="reduce"><a href="#reduce" class="headerlink" title="reduce"></a>reduce</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">def reduce(f: (T, T) ⇒ T): T</span><br><span class="line"></span><br><span class="line">根据映射函数f，对RDD中的元素进行二元计算，返回计算结果。</span><br><span class="line"></span><br><span class="line">scala&gt; var rdd1 = sc.makeRDD(1 to 10,2)</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[36] at makeRDD at :21</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.reduce(_ + _)</span><br><span class="line">res18: Int = 55</span><br><span class="line"> </span><br><span class="line">scala&gt; var rdd2 = sc.makeRDD(Array((<span class="string">"A"</span>,0),(<span class="string">"A"</span>,2),(<span class="string">"B"</span>,1),(<span class="string">"B"</span>,2),(<span class="string">"C"</span>,1)))</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[38] at makeRDD at :21</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd2.reduce((x,y) =&gt; &#123;</span><br><span class="line">     |       (x._1 + y._1,x._2 + y._2)</span><br><span class="line">     |     &#125;)</span><br><span class="line">res21: (String, Int) = (CBBAA,6)</span><br><span class="line"> </span><br><span class="line">collect</span><br><span class="line"></span><br><span class="line">def collect(): Array[T]</span><br><span class="line"></span><br><span class="line">collect用于将一个RDD转换成数组。</span><br><span class="line"></span><br><span class="line">scala&gt; var rdd1 = sc.makeRDD(1 to 10,2)</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[36] at makeRDD at :21</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.collect</span><br><span class="line">res23: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)</span><br></pre></td></tr></table></figure></div><h2 id="take"><a href="#take" class="headerlink" title="take"></a>take</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def take(num: Int): Array[T]</span><br><span class="line"></span><br><span class="line">take用于获取RDD中从0到num-1下标的元素，不排序。</span><br><span class="line"></span><br><span class="line">scala&gt; var rdd1 = sc.makeRDD(Seq(10, 4, 2, 12, 3))</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[40] at makeRDD at :21</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.take(1)</span><br><span class="line">res0: Array[Int] = Array(10)                                                    </span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.take(2)</span><br><span class="line">res1: Array[Int] = Array(10, 4)</span><br></pre></td></tr></table></figure></div><h2 id="top"><a href="#top" class="headerlink" title="top"></a>top</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">def top(num: Int)(implicit ord: Ordering[T]): Array[T]</span><br><span class="line"></span><br><span class="line">top函数用于从RDD中，按照默认（降序）或者指定的排序规则，返回前num个元素。</span><br><span class="line"></span><br><span class="line">scala&gt; var rdd1 = sc.makeRDD(Seq(10, 4, 2, 12, 3))</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[40] at makeRDD at :21</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.top(1)</span><br><span class="line">res2: Array[Int] = Array(12)</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.top(2)</span><br><span class="line">res3: Array[Int] = Array(12, 10)</span><br><span class="line"> </span><br><span class="line">//指定排序规则</span><br><span class="line">scala&gt; implicit val myOrd = implicitly[Ordering[Int]].reverse</span><br><span class="line">myOrd: scala.math.Ordering[Int] = scala.math.Ordering$<span class="variable">$anon</span><span class="variable">$4</span>@767499ef</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.top(1)</span><br><span class="line">res4: Array[Int] = Array(2)</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.top(2)</span><br><span class="line">res5: Array[Int] = Array(2, 3)</span><br></pre></td></tr></table></figure></div><h2 id="takeOrdered"><a href="#takeOrdered" class="headerlink" title="takeOrdered"></a>takeOrdered</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">def takeOrdered(num: Int)(implicit ord: Ordering[T]): Array[T]</span><br><span class="line"></span><br><span class="line">takeOrdered和top类似，只不过以和top相反的顺序返回元素。</span><br><span class="line"></span><br><span class="line">scala&gt; var rdd1 = sc.makeRDD(Seq(10, 4, 2, 12, 3))</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[40] at makeRDD at :21</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.top(1)</span><br><span class="line">res4: Array[Int] = Array(12)</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.top(2)</span><br><span class="line">res5: Array[Int] = Array(12, 10)</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.takeOrdered(1)</span><br><span class="line">res6: Array[Int] = Array(2)</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.takeOrdered(2)</span><br><span class="line">res7: Array[Int] = Array(2, 3)</span><br></pre></td></tr></table></figure></div><h2 id="aggregate"><a href="#aggregate" class="headerlink" title="aggregate"></a>aggregate</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">def aggregate[U](zeroValue: U)(seqOp: (U, T) ⇒ U, combOp: (U, U) ⇒ U)(implicit arg0: ClassTag[U]): U</span><br><span class="line"></span><br><span class="line">aggregate用户聚合RDD中的元素，先使用seqOp将RDD中每个分区中的T类型元素聚合成U类型，再使用combOp将之前每个分区聚合后的U类型聚合成U类型，特别注意seqOp和combOp都会使用zeroValue的值，zeroValue的类型为U。</span><br><span class="line"></span><br><span class="line">var rdd1 = sc.makeRDD(1 to 10,2)</span><br><span class="line">rdd1.mapPartitionsWithIndex&#123;</span><br><span class="line">        (partIdx,iter) =&gt; &#123;</span><br><span class="line">          var part_map = scala.collection.mutable.Map[String,List[Int]]()</span><br><span class="line">            <span class="keyword">while</span>(iter.hasNext)&#123;</span><br><span class="line">              var part_name = <span class="string">"part_"</span> + partIdx;</span><br><span class="line">              var elem = iter.next()</span><br><span class="line">              <span class="keyword">if</span>(part_map.contains(part_name)) &#123;</span><br><span class="line">                var elems = part_map(part_name)</span><br><span class="line">                elems ::= elem</span><br><span class="line">                part_map(part_name) = elems</span><br><span class="line">              &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                part_map(part_name) = List[Int]&#123;elem&#125;</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            part_map.iterator</span><br><span class="line">           </span><br><span class="line">        &#125;</span><br><span class="line">      &#125;.collect</span><br><span class="line">res16: Array[(String, List[Int])] = Array((part_0,List(5, 4, 3, 2, 1)), (part_1,List(10, 9, 8, 7, 6)))</span><br><span class="line"> </span><br><span class="line"><span class="comment">##第一个分区中包含5,4,3,2,1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##第二个分区中包含10,9,8,7,6</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.aggregate(1)(</span><br><span class="line">     |           &#123;(x : Int,y : Int) =&gt; x + y&#125;, </span><br><span class="line">     |           &#123;(a : Int,b : Int) =&gt; a + b&#125;</span><br><span class="line">     |     )</span><br><span class="line">res17: Int = 58</span><br><span class="line"> </span><br><span class="line">结果为什么是58，看下面的计算过程：</span><br><span class="line"></span><br><span class="line"><span class="comment">##先在每个分区中迭代执行 (x : Int,y : Int) =&gt; x + y 并且使用zeroValue的值1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##即：part_0中 zeroValue+5+4+3+2+1 = 1+5+4+3+2+1 = 16</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## part_1中 zeroValue+10+9+8+7+6 = 1+10+9+8+7+6 = 41</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##再将两个分区的结果合并(a : Int,b : Int) =&gt; a + b ，并且使用zeroValue的值1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##即：zeroValue+part_0+part_1 = 1 + 16 + 41 = 58</span></span><br><span class="line"></span><br><span class="line">再比如：</span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.aggregate(2)(</span><br><span class="line">     |           &#123;(x : Int,y : Int) =&gt; x + y&#125;, </span><br><span class="line">     |           &#123;(a : Int,b : Int) =&gt; a * b&#125;</span><br><span class="line">     |     )</span><br><span class="line">res18: Int = 1428</span><br><span class="line"> </span><br><span class="line"><span class="comment">##这次zeroValue=2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##part_0中 zeroValue+5+4+3+2+1 = 2+5+4+3+2+1 = 17</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##part_1中 zeroValue+10+9+8+7+6 = 2+10+9+8+7+6 = 42</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##最后：zeroValue*part_0*part_1 = 2 * 17 * 42 = 1428</span></span><br><span class="line"></span><br><span class="line">因此，zeroValue即确定了U的类型，也会对结果产生至关重要的影响，使用时候要特别注意。</span><br></pre></td></tr></table></figure></div><h2 id="fold"><a href="#fold" class="headerlink" title="fold"></a>fold</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def fold(zeroValue: T)(op: (T, T) ⇒ T): T</span><br><span class="line"></span><br><span class="line">fold是aggregate的简化，将aggregate中的seqOp和combOp使用同一个函数op。</span><br><span class="line">var rdd1 = sc.makeRDD(1 to 10, 2)</span><br><span class="line">scala&gt; rdd1.fold(1)(</span><br><span class="line">     |       (x,y) =&gt; x + y    </span><br><span class="line">     |     )</span><br><span class="line">res19: Int = 58</span><br><span class="line"> </span><br><span class="line"><span class="comment">##结果同上面使用aggregate的第一个例子一样，即：</span></span><br><span class="line">scala&gt; rdd1.aggregate(1)(</span><br><span class="line">     |           &#123;(x,y) =&gt; x + y&#125;, </span><br><span class="line">     |           &#123;(a,b) =&gt; a + b&#125;</span><br><span class="line">     |     )</span><br><span class="line">res20: Int = 58</span><br></pre></td></tr></table></figure></div><h2 id="lookup"><a href="#lookup" class="headerlink" title="lookup"></a>lookup</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def lookup(key: K): Seq[V]</span><br><span class="line"></span><br><span class="line">lookup用于(K,V)类型的RDD,指定K值，返回RDD中该K对应的所有V值。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; var rdd1 = sc.makeRDD(Array((<span class="string">"A"</span>,0),(<span class="string">"A"</span>,2),(<span class="string">"B"</span>,1),(<span class="string">"B"</span>,2),(<span class="string">"C"</span>,1)))</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[0] at makeRDD at :21</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.lookup(<span class="string">"A"</span>)</span><br><span class="line">res0: Seq[Int] = WrappedArray(0, 2)</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.lookup(<span class="string">"B"</span>)</span><br><span class="line">res1: Seq[Int] = WrappedArray(1, 2)</span><br></pre></td></tr></table></figure></div><h2 id="countByKey"><a href="#countByKey" class="headerlink" title="countByKey"></a>countByKey</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def countByKey(): Map[K, Long]</span><br><span class="line"></span><br><span class="line">countByKey用于统计RDD[K,V]中每个K的数量。</span><br><span class="line"></span><br><span class="line">scala&gt; var rdd1 = sc.makeRDD(Array((<span class="string">"A"</span>,0),(<span class="string">"A"</span>,2),(<span class="string">"B"</span>,1),(<span class="string">"B"</span>,2),(<span class="string">"B"</span>,3)))</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[7] at makeRDD at :21</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.countByKey</span><br><span class="line">res5: scala.collection.Map[String,Long] = Map(A -&gt; 2, B -&gt; 3)</span><br></pre></td></tr></table></figure></div><h2 id="foreach"><a href="#foreach" class="headerlink" title="foreach"></a>foreach</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">def foreach(f: (T) ⇒ Unit): Unit</span><br><span class="line"></span><br><span class="line">foreach用于遍历RDD,将函数f应用于每一个元素。</span><br><span class="line">但要注意，如果对RDD执行foreach，只会在Executor端有效，而并不是Driver端。</span><br><span class="line">比如：rdd.foreach(println)，只会在Executor的stdout中打印出来，Driver端是看不到的。</span><br><span class="line">我在Spark1.4中是这样，不知道是否真如此。</span><br><span class="line">这时候，使用accumulator共享变量与foreach结合，倒是个不错的选择。</span><br><span class="line"></span><br><span class="line">scala&gt; var cnt = sc.accumulator(0)</span><br><span class="line">cnt: org.apache.spark.Accumulator[Int] = 0</span><br><span class="line"> </span><br><span class="line">scala&gt; var rdd1 = sc.makeRDD(1 to 10,2)</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[5] at makeRDD at :21</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.foreach(x =&gt; cnt += x)</span><br><span class="line"> </span><br><span class="line">scala&gt; cnt.value</span><br><span class="line">res51: Int = 55</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.collect.foreach(println)</span><br></pre></td></tr></table></figure></div><h2 id="foreachPartition"><a href="#foreachPartition" class="headerlink" title="foreachPartition"></a>foreachPartition</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">def foreachPartition(f: (Iterator[T]) ⇒ Unit): Unit</span><br><span class="line"></span><br><span class="line">foreachPartition和foreach类似，只不过是对每一个分区使用f。</span><br><span class="line"></span><br><span class="line">scala&gt; var rdd1 = sc.makeRDD(1 to 10,2)</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[5] at makeRDD at :21</span><br><span class="line"> </span><br><span class="line">scala&gt; var allsize = sc.accumulator(0)</span><br><span class="line">size: org.apache.spark.Accumulator[Int] = 0</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">scala&gt;     rdd1.foreachPartition &#123; x =&gt; &#123;</span><br><span class="line">     |       allsize += x.size</span><br><span class="line">     |     &#125;&#125;</span><br><span class="line"> </span><br><span class="line">scala&gt; println(allsize.value)</span><br><span class="line">10</span><br><span class="line"></span><br><span class="line"><span class="comment">## sortBy</span></span><br><span class="line">``` bash</span><br><span class="line">def sortBy[K](f: (T) ⇒ K, ascending: Boolean = <span class="literal">true</span>, numPartitions: Int = this.partitions.length)(implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T]</span><br><span class="line"></span><br><span class="line">sortBy根据给定的排序k函数将RDD中的元素进行排序。</span><br><span class="line"></span><br><span class="line">scala&gt; var rdd1 = sc.makeRDD(Seq(3,6,7,1,2,0),2)</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.sortBy(x =&gt; x).collect</span><br><span class="line">res1: Array[Int] = Array(0, 1, 2, 3, 6, 7) //默认升序</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.sortBy(x =&gt; x,<span class="literal">false</span>).collect</span><br><span class="line">res2: Array[Int] = Array(7, 6, 3, 2, 1, 0)  //降序</span><br><span class="line"> </span><br><span class="line">//RDD[K,V]类型</span><br><span class="line">scala&gt;var rdd1 = sc.makeRDD(Array((<span class="string">"A"</span>,2),(<span class="string">"A"</span>,1),(<span class="string">"B"</span>,6),(<span class="string">"B"</span>,3),(<span class="string">"B"</span>,7)))</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.sortBy(x =&gt; x).collect</span><br><span class="line">res3: Array[(String, Int)] = Array((A,1), (A,2), (B,3), (B,6), (B,7))</span><br><span class="line"> </span><br><span class="line">//按照V进行降序排序</span><br><span class="line">scala&gt; rdd1.sortBy(x =&gt; x._2,<span class="literal">false</span>).collect</span><br><span class="line">res4: Array[(String, Int)] = Array((B,7), (B,6), (B,3), (A,2), (A,1))</span><br></pre></td></tr></table></figure></div><h2 id="saveAsTextFile"><a href="#saveAsTextFile" class="headerlink" title="saveAsTextFile"></a>saveAsTextFile</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">def saveAsTextFile(path: String): Unit</span><br><span class="line"></span><br><span class="line">def saveAsTextFile(path: String, codec: Class[_ &lt;: CompressionCodec]): Unit</span><br><span class="line"></span><br><span class="line">saveAsTextFile用于将RDD以文本文件的格式存储到文件系统中。</span><br><span class="line"></span><br><span class="line">codec参数可以指定压缩的类名。</span><br><span class="line"></span><br><span class="line">var rdd1 = sc.makeRDD(1 to 10,2)</span><br><span class="line">scala&gt; rdd1.saveAsTextFile(<span class="string">"hdfs://cdh5/tmp/lxw1234.com/"</span>) //保存到HDFS</span><br><span class="line">hadoop fs -ls /tmp/lxw1234.com</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r--   2 lxw1234 supergroup        0 2015-07-10 09:15 /tmp/lxw1234.com/_SUCCESS</span><br><span class="line">-rw-r--r--   2 lxw1234 supergroup        21 2015-07-10 09:15 /tmp/lxw1234.com/part-00000</span><br><span class="line"> </span><br><span class="line">hadoop fs -cat /tmp/lxw1234.com/part-00000</span><br><span class="line"></span><br><span class="line">注意：如果使用rdd1.saveAsTextFile(“file:///tmp/lxw1234.com”)将文件保存到本地文件系统，那么只会保存在Executor所在机器的本地目录。</span><br><span class="line"></span><br><span class="line">//指定压缩格式保存</span><br><span class="line"></span><br><span class="line">rdd1.saveAsTextFile(<span class="string">"hdfs://cdh5/tmp/lxw1234.com/"</span>,classOf[com.hadoop.compression.lzo.LzopCodec])</span><br><span class="line"> </span><br><span class="line">hadoop fs -ls /tmp/lxw1234.com</span><br><span class="line">-rw-r--r--   2 lxw1234 supergroup    0 2015-07-10 09:20 /tmp/lxw1234.com/_SUCCESS</span><br><span class="line">-rw-r--r--   2 lxw1234 supergroup    71 2015-07-10 09:20 /tmp/lxw1234.com/part-00000.lzo</span><br><span class="line"> </span><br><span class="line">hadoop fs -text /tmp/lxw1234.com/part-00000.lzo</span><br></pre></td></tr></table></figure></div><h2 id="saveAsSequenceFile"><a href="#saveAsSequenceFile" class="headerlink" title="saveAsSequenceFile"></a>saveAsSequenceFile</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">saveAsSequenceFile用于将RDD以SequenceFile的文件格式保存到HDFS上。</span><br><span class="line">用法同saveAsTextFile。</span><br></pre></td></tr></table></figure></div><h2 id="saveAsObjectFile"><a href="#saveAsObjectFile" class="headerlink" title="saveAsObjectFile"></a>saveAsObjectFile</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def saveAsObjectFile(path: String): Unit</span><br><span class="line"></span><br><span class="line">saveAsObjectFile用于将RDD中的元素序列化成对象，存储到文件中。</span><br><span class="line"></span><br><span class="line">对于HDFS，默认采用SequenceFile保存。</span><br><span class="line"></span><br><span class="line">var rdd1 = sc.makeRDD(1 to 10,2)</span><br><span class="line">rdd1.saveAsObjectFile(<span class="string">"hdfs://cdh5/tmp/lxw1234.com/"</span>)</span><br><span class="line"> </span><br><span class="line">hadoop fs -cat /tmp/lxw1234.com/part-00000</span><br><span class="line">SEQ !org.apache.hadoop.io.NullWritable<span class="string">"org.apache.hadoop.io.BytesWritableT</span></span><br></pre></td></tr></table></figure></div><h2 id="saveAsHadoopFile"><a href="#saveAsHadoopFile" class="headerlink" title="saveAsHadoopFile"></a>saveAsHadoopFile</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">def saveAsHadoopFile(path: String, keyClass: Class[_], valueClass: Class[_], outputFormatClass: Class[_ &lt;: OutputFormat[_, _]], codec: Class[_ &lt;: CompressionCodec]): Unit</span><br><span class="line">def saveAsHadoopFile(path: String, keyClass: Class[_], valueClass: Class[_], outputFormatClass: Class[_ &lt;: OutputFormat[_, _]], conf: JobConf = …, codec: Option[Class[_ &lt;: CompressionCodec]] = None): Unit</span><br><span class="line"></span><br><span class="line">saveAsHadoopFile是将RDD存储在HDFS上的文件中，支持老版本Hadoop API。</span><br><span class="line"></span><br><span class="line">可以指定outputKeyClass、outputValueClass以及压缩格式。</span><br><span class="line">每个分区输出一个文件。</span><br><span class="line">var rdd1 = sc.makeRDD(Array((<span class="string">"A"</span>,2),(<span class="string">"A"</span>,1),(<span class="string">"B"</span>,6),(<span class="string">"B"</span>,3),(<span class="string">"B"</span>,7)))</span><br><span class="line">import org.apache.hadoop.mapred.TextOutputFormat</span><br><span class="line">import org.apache.hadoop.io.Text</span><br><span class="line">import org.apache.hadoop.io.IntWritable</span><br><span class="line"> </span><br><span class="line">rdd1.saveAsHadoopFile(<span class="string">"/tmp/lxw1234.com/"</span>,classOf[Text],classOf[IntWritable],classOf[TextOutputFormat[Text,IntWritable]])</span><br><span class="line"> </span><br><span class="line">rdd1.saveAsHadoopFile(<span class="string">"/tmp/lxw1234.com/"</span>,classOf[Text],classOf[IntWritable],classOf[TextOutputFormat[Text,IntWritable]],</span><br><span class="line">                      classOf[com.hadoop.compression.lzo.LzopCodec])</span><br></pre></td></tr></table></figure></div><h2 id="saveAsHadoopDataset"><a href="#saveAsHadoopDataset" class="headerlink" title="saveAsHadoopDataset"></a>saveAsHadoopDataset</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line">def saveAsHadoopDataset(conf: JobConf): Unit</span><br><span class="line">saveAsHadoopDataset用于将RDD保存到除了HDFS的其他存储中，比如HBase。</span><br><span class="line">在JobConf中，通常需要关注或者设置五个参数：</span><br><span class="line">文件的保存路径、key值的class类型、value值的class类型、RDD的输出格式(OutputFormat)、以及压缩相关的参数。</span><br><span class="line"> </span><br><span class="line"><span class="comment">##使用saveAsHadoopDataset将RDD保存到HDFS中</span></span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line">import SparkContext._</span><br><span class="line">import org.apache.hadoop.mapred.TextOutputFormat</span><br><span class="line">import org.apache.hadoop.io.Text</span><br><span class="line">import org.apache.hadoop.io.IntWritable</span><br><span class="line">import org.apache.hadoop.mapred.JobConf</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">var rdd1 = sc.makeRDD(Array((<span class="string">"A"</span>,2),(<span class="string">"A"</span>,1),(<span class="string">"B"</span>,6),(<span class="string">"B"</span>,3),(<span class="string">"B"</span>,7)))</span><br><span class="line">var jobConf = new JobConf()</span><br><span class="line">jobConf.setOutputFormat(classOf[TextOutputFormat[Text,IntWritable]])</span><br><span class="line">jobConf.setOutputKeyClass(classOf[Text])</span><br><span class="line">jobConf.setOutputValueClass(classOf[IntWritable])</span><br><span class="line">jobConf.set(<span class="string">"mapred.output.dir"</span>,<span class="string">"/tmp/lxw1234/"</span>)</span><br><span class="line">rdd1.saveAsHadoopDataset(jobConf)</span><br><span class="line"> </span><br><span class="line">结果：</span><br><span class="line">hadoop fs -cat /tmp/lxw1234/part-00000</span><br><span class="line">A       2</span><br><span class="line">A       1</span><br><span class="line">hadoop fs -cat /tmp/lxw1234/part-00001</span><br><span class="line">B       6</span><br><span class="line">B       3</span><br><span class="line">B       7</span><br><span class="line"> </span><br><span class="line"><span class="comment">##保存数据到HBASE</span></span><br><span class="line"></span><br><span class="line">HBase建表：</span><br><span class="line"></span><br><span class="line">create ‘lxw1234′,&#123;NAME =&gt; ‘f1′,VERSIONS =&gt; 1&#125;,&#123;NAME =&gt; ‘f2′,VERSIONS =&gt; 1&#125;,&#123;NAME =&gt; ‘f3′,VERSIONS =&gt; 1&#125;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line">import SparkContext._</span><br><span class="line">import org.apache.hadoop.mapred.TextOutputFormat</span><br><span class="line">import org.apache.hadoop.io.Text</span><br><span class="line">import org.apache.hadoop.io.IntWritable</span><br><span class="line">import org.apache.hadoop.mapred.JobConf</span><br><span class="line">import org.apache.hadoop.hbase.HBaseConfiguration</span><br><span class="line">import org.apache.hadoop.hbase.mapred.TableOutputFormat</span><br><span class="line">import org.apache.hadoop.hbase.client.Put</span><br><span class="line">import org.apache.hadoop.hbase.util.Bytes</span><br><span class="line">import org.apache.hadoop.hbase.io.ImmutableBytesWritable</span><br><span class="line"> </span><br><span class="line">var conf = HBaseConfiguration.create()</span><br><span class="line">    var jobConf = new JobConf(conf)</span><br><span class="line">    jobConf.set(<span class="string">"hbase.zookeeper.quorum"</span>,<span class="string">"zkNode1,zkNode2,zkNode3"</span>)</span><br><span class="line">    jobConf.set(<span class="string">"zookeeper.znode.parent"</span>,<span class="string">"/hbase"</span>)</span><br><span class="line">    jobConf.set(TableOutputFormat.OUTPUT_TABLE,<span class="string">"lxw1234"</span>)</span><br><span class="line">    jobConf.setOutputFormat(classOf[TableOutputFormat])</span><br><span class="line">    </span><br><span class="line">    var rdd1 = sc.makeRDD(Array((<span class="string">"A"</span>,2),(<span class="string">"B"</span>,6),(<span class="string">"C"</span>,7)))</span><br><span class="line">    rdd1.map(x =&gt; </span><br><span class="line">      &#123;</span><br><span class="line">        var put = new Put(Bytes.toBytes(x._1))</span><br><span class="line">        put.add(Bytes.toBytes(<span class="string">"f1"</span>), Bytes.toBytes(<span class="string">"c1"</span>), Bytes.toBytes(x._2))</span><br><span class="line">        (new ImmutableBytesWritable,put)</span><br><span class="line">      &#125;</span><br><span class="line">    ).saveAsHadoopDataset(jobConf)</span><br><span class="line"> </span><br><span class="line"><span class="comment">##结果：</span></span><br><span class="line">hbase(main):005:0&gt; scan <span class="string">'lxw1234'</span></span><br><span class="line">ROW     COLUMN+CELL                                                                                                </span><br><span class="line"> A       column=f1:c1, timestamp=1436504941187, value=\x00\x00\x00\x02                                              </span><br><span class="line"> B       column=f1:c1, timestamp=1436504941187, value=\x00\x00\x00\x06                                              </span><br><span class="line"> C       column=f1:c1, timestamp=1436504941187, value=\x00\x00\x00\x07                                              </span><br><span class="line">3 row(s) <span class="keyword">in</span> 0.0550 seconds</span><br><span class="line"> </span><br><span class="line">注意：保存到HBase，运行时候需要在SPARK_CLASSPATH中加入HBase相关的jar包。</span><br></pre></td></tr></table></figure></div><h2 id="saveAsNewAPIHadoopFile"><a href="#saveAsNewAPIHadoopFile" class="headerlink" title="saveAsNewAPIHadoopFile"></a>saveAsNewAPIHadoopFile</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def saveAsNewAPIHadoopFile[F &lt;: OutputFormat[K, V]](path: String)(implicit fm: ClassTag[F]): Unit</span><br><span class="line"></span><br><span class="line">def saveAsNewAPIHadoopFile(path: String, keyClass: Class[_], valueClass: Class[_], outputFormatClass: Class[_ &lt;: OutputFormat[_, _]], conf: Configuration = self.context.hadoopConfiguration): Unit</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">saveAsNewAPIHadoopFile用于将RDD数据保存到HDFS上，使用新版本Hadoop API。</span><br><span class="line"></span><br><span class="line">用法基本同saveAsHadoopFile。</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line">import SparkContext._</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat</span><br><span class="line">import org.apache.hadoop.io.Text</span><br><span class="line">import org.apache.hadoop.io.IntWritable</span><br><span class="line"> </span><br><span class="line">var rdd1 = sc.makeRDD(Array((<span class="string">"A"</span>,2),(<span class="string">"A"</span>,1),(<span class="string">"B"</span>,6),(<span class="string">"B"</span>,3),(<span class="string">"B"</span>,7)))</span><br><span class="line">rdd1.saveAsNewAPIHadoopFile(<span class="string">"/tmp/lxw1234/"</span>,classOf[Text],classOf[IntWritable],classOf[TextOutputFormat[Text,IntWritable]])</span><br></pre></td></tr></table></figure></div><h2 id="saveAsNewAPIHadoopDataset"><a href="#saveAsNewAPIHadoopDataset" class="headerlink" title="saveAsNewAPIHadoopDataset"></a>saveAsNewAPIHadoopDataset</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">def saveAsNewAPIHadoopDataset(conf: Configuration): Unit</span><br><span class="line"></span><br><span class="line">作用同saveAsHadoopDataset,只不过采用新版本Hadoop API。</span><br><span class="line"></span><br><span class="line">以写入HBase为例：</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">HBase建表：</span><br><span class="line"></span><br><span class="line">create ‘lxw1234′,&#123;NAME =&gt; ‘f1′,VERSIONS =&gt; 1&#125;,&#123;NAME =&gt; ‘f2′,VERSIONS =&gt; 1&#125;,&#123;NAME =&gt; ‘f3′,VERSIONS =&gt; 1&#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">完整的Spark应用程序：</span><br><span class="line"></span><br><span class="line">package com.lxw1234.test</span><br><span class="line"> </span><br><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line">import SparkContext._</span><br><span class="line">import org.apache.hadoop.hbase.HBaseConfiguration</span><br><span class="line">import org.apache.hadoop.mapreduce.Job</span><br><span class="line">import org.apache.hadoop.hbase.mapreduce.TableOutputFormat</span><br><span class="line">import org.apache.hadoop.hbase.io.ImmutableBytesWritable</span><br><span class="line">import org.apache.hadoop.hbase.client.Result</span><br><span class="line">import org.apache.hadoop.hbase.util.Bytes</span><br><span class="line">import org.apache.hadoop.hbase.client.Put</span><br><span class="line"> </span><br><span class="line">object Test &#123;</span><br><span class="line">  def main(args : Array[String]) &#123;</span><br><span class="line">   val sparkConf = new SparkConf().setMaster(<span class="string">"spark://lxw1234.com:7077"</span>).setAppName(<span class="string">"lxw1234.com"</span>)</span><br><span class="line">   val sc = new SparkContext(sparkConf);</span><br><span class="line">   var rdd1 = sc.makeRDD(Array((<span class="string">"A"</span>,2),(<span class="string">"B"</span>,6),(<span class="string">"C"</span>,7)))</span><br><span class="line">   </span><br><span class="line">    sc.hadoopConfiguration.set(<span class="string">"hbase.zookeeper.quorum "</span>,<span class="string">"zkNode1,zkNode2,zkNode3"</span>)</span><br><span class="line">    sc.hadoopConfiguration.set(<span class="string">"zookeeper.znode.parent"</span>,<span class="string">"/hbase"</span>)</span><br><span class="line">    sc.hadoopConfiguration.set(TableOutputFormat.OUTPUT_TABLE,<span class="string">"lxw1234"</span>)</span><br><span class="line">    var job = new Job(sc.hadoopConfiguration)</span><br><span class="line">    job.setOutputKeyClass(classOf[ImmutableBytesWritable])</span><br><span class="line">    job.setOutputValueClass(classOf[Result])</span><br><span class="line">    job.setOutputFormatClass(classOf[TableOutputFormat[ImmutableBytesWritable]])</span><br><span class="line">    </span><br><span class="line">    rdd1.map(</span><br><span class="line">      x =&gt; &#123;</span><br><span class="line">        var put = new Put(Bytes.toBytes(x._1))</span><br><span class="line">        put.add(Bytes.toBytes(<span class="string">"f1"</span>), Bytes.toBytes(<span class="string">"c1"</span>), Bytes.toBytes(x._2))</span><br><span class="line">        (new ImmutableBytesWritable,put)</span><br><span class="line">      &#125;    </span><br><span class="line">    ).saveAsNewAPIHadoopDataset(job.getConfiguration)</span><br><span class="line">    </span><br><span class="line">    sc.stop()   </span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">注意：保存到HBase，运行时候需要在SPARK_CLASSPATH中加入HBase相关的jar包。</span><br></pre></td></tr></table></figure></div><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>sqoop一些参数</title>
      <link href="/bin1002/2019/11/19/sqoop_canshu/"/>
      <url>/bin1002/2019/11/19/sqoop_canshu/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon Jun 08 2020 17:37:04 GMT+0800 (GMT+08:00) --><h1 id="sqoop的数据导入"><a href="#sqoop的数据导入" class="headerlink" title="sqoop的数据导入"></a>sqoop的数据导入</h1><h2 id="导入数据库表到HDFS"><a href="#导入数据库表到HDFS" class="headerlink" title="导入数据库表到HDFS"></a>导入数据库表到HDFS</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">下面的命令用于从MySQL数据库服务器中的emp表导入HDFS</span><br><span class="line">bin/sqoop import </span><br><span class="line">--connect jdbc:mysql://192.168.1.101:3306/userdb </span><br><span class="line">--password admin </span><br><span class="line">--username root </span><br><span class="line">--table emp  </span><br><span class="line">--m 1</span><br><span class="line"></span><br><span class="line">检查是否导入成功</span><br><span class="line">hdfs dfs -ls /user/root/emp</span><br></pre></td></tr></table></figure></div><h2 id="导入到HDFS指定目录"><a href="#导入到HDFS指定目录" class="headerlink" title="导入到HDFS指定目录"></a>导入到HDFS指定目录</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"> 在导入表数据到HDFS使用Sqoop导入工具，我们可以指定目标目录。</span><br><span class="line">代码如下：</span><br><span class="line">bin/sqoop import  </span><br><span class="line">--connect jdbc:mysql://172.16.43.67:3306/userdb </span><br><span class="line">--username root </span><br><span class="line">--password admin </span><br><span class="line">--delete-target-dir <span class="comment">#判断导出目录是否存在，如果存在就删掉</span></span><br><span class="line">--table emp  </span><br><span class="line">--target-dir /sqoop/emp <span class="comment">#指定HDFS导出目的地</span></span><br><span class="line">--m 1</span><br><span class="line"></span><br><span class="line">最后检查是否导入成功</span><br><span class="line"> hdfs dfs -text /sqoop/emp/part-m-00000</span><br></pre></td></tr></table></figure></div><h2 id="导入到hdfs指定目录并指定字段之间的分隔符"><a href="#导入到hdfs指定目录并指定字段之间的分隔符" class="headerlink" title="导入到hdfs指定目录并指定字段之间的分隔符"></a>导入到hdfs指定目录并指定字段之间的分隔符</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">bin/sqoop import  </span><br><span class="line">--connect jdbc:mysql://172.16.43.67:3306/userdb </span><br><span class="line">--username root --password admin </span><br><span class="line">--delete-target-dir </span><br><span class="line">--table emp  </span><br><span class="line">--target-dir /sqoop/emp2 </span><br><span class="line">--m 1  <span class="comment"># mr的数量</span></span><br><span class="line">** --fields-terminated-by <span class="string">'\t'</span> ** </span><br><span class="line"></span><br><span class="line">查看文件内容</span><br><span class="line">hdfs dfs -text /sqoop/emp2/part-m-00000</span><br></pre></td></tr></table></figure></div><h2 id="导入关系表到hive"><a href="#导入关系表到hive" class="headerlink" title="导入关系表到hive"></a>导入关系表到hive</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">1.在hive建表</span><br><span class="line"> create database sqooptohive;</span><br><span class="line"> use sqooptohive;</span><br><span class="line">  create external table emp_hive(</span><br><span class="line">  id int</span><br><span class="line">  ,name string</span><br><span class="line">  ,deg string</span><br><span class="line">  ,salary int </span><br><span class="line">  ,dept string) </span><br><span class="line">  row format delimited fields terminated by <span class="string">'\001'</span>;</span><br><span class="line"></span><br><span class="line">2.开始导入</span><br><span class="line">bin/sqoop import </span><br><span class="line">--connect jdbc:mysql://172.16.43.67:3306/userdb </span><br><span class="line">--username root </span><br><span class="line">--password admin </span><br><span class="line">--table emp <span class="comment"># 需要导出的数据库表</span></span><br><span class="line">--fields-terminated-by <span class="string">'\001'</span> </span><br><span class="line">--hive-import </span><br><span class="line">--hive-table sqooptohive.emp_hive <span class="comment">#导入hive的库</span></span><br><span class="line">--hive-overwrite <span class="comment"># 如果有这个表就进行覆盖</span></span><br><span class="line">--delete-target-dir  <span class="comment"># 如果目标目录已存在，则先删除</span></span><br><span class="line">--m 1</span><br></pre></td></tr></table></figure></div><h2 id="导入关系表到hive并自动创建hive表"><a href="#导入关系表到hive并自动创建hive表" class="headerlink" title="导入关系表到hive并自动创建hive表"></a>导入关系表到hive并自动创建hive表</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">我们也可以通过命令来将我们的mysql的表直接导入到hive表当中去</span><br><span class="line">bin/sqoop import </span><br><span class="line">--connect jdbc:mysql://172.16.43.67:3306/userdb </span><br><span class="line">--username root </span><br><span class="line">--password admin </span><br><span class="line">--table emp_conn </span><br><span class="line">--hive-import </span><br><span class="line">-m 1 </span><br><span class="line">--hive-database sqooptohive; <span class="comment">#可以写成库.表名的形式</span></span><br><span class="line"></span><br><span class="line">通过这个命令，我们可以直接将我们mysql表当中的数据以及表结构一起倒入到hive当中去</span><br></pre></td></tr></table></figure></div><h2 id="sql语句查找导入hdfs"><a href="#sql语句查找导入hdfs" class="headerlink" title="sql语句查找导入hdfs"></a>sql语句查找导入hdfs</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://172.16.43.67:3306/userdb </span><br><span class="line">--username root </span><br><span class="line">--password admin \</span><br><span class="line">--delete-target-dir -m 1 \</span><br><span class="line">--query <span class="string">'select phno from emp_conn where 1=1 and  $CONDITIONS'</span> \</span><br><span class="line">--target-dir /sqoop/emp_conn</span><br></pre></td></tr></table></figure></div><h1 id="sqoop的数据导出"><a href="#sqoop的数据导出" class="headerlink" title="sqoop的数据导出"></a>sqoop的数据导出</h1><h2 id="hdfs导出到mysql"><a href="#hdfs导出到mysql" class="headerlink" title="hdfs导出到mysql"></a>hdfs导出到mysql</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">数据是在HDFS当中的如下目录/sqoop/emp，数据内容如下</span><br><span class="line">1201,gopal,manager,50000,TP,2018-06-17 18:54:32.0,2018-06-17 18:54:32.0,1</span><br><span class="line">1202,manisha,Proof reader,50000,TP,2018-06-15 18:54:32.0,2018-06-17 20:26:08.0,1</span><br><span class="line">1203,khalil,php dev,30000,AC,2018-06-17 18:54:32.0,2018-06-17 18:54:32.0,1</span><br><span class="line">1204,prasanth,php dev,30000,AC,2018-06-17 18:54:32.0,2018-06-17 21:05:52.0,0</span><br><span class="line">1205,kranthi,admin,20000,TP,2018-06-17 18:54:32.0,2018-06-17 18:54:32.0,1</span><br><span class="line"></span><br><span class="line">第一步：创建mysql表</span><br><span class="line">CREATE TABLE `emp_out` (</span><br><span class="line">  `id` INT(11) DEFAULT NULL,</span><br><span class="line">  `name` VARCHAR(100) DEFAULT NULL,</span><br><span class="line">  `deg` VARCHAR(100) DEFAULT NULL,</span><br><span class="line">  `salary` INT(11) DEFAULT NULL,</span><br><span class="line">  `dept` VARCHAR(10) DEFAULT NULL,</span><br><span class="line">  `create_time` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,</span><br><span class="line">  `update_time` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,</span><br><span class="line">  `is_delete` BIGINT(20) DEFAULT <span class="string">'1'</span></span><br><span class="line">) ENGINE=INNODB DEFAULT CHARSET=utf8;</span><br><span class="line"></span><br><span class="line">第二步：执行导出命令</span><br><span class="line">通过<span class="built_in">export</span>来实现数据的导出，将hdfs的数据导出到mysql当中去</span><br><span class="line">bin/sqoop <span class="built_in">export</span> \</span><br><span class="line">--connect jdbc:mysql://172.16.43.67:3306/userdb \</span><br><span class="line">--username root </span><br><span class="line">--password admin \</span><br><span class="line">--table emp_out \</span><br><span class="line">--<span class="built_in">export</span>-dir /sqoop/emp \ <span class="comment">## hdfs的目录</span></span><br><span class="line">--input-fields-terminated-by <span class="string">","</span></span><br><span class="line"></span><br><span class="line">第三步验证</span><br><span class="line">mysql查询</span><br><span class="line">select * from emp_out <span class="built_in">limit</span> 10;</span><br></pre></td></tr></table></figure></div><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ETL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hive常用函数</title>
      <link href="/bin1002/2019/11/19/hive_hanshu/"/>
      <url>/bin1002/2019/11/19/hive_hanshu/</url>
      
        <content type="html"><![CDATA[<!-- build time:Mon Jun 08 2020 17:37:04 GMT+0800 (GMT+08:00) --><p>记录一些常见的hive函数</p><h1 id="常用函数"><a href="#常用函数" class="headerlink" title="常用函数"></a>常用函数</h1><h2 id="1-关系运算"><a href="#1-关系运算" class="headerlink" title="1.关系运算"></a>1.关系运算</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line">Hive支持的关系运算符</span><br><span class="line">•常见的关系运算符</span><br><span class="line">•等值比较: =</span><br><span class="line">•不等值比较: &lt;&gt;</span><br><span class="line">•小于比较: &lt;</span><br><span class="line">•小于等于比较: &lt;=</span><br><span class="line">•大于比较: &gt;</span><br><span class="line">•大于等于比较: &gt;=</span><br><span class="line">•空值判断: IS NULL</span><br><span class="line">•非空判断: IS NOT NULL</span><br><span class="line">•LIKE比较: LIKE</span><br><span class="line">•JAVA的LIKE操作: RLIKE</span><br><span class="line">•REGEXP操作: REGEXP</span><br><span class="line">•等值比较: =</span><br><span class="line">   语法：A=B</span><br><span class="line">操作类型：所有基本类型</span><br><span class="line">描述: 如果表达式A与表达式B相等，则为TRUE；否则为FALSE</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select 1 from dual <span class="built_in">where</span> 1=1;</span><br><span class="line"></span><br><span class="line">•不等值比较: &lt;&gt;</span><br><span class="line">语法: A &lt;&gt; B</span><br><span class="line">操作类型: 所有基本类型</span><br><span class="line">描述: 如果表达式A为NULL，或者表达式B为NULL，返回NULL；如果表达式A与表达式B不相等，则为TRUE；否则为FALSE</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select 1 from dual <span class="built_in">where</span> 1 &lt;&gt; 2;</span><br><span class="line"></span><br><span class="line">•小于比较: &lt;</span><br><span class="line">语法: A &lt; B</span><br><span class="line">操作类型: 所有基本类型</span><br><span class="line">描述: 如果表达式A为NULL，或者表达式B为NULL，返回NULL；如果表达式A小于表达式B，则为TRUE；否则为FALSE</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select 1 from dual <span class="built_in">where</span> 1 &lt; 2;</span><br><span class="line"></span><br><span class="line">•小于等于比较: &lt;=</span><br><span class="line">语法: A &lt;= B</span><br><span class="line">操作类型: 所有基本类型</span><br><span class="line">描述: 如果表达式A为NULL，或者表达式B为NULL，返回NULL；如果表达式A小于或者等于表达式B，则为TRUE；否则为FALSE</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select 1 from dual <span class="built_in">where</span> 1 &lt;= 1;</span><br><span class="line"></span><br><span class="line">•大于等于比较: &gt;=</span><br><span class="line">语法: A &gt;= B</span><br><span class="line">操作类型: 所有基本类型</span><br><span class="line">描述: 如果表达式A为NULL，或者表达式B为NULL，返回NULL；如果表达式A大于或者等于表达式B，则为TRUE；否则为FALSE</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select 1 from dual <span class="built_in">where</span> 1 &gt;= 1;</span><br><span class="line">•空值判断: IS NULL</span><br><span class="line">语法: A IS NULL</span><br><span class="line">操作类型: 所有类型</span><br><span class="line">描述: 如果表达式A的值为NULL，则为TRUE；否则为FALSE</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select 1 from dual <span class="built_in">where</span> null is null;</span><br><span class="line"> </span><br><span class="line">•非空判断: IS NOT NULL</span><br><span class="line">语法: A IS NOT NULL</span><br><span class="line">操作类型: 所有类型</span><br><span class="line">描述: 如果表达式A的值为NULL，则为FALSE；否则为TRUE</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select 1 from dual <span class="built_in">where</span> 1 is not null;</span><br><span class="line"></span><br><span class="line">•LIKE比较: LIKE</span><br><span class="line">语法: A LIKE B</span><br><span class="line">操作类型: strings</span><br><span class="line">描述: 如果字符串A或者字符串B为NULL，则返回NULL；如果字符串A符合表达式B   的正则语法，则为TRUE；否则为FALSE。B中字符”_”表示任意单个字符，而字符”%”表示任意数量的字符。</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select 1 from dual <span class="built_in">where</span> ‘key<span class="string">' like '</span>foot%<span class="string">';</span></span><br><span class="line"><span class="string">1</span></span><br><span class="line"><span class="string">hive&gt; select 1 from dual where ‘key '</span> like <span class="string">'foot____'</span>;</span><br><span class="line">1</span><br><span class="line">注意：否定比较时候用 NOT A LIKE B</span><br><span class="line">hive&gt; select 1 from dual <span class="built_in">where</span> NOT ‘key <span class="string">' like '</span>fff%<span class="string">';</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">•JAVA的LIKE操作: RLIKE</span></span><br><span class="line"><span class="string">语法: A RLIKE B</span></span><br><span class="line"><span class="string">操作类型: strings</span></span><br><span class="line"><span class="string">描述: 如果字符串A或者字符串B为NULL，则返回NULL；如果字符串A符合JAVA正则表达式B的正则语法，则为TRUE；否则为FALSE。</span></span><br><span class="line"><span class="string">举例：</span></span><br><span class="line"><span class="string">hive&gt; select 1 from dual where '</span>footbar’ rlike <span class="string">'^f.*r$’;</span></span><br><span class="line"><span class="string">1</span></span><br><span class="line"><span class="string">注意：判断一个字符串是否全为数字：</span></span><br><span class="line"><span class="string">hive&gt;select 1 from dual where '</span>123456<span class="string">' rlike '</span>^\\d+$<span class="string">';</span></span><br><span class="line"><span class="string">1</span></span><br><span class="line"><span class="string">hive&gt; select 1 from dual where '</span>123456aa<span class="string">' rlike '</span>^\\d+$<span class="string">';</span></span><br><span class="line"><span class="string">1</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">•REGEXP操作: REGEXP</span></span><br><span class="line"><span class="string">语法: A REGEXP B</span></span><br><span class="line"><span class="string">操作类型: strings</span></span><br><span class="line"><span class="string">描述: 功能与RLIKE相同</span></span><br><span class="line"><span class="string">举例：</span></span><br><span class="line"><span class="string">hive&gt; select 1 from dual where ‘key'</span> REGEXP <span class="string">'^f.*r$'</span>;</span><br><span class="line">1<span class="string">"</span></span><br></pre></td></tr></table></figure></div><h2 id="2-逻辑运算与数学运算"><a href="#2-逻辑运算与数学运算" class="headerlink" title="2.逻辑运算与数学运算"></a>2.逻辑运算与数学运算</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br></pre></td><td class="code"><pre><span class="line">•加法操作: +</span><br><span class="line">•减法操作: -</span><br><span class="line">•乘法操作: *</span><br><span class="line">•除法操作: /</span><br><span class="line">•取余操作: %</span><br><span class="line">•位与操作: &amp;</span><br><span class="line">•位或操作: |</span><br><span class="line">•位异或操作: ^</span><br><span class="line">•位取反操作: ~</span><br><span class="line">•加法操作: +</span><br><span class="line">语法: A + B</span><br><span class="line">操作类型：所有数值类型</span><br><span class="line">说明：返回A与B相加的结果。结果的数值类型等于A的类型和B的类型的最小父类型（详见数据类型的继承关系）。比如，int + int 一般结果为int类型，而int + double 一般结果为double类型</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select 1 + 9 from dual;</span><br><span class="line">10</span><br><span class="line">•减法操作: -</span><br><span class="line">语法: A – B</span><br><span class="line">操作类型：所有数值类型</span><br><span class="line">说明：返回A与B相减的结果。结果的数值类型等于A的类型和B的类型的最小父类型（详见数据类型的继承关系）。比如，int – int 一般结果为int类型，而int – double 一般结果为double类型</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select 10 – 5 from dual;</span><br><span class="line">5</span><br><span class="line"> </span><br><span class="line">• 乘法操作 : *</span><br><span class="line">语法: A * B</span><br><span class="line">操作类型：所有数值类型</span><br><span class="line">说明：返回A与B相乘的结果。结果的数值类型等于A的类型和B的类型的最小父类型（详见数据类型的继承关系）。注意，如果A乘以B的结果超过默认结果类型的数值范围，则需要通过cast将结果转换成范围更大的数值类型</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select 40 * 5 from dual;</span><br><span class="line">200</span><br><span class="line">• 除法操作 : /</span><br><span class="line">语法: A / B</span><br><span class="line">操作类型：所有数值类型</span><br><span class="line">说明：返回A除以B的结果。结果的数值类型为double</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select 40 / 5 from dual;</span><br><span class="line">8.0</span><br><span class="line"> </span><br><span class="line">注意： hive 中最高精度的数据类型是 double, 只精确到小数点后 16 位，在做除法运算的时候要 特别注意</span><br><span class="line">hive&gt;select ceil(28.0/6.999999999999999999999) from dual <span class="built_in">limit</span> 1;   </span><br><span class="line">结果为4</span><br><span class="line">hive&gt;select ceil(28.0/6.99999999999999) from dual <span class="built_in">limit</span> 1;          </span><br><span class="line">结果为5</span><br><span class="line"> </span><br><span class="line">• 取余操作 : %</span><br><span class="line">语法: A % B</span><br><span class="line">操作类型：所有数值类型</span><br><span class="line">说明：返回A除以B的余数。结果的数值类型等于A的类型和B的类型的最小父类型（详见数据类型的继承关系）。</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select 41 % 5 from dual;</span><br><span class="line">1</span><br><span class="line">hive&gt; select 8.4 % 4 from dual;</span><br><span class="line">0.40000000000000036</span><br><span class="line">注意：精度在 hive 中是个很大的问题，类似这样的操作最好通过 round 指定精度</span><br><span class="line">hive&gt; select round(8.4 % 4 , 2) from dual;</span><br><span class="line">0.4</span><br><span class="line"> </span><br><span class="line">• 位与操作 : &amp;</span><br><span class="line">语法: A &amp; B</span><br><span class="line">操作类型：所有数值类型</span><br><span class="line">说明：返回A和B按位进行与操作的结果。结果的数值类型等于A的类型和B的类型的最小父类型（详见数据类型的继承关系）。</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select 4 &amp; 8 from dual;</span><br><span class="line">0</span><br><span class="line">hive&gt; select 6 &amp; 4 from dual;</span><br><span class="line">4</span><br><span class="line"></span><br><span class="line">• 位或操作 : |</span><br><span class="line">语法: A | B</span><br><span class="line">操作类型：所有数值类型</span><br><span class="line">说明：返回A和B按位进行或操作的结果。结果的数值类型等于A的类型和B的类型的最小父类型（详见数据类型的继承关系）。</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select 4 | 8 from dual;</span><br><span class="line">12</span><br><span class="line">hive&gt; select 6 | 8 from dual;</span><br><span class="line">14</span><br><span class="line"></span><br><span class="line">• 位异或操作 : ^</span><br><span class="line">语法: A ^ B</span><br><span class="line">操作类型：所有数值类型</span><br><span class="line">说明：返回A和B按位进行异或操作的结果。结果的数值类型等于A的类型和B的类型的最小父类型（详见数据类型的继承关系）。</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select 4 ^ 8 from dual;</span><br><span class="line">12</span><br><span class="line">hive&gt; select 6 ^ 4 from dual;</span><br><span class="line">2</span><br><span class="line"></span><br><span class="line">• 位取反操作 : ~</span><br><span class="line">语法: ~A</span><br><span class="line">操作类型：所有数值类型</span><br><span class="line">说明：返回A按位取反操作的结果。结果的数值类型等于A的类型。</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select ~6 from dual;</span><br><span class="line">-7</span><br><span class="line">hive&gt; select ~4 from dual;</span><br><span class="line">-5</span><br><span class="line"></span><br><span class="line">Hive逻辑运算</span><br><span class="line">•逻辑与操作: AND</span><br><span class="line">•逻辑或操作: OR</span><br><span class="line">•逻辑非操作: NOT</span><br><span class="line">• 逻辑与操作 : AND</span><br><span class="line">语法: A AND B</span><br><span class="line">操作类型：boolean</span><br><span class="line">说明：如果A和B均为TRUE，则为TRUE；否则为FALSE。如果A为NULL或B为NULL，则为NULL</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select 1 from dual <span class="built_in">where</span> 1=1 and 2=2;</span><br><span class="line">1</span><br><span class="line"></span><br><span class="line">• 逻辑或操作 : OR</span><br><span class="line">语法: A OR B</span><br><span class="line">操作类型：boolean</span><br><span class="line">说明：如果A为TRUE，或者B为TRUE，或者A和B均为TRUE，则为TRUE；否则为FALSE</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select 1 from dual <span class="built_in">where</span> 1=2 or 2=2;</span><br><span class="line">1</span><br><span class="line"></span><br><span class="line">• 逻辑非操作 : NOT</span><br><span class="line">语法: NOT A</span><br><span class="line">操作类型：boolean</span><br><span class="line">说明：如果A为FALSE，或者A为NULL，则为TRUE；否则为FALSE</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select 1 from dual <span class="built_in">where</span> not 1=2;</span><br><span class="line"></span><br><span class="line">• 逻辑非操作 : NOT</span><br><span class="line">语法: NOT A</span><br><span class="line">操作类型：boolean</span><br><span class="line">说明：如果A为FALSE，或者A为NULL，则为TRUE；否则为FALSE</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select 1 from dual <span class="built_in">where</span>  not 1=2 ;</span><br></pre></td></tr></table></figure></div><h2 id="3-数值运算"><a href="#3-数值运算" class="headerlink" title="3.数值运算"></a>3.数值运算</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br></pre></td><td class="code"><pre><span class="line">•取整函数: round</span><br><span class="line">•指定精度取整函数: round</span><br><span class="line">•向下取整函数: floor</span><br><span class="line">•向上取整函数: ceil</span><br><span class="line">•向上取整函数: ceiling</span><br><span class="line">•取随机数函数: rand</span><br><span class="line">•自然指数函数: exp</span><br><span class="line">•以10为底对数函数: log10</span><br><span class="line">•以2为底对数函数: log2</span><br><span class="line">• 对数函数: <span class="built_in">log</span></span><br><span class="line">•幂运算函数: pow</span><br><span class="line">•幂运算函数: power</span><br><span class="line">•开平方函数: sqrt</span><br><span class="line">•二进制函数: bin</span><br><span class="line">•十六进制函数: hex</span><br><span class="line">•反转十六进制函数: unhex</span><br><span class="line">•进制转换函数: conv</span><br><span class="line">•绝对值函数: abs</span><br><span class="line">•正取余函数: pmod</span><br><span class="line">•正弦函数: sin</span><br><span class="line">•反正弦函数: asin</span><br><span class="line">•余弦函数: cos</span><br><span class="line">•反余弦函数: acos</span><br><span class="line">•positive函数: positive</span><br><span class="line">•negative函数: negative</span><br><span class="line"></span><br><span class="line">• 取整函数 : round</span><br><span class="line">语法: round(double a)</span><br><span class="line">返回值: BIGINT</span><br><span class="line">说明: 返回double类型的整数值部分 （遵循四舍五入）</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select round(3.1415926) from dual;</span><br><span class="line">3</span><br><span class="line">hive&gt; select round(3.5) from dual;</span><br><span class="line">4</span><br><span class="line">hive&gt; create table dual as select round(9542.158) from dual;</span><br><span class="line">hive&gt; describe dual;</span><br><span class="line">_c0     bigint</span><br><span class="line"> </span><br><span class="line">• 指定精度取整函数 : round</span><br><span class="line">语法: round(double a, int d)</span><br><span class="line">返回值: DOUBLE</span><br><span class="line">说明: 返回指定精度d的double类型</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select round(3.1415926,4) from dual;</span><br><span class="line">3.1416</span><br><span class="line"> </span><br><span class="line">• 向下取整函数 : floor</span><br><span class="line">语法: floor(double a)</span><br><span class="line">返回值: BIGINT</span><br><span class="line">说明: 返回等于或者小于该double变量的最大的整数</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select floor(3.1415926) from dual;</span><br><span class="line">3</span><br><span class="line">hive&gt; select floor(25) from dual;</span><br><span class="line">25</span><br><span class="line"> </span><br><span class="line">• 向上取整函数 : ceil</span><br><span class="line">语法: ceil(double a)</span><br><span class="line">返回值: BIGINT</span><br><span class="line">说明: 返回等于或者大于该double变量的最小的整数</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select ceil(3.1415926) from dual;</span><br><span class="line">4</span><br><span class="line">hive&gt; select ceil(46) from dual;</span><br><span class="line">46</span><br><span class="line"></span><br><span class="line">• 向上取整函数 : ceiling</span><br><span class="line">语法: ceiling(double a)</span><br><span class="line">返回值: BIGINT</span><br><span class="line">说明: 与ceil功能相同</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select ceiling(3.1415926) from dual;</span><br><span class="line">4</span><br><span class="line">hive&gt; select ceiling(46) from dual;</span><br><span class="line">46</span><br><span class="line"> </span><br><span class="line">• 取随机数函数 : rand</span><br><span class="line">语法: rand(),rand(int seed)</span><br><span class="line">返回值: double</span><br><span class="line">说明: 返回一个0到1范围内的随机数。如果指定种子seed，则会等到一个稳定的随机数序列</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select rand() from dual;</span><br><span class="line">0.5577432776034763</span><br><span class="line"> </span><br><span class="line">• 自然指数函数 : exp</span><br><span class="line">语法: exp(double a)</span><br><span class="line">返回值: double</span><br><span class="line">说明: 返回自然对数e的a次方</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select exp(2) from dual;</span><br><span class="line">7.38905609893065</span><br><span class="line">自然对数函数: ln</span><br><span class="line">语法: ln(double a)</span><br><span class="line">返回值: double</span><br><span class="line">说明: 返回a的自然对数</span><br><span class="line"></span><br><span class="line">• 以 10 为底对数函数 : log10</span><br><span class="line">语法: log10(double a)</span><br><span class="line">返回值: double</span><br><span class="line">说明: 返回以10为底的a的对数</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select log10(100) from dual;</span><br><span class="line">2.0</span><br><span class="line"></span><br><span class="line">• 以 2 为底对数函数 : log2</span><br><span class="line">语法: log2(double a)</span><br><span class="line">返回值: double</span><br><span class="line">说明: 返回以2为底的a的对数</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select log2(8) from dual;</span><br><span class="line">3.0</span><br><span class="line"></span><br><span class="line">• 对数函数 : <span class="built_in">log</span></span><br><span class="line">语法: <span class="built_in">log</span>(double base, double a)</span><br><span class="line">返回值: double</span><br><span class="line">说明: 返回以base为底的a的对数</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select <span class="built_in">log</span>(4,256) from dual;</span><br><span class="line">4.0</span><br><span class="line"></span><br><span class="line">• 幂运算函数 : pow</span><br><span class="line">语法: pow(double a, double p)</span><br><span class="line">返回值: double</span><br><span class="line">说明: 返回a的p次幂</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select pow(2,4) from dual;</span><br><span class="line">16.0</span><br><span class="line"></span><br><span class="line">• 幂运算函数 : power</span><br><span class="line">语法: power(double a, double p)</span><br><span class="line">返回值: double</span><br><span class="line">说明: 返回a的p次幂,与pow功能相同</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select power(2,4) from dual;</span><br><span class="line">16.0</span><br><span class="line"></span><br><span class="line">• 开平方函数 : sqrt</span><br><span class="line">语法: sqrt(double a)</span><br><span class="line">返回值: double</span><br><span class="line">说明: 返回a的平方根</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select sqrt(16) from dual;</span><br><span class="line">4.0</span><br><span class="line"></span><br><span class="line">• 二进制函数 : bin</span><br><span class="line">语法: bin(BIGINT a)</span><br><span class="line">返回值: string</span><br><span class="line">说明: 返回a的二进制代码表示</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select bin(7) from dual;</span><br><span class="line">111</span><br><span class="line"></span><br><span class="line">• 十六进制函数 : hex</span><br><span class="line">语法: hex(BIGINT a)</span><br><span class="line">返回值: string</span><br><span class="line">说明: 如果变量是int类型，那么返回a的十六进制表示；如果变量是string类型，则返回该字符串的十六进制表示</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select hex(17) from dual;</span><br><span class="line">11</span><br><span class="line">hive&gt; select hex(‘abc’) from dual;</span><br><span class="line">616263</span><br><span class="line"></span><br><span class="line">• 反转十六进制函数 : unhex</span><br><span class="line">语法: unhex(string a)</span><br><span class="line">返回值: string</span><br><span class="line">说明: 返回该十六进制字符串所代码的字符串</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select unhex(‘616263’) from dual;</span><br><span class="line">abc</span><br><span class="line">hive&gt; select unhex(‘11’) from dual;</span><br><span class="line">-</span><br><span class="line">hive&gt; select unhex(616263) from dual;</span><br><span class="line">abc</span><br><span class="line"></span><br><span class="line">• 进制转换函数 : conv</span><br><span class="line">语法: conv(BIGINT num, int from_base, int to_base)</span><br><span class="line">返回值: string</span><br><span class="line">说明: 将数值num从from_base进制转化到to_base进制</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select conv(17,10,16) from dual;</span><br><span class="line">11</span><br><span class="line">hive&gt; select conv(17,10,2) from dual;</span><br><span class="line">10001</span><br><span class="line"></span><br><span class="line">• 绝对值函数 : abs</span><br><span class="line">语法: abs(double a)   abs(int a)</span><br><span class="line">返回值: double        int</span><br><span class="line">说明: 返回数值a的绝对值</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select abs(-3.9) from dual;</span><br><span class="line">3.9</span><br><span class="line">hive&gt; select abs(10.9) from dual;</span><br><span class="line">10.9</span><br><span class="line"></span><br><span class="line">• 正取余函数 : pmod</span><br><span class="line">语法: pmod(int a, int b),pmod(double a, double b)</span><br><span class="line">返回值: int double</span><br><span class="line">说明: 返回正的a除以b的余数</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select pmod(9,4) from dual;</span><br><span class="line">1</span><br><span class="line">hive&gt; select pmod(-9,4) from dual;</span><br><span class="line">3</span><br><span class="line"></span><br><span class="line">• 正弦函数 : sin</span><br><span class="line">语法: sin(double a)</span><br><span class="line">返回值: double</span><br><span class="line">说明: 返回a的正弦值</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select sin(0.8) from dual;</span><br><span class="line">0.7173560908995228</span><br><span class="line"></span><br><span class="line">• 反正弦函数 : asin</span><br><span class="line">语法: asin(double a)</span><br><span class="line">返回值: double</span><br><span class="line">说明: 返回a的反正弦值</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select asin(0.7173560908995228) from dual;</span><br><span class="line">0.8</span><br><span class="line"></span><br><span class="line">• 余弦函数 : cos</span><br><span class="line">语法: cos(double a)</span><br><span class="line">返回值: double</span><br><span class="line">说明: 返回a的余弦值</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select cos(0.9) from dual;</span><br><span class="line">0.6216099682706644</span><br><span class="line"></span><br><span class="line">• 反余弦函数 : acos</span><br><span class="line">语法: acos(double a)</span><br><span class="line">返回值: double</span><br><span class="line">说明: 返回a的反余弦值</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select acos(0.6216099682706644) from dual;</span><br><span class="line">0.9</span><br><span class="line"></span><br><span class="line">• positive 函数 : positive</span><br><span class="line">语法: positive(int a), positive(double a)</span><br><span class="line">返回值: int double</span><br><span class="line">说明: 返回a</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select positive(-10) from dual;</span><br><span class="line">-10</span><br><span class="line">hive&gt; select positive(12) from dual;</span><br><span class="line">12</span><br><span class="line"></span><br><span class="line">• negative 函数 : negative</span><br><span class="line">语法: negative(int a), negative(double a)</span><br><span class="line">返回值: int double</span><br><span class="line">说明: 返回-a</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select negative(-5) from dual;</span><br><span class="line">5</span><br><span class="line">hive&gt; select negative(8) from dual;</span><br><span class="line">-8</span><br></pre></td></tr></table></figure></div><h2 id="4-日期函数"><a href="#4-日期函数" class="headerlink" title="4.日期函数"></a>4.日期函数</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br></pre></td><td class="code"><pre><span class="line">•UNIX时间戳转日期函数: from_unixtime</span><br><span class="line">• 获取当前UNIX时间戳函数: unix_timestamp</span><br><span class="line">•日期转UNIX时间戳函数: unix_timestamp</span><br><span class="line">• 指定格式日期转UNIX时间戳函数: unix_timestamp</span><br><span class="line">•日期时间转日期函数: to_date</span><br><span class="line">•日期转年函数: year</span><br><span class="line">• 日期转月函数: month</span><br><span class="line">• 日期转天函数: day</span><br><span class="line">• 日期转小时函数: hour</span><br><span class="line">• 日期转分钟函数: minute</span><br><span class="line">• 日期转秒函数: second</span><br><span class="line">• 日期转周函数: weekofyear</span><br><span class="line">• 日期比较函数: datediff</span><br><span class="line">• 日期增加函数: date_add</span><br><span class="line">• 日期减少函数: date_sub</span><br><span class="line"></span><br><span class="line">• UNIX 时间戳转日期函数 : from_unixtime</span><br><span class="line">语法: from_unixtime(bigint unixtime[, string format])</span><br><span class="line">返回值: string</span><br><span class="line">说明: 转化UNIX时间戳（从1970-01-01 00:00:00 UTC到指定时间的秒数）到当前时区的时间格式</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select from_unixtime(1323308943,<span class="string">'yyyyMMdd'</span>) from dual;</span><br><span class="line">20111208</span><br><span class="line"></span><br><span class="line">• 获取当前 UNIX 时间戳函数 : unix_timestamp</span><br><span class="line">语法: unix_timestamp()</span><br><span class="line">返回值: bigint</span><br><span class="line">说明: 获得当前时区的UNIX时间戳</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select unix_timestamp() from dual;</span><br><span class="line">1323309615</span><br><span class="line"></span><br><span class="line">• 日期转 UNIX 时间戳函数 : unix_timestamp</span><br><span class="line">语法: unix_timestamp(string date)</span><br><span class="line">返回值: bigint</span><br><span class="line">说明: 转换格式为<span class="string">"yyyy-MM-dd HH:mm:ss"</span>的日期到UNIX时间戳。如果转化失败，则返回0。</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select unix_timestamp(<span class="string">'2011-12-07 13:01:03'</span>) from dual;</span><br><span class="line">1323234063</span><br><span class="line"></span><br><span class="line">• 指定格式日期转 UNIX 时间戳函数 : unix_timestamp</span><br><span class="line">语法: unix_timestamp(string date, string pattern)</span><br><span class="line">返回值: bigint</span><br><span class="line">说明: 转换pattern格式的日期到UNIX时间戳。如果转化失败，则返回0。</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select unix_timestamp(<span class="string">'20111207 13:01:03'</span>,<span class="string">'yyyyMMdd HH:mm:ss'</span>) from dual;</span><br><span class="line">1323234063</span><br><span class="line"></span><br><span class="line">• 日期时间转日期函数 : to_date</span><br><span class="line">语法: to_date(string timestamp)</span><br><span class="line">返回值: string</span><br><span class="line">说明: 返回日期时间字段中的日期部分。</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select to_date(<span class="string">'2011-12-08 10:03:01'</span>) from dual;</span><br><span class="line">2011-12-08</span><br><span class="line"></span><br><span class="line">• 日期转年函数 : year</span><br><span class="line">语法: year(string date)</span><br><span class="line">返回值: int</span><br><span class="line">说明: 返回日期中的年。</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select year(<span class="string">'2011-12-08 10:03:01'</span>) from dual;</span><br><span class="line">2011</span><br><span class="line">hive&gt; select year(<span class="string">'2012-12-08'</span>) from dual;</span><br><span class="line">2012</span><br><span class="line"></span><br><span class="line">• 日期转月函数 : month</span><br><span class="line">语法: month (string date)</span><br><span class="line">返回值: int</span><br><span class="line">说明: 返回日期中的月份。</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select month(<span class="string">'2011-12-08 10:03:01'</span>) from dual;</span><br><span class="line">12</span><br><span class="line">hive&gt; select month(<span class="string">'2011-08-08'</span>) from dual;</span><br><span class="line">8</span><br><span class="line"></span><br><span class="line">• 日期转天函数 : day</span><br><span class="line">语法: day (string date)</span><br><span class="line">返回值: int</span><br><span class="line">说明: 返回日期中的天。</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select day(<span class="string">'2011-12-08 10:03:01'</span>) from dual;</span><br><span class="line">8</span><br><span class="line">hive&gt; select day(<span class="string">'2011-12-24'</span>) from dual;</span><br><span class="line">24</span><br><span class="line"></span><br><span class="line">• 日期转小时函数 : hour</span><br><span class="line">语法: hour (string date)</span><br><span class="line">返回值: int</span><br><span class="line">说明: 返回日期中的小时。</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select hour(<span class="string">'2011-12-08 10:03:01'</span>) from dual;</span><br><span class="line">10</span><br><span class="line"></span><br><span class="line">• 日期转分钟函数 : minute</span><br><span class="line">语法: minute (string date)</span><br><span class="line">返回值: int</span><br><span class="line">说明: 返回日期中的分钟。</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select minute(<span class="string">'2011-12-08 10:03:01'</span>) from dual;</span><br><span class="line">3</span><br><span class="line"></span><br><span class="line">• 日期转秒函数 : second</span><br><span class="line">语法: second (string date)</span><br><span class="line">返回值: int</span><br><span class="line">说明: 返回日期中的秒。</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select second(<span class="string">'2011-12-08 10:03:01'</span>) from dual;</span><br><span class="line">1</span><br><span class="line"> </span><br><span class="line">• 日期转周函数 : weekofyear</span><br><span class="line">语法: weekofyear (string date)</span><br><span class="line">返回值: int</span><br><span class="line">说明: 返回日期在当前的周数。</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select weekofyear(<span class="string">'2011-12-08 10:03:01'</span>) from dual;</span><br><span class="line">49</span><br><span class="line"> </span><br><span class="line">• 日期比较函数 : datediff</span><br><span class="line">语法: datediff(string enddate, string startdate)</span><br><span class="line">返回值: int</span><br><span class="line">说明: 返回结束日期减去开始日期的天数。</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select datediff(<span class="string">'2012-12-08'</span>,<span class="string">'2012-05-09'</span>) from dual;</span><br><span class="line">213</span><br><span class="line"></span><br><span class="line">• 日期增加函数 : date_add</span><br><span class="line">语法: date_add(string startdate, int days)</span><br><span class="line">返回值: string</span><br><span class="line">说明: 返回开始日期startdate增加days天后的日期。</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select date_add(<span class="string">'2012-12-08'</span>,10) from dual;</span><br><span class="line">2012-12-18</span><br><span class="line"> </span><br><span class="line">• 日期减少函数 : date_sub</span><br><span class="line">语法: date_sub (string startdate, int days)</span><br><span class="line">返回值: string</span><br><span class="line">说明: 返回开始日期startdate减少days天后的日期。</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select date_sub(<span class="string">'2012-12-08'</span>,10) from dual;</span><br><span class="line">2012-11-28</span><br></pre></td></tr></table></figure></div><h2 id="5-条件函数"><a href="#5-条件函数" class="headerlink" title="5.条件函数"></a>5.条件函数</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">•If函数: <span class="keyword">if</span></span><br><span class="line">•非空查找函数: COALESCE</span><br><span class="line">•条件判断函数：CASE when <span class="keyword">then</span> <span class="keyword">else</span> end</span><br><span class="line"></span><br><span class="line">• If 函数 : <span class="keyword">if</span></span><br><span class="line">语法: <span class="keyword">if</span>(boolean testCondition, T valueTrue, T valueFalseOrNull)</span><br><span class="line">返回值: T</span><br><span class="line">说明:  当条件testCondition为TRUE时，返回valueTrue；否则返回valueFalseOrNull</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select <span class="keyword">if</span>(1=2,100,200) from dual;</span><br><span class="line">200</span><br><span class="line">hive&gt; select <span class="keyword">if</span>(1=1,100,200) from dual;</span><br><span class="line">100</span><br><span class="line"></span><br><span class="line">• 非空查找函数 : COALESCE</span><br><span class="line">语法: COALESCE(T v1, T v2, …)</span><br><span class="line">返回值: T</span><br><span class="line">说明:  返回参数中的第一个非空值；如果所有值都为NULL，那么返回NULL</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select COALESCE(null,<span class="string">'100'</span>,<span class="string">'50′) from dual;</span></span><br><span class="line"><span class="string">100</span></span><br><span class="line"><span class="string">条件判断函数： CASE</span></span><br><span class="line"><span class="string">语法 : CASE a WHEN b THEN c [WHEN d THEN e]* [ELSE f] END</span></span><br><span class="line"><span class="string">返回值 : T</span></span><br><span class="line"><span class="string">说明：如果 a 等于 b ，那么返回 c ；如果 a 等于 d ，那么返回 e ；否则返回 f</span></span><br><span class="line"><span class="string">举例：</span></span><br><span class="line"><span class="string">hive&gt; Select case 100 when 50 then '</span>tom<span class="string">' when 100 then '</span>mary<span class="string">' else '</span>tim<span class="string">' end from dual;</span></span><br><span class="line"><span class="string">mary</span></span><br></pre></td></tr></table></figure></div><h2 id="6-字符串函数"><a href="#6-字符串函数" class="headerlink" title="6.字符串函数"></a>6.字符串函数</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">•字符串长度函数：length</span><br><span class="line">•字符串反转函数：reverse</span><br><span class="line">•字符串连接函数：concat</span><br><span class="line">• 带分隔符字符串连接函数：concat_ws</span><br><span class="line">• 字符串截取函数：substr,substring</span><br><span class="line">• 字符串截取函数：substr,substring</span><br><span class="line">• 字符串转大写函数：upper,ucase</span><br><span class="line">• 字符串转小写函数：lower,lcase</span><br><span class="line">• 去空格函数：trim</span><br><span class="line">• 左边去空格函数：ltrim</span><br><span class="line">• 右边去空格函数：rtrim</span><br><span class="line">•正则表达式替换函数：regexp_replace</span><br><span class="line">•正则表达式解析函数：regexp_extract</span><br><span class="line">•URL解析函数：parse_url</span><br><span class="line">•json解析函数：get_json_object</span><br><span class="line">•空格字符串函数：space</span><br><span class="line">•重复字符串函数：repeat</span><br><span class="line">•首字符ascii函数：ascii</span><br><span class="line">•左补足函数：lpad</span><br><span class="line">•右补足函数：rpad</span><br><span class="line">•分割字符串函数: split</span><br><span class="line">•集合查找函数: find_in_set</span><br><span class="line"></span><br><span class="line">• 字符串长度函数： length</span><br><span class="line">语法: length(string A)</span><br><span class="line">返回值: int</span><br><span class="line">说明：返回字符串A的长度</span><br><span class="line">举例：</span><br><span class="line">hive&gt; select length(<span class="string">'abcedfg'</span>) from dual;</span><br><span class="line">7</span><br><span class="line">• 字符串反转函数： reverse</span><br><span class="line">语法: reverse(string A)</span><br><span class="line">返回值: string</span><br><span class="line">说明：返回字符串A的反转结果999999举例：</span><br><span class="line">hive&gt; select reverse(abcedfg’) from dual;</span><br></pre></td></tr></table></figure></div><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
